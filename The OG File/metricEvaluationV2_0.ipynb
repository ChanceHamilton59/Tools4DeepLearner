{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "from scipy.io import loadmat \n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from hyperdash import monitor_cell\n",
    "from hyperdash import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def printf(format, *args):\n",
    "    sys.stdout.write(format % args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split (tdata):\n",
    "    #tdata = tdata.transpose()\n",
    "    td1, td2, td3, td4, td5, td6, td7, td8, td9, td10 = np.array_split(tdata, 10)\n",
    "    \n",
    "    tdata = np.concatenate((td1, td2, td3, td4, td5, td6, td7, td8), axis = 0)\n",
    "    test_data = np.concatenate((td9, td10), axis = 0)\n",
    "    \n",
    "    #tdata = tdata.transpose()\n",
    "    #test_data = test_data.transpose()\n",
    "    \n",
    "    \n",
    "    return tdata, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(trainDir):\n",
    "    ###############################################################################\n",
    "    # This section loads the training dataset\n",
    "    ###############################################################################\n",
    "    trainDir            = trainDir\n",
    "    TrainingDataFiles   = []\n",
    "\n",
    "    for r, d, f in os.walk(trainDir):\n",
    "        for file in f:\n",
    "            TrainingDataFiles.append(os.path.join(r,file))\n",
    "\n",
    "    # load files and put them into a matrix\n",
    "    tdata = loadmat(TrainingDataFiles[0])['X']          \n",
    "    for i in range(1,len(TrainingDataFiles)):            \n",
    "        td      = loadmat(TrainingDataFiles[i])['X']\n",
    "        tdata   = np.concatenate((tdata,td), axis = 1)\n",
    "\n",
    "    # put observations in rows\n",
    "    tdata       = tdata.transpose()\n",
    "    print(tdata.shape)\n",
    "    ###############################################################################\n",
    "    # Splits Data into training (80%) and testing (20%)\n",
    "    ###############################################################################\n",
    "    #tdata, test_data = data_split(tdata)\n",
    "    return tdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparseLinAutoEncoder(wscale, learn_rate, num_steps, beta, counter, tdata):   \n",
    "    # simulation meta-parameters\n",
    "    wscale          = wscale;  # scale of the small random weights for model initialization\n",
    "    beta            = beta;  # hyper-parameter which determines penality size\n",
    "    learn_rate      = learn_rate;\n",
    "    batch_size      = 5000;\n",
    "    nBat            = 50000;\n",
    "    num_steps       = num_steps;\n",
    "    display_step    = 100\n",
    "    examples_to_show    = 5\n",
    "\n",
    "  \n",
    "    exp = Experiment(\"SparceLinAutoEncoderRELU8\")\n",
    "    \n",
    "    ###############################################################################\n",
    "    # Splits Data into training (80%) and testing (20%)\n",
    "    ###############################################################################\n",
    "    tdata, test_data = data_split(tdata)\n",
    "    \n",
    "    \n",
    "    ###############################################################################\n",
    "    # Define neural network graph\n",
    "    ###############################################################################\n",
    "    num_input   = tdata.shape[1]\n",
    "    num_train   = tdata.shape[0]\n",
    "    pixel_size  = int(np.sqrt(float(num_input)))\n",
    "\n",
    "    # compressive auto-encoder reduces dimensionality\n",
    "    # num_hidden  = round(num_input/2)\n",
    "    # sparse auto-encoder keeps dimensionality same or larger\n",
    "    num_hidden = num_input \n",
    "\n",
    "    # this placeholder will hold the training batch\n",
    "    X = tf.placeholder(\"float\", [None, num_input])\n",
    "\n",
    "\n",
    "    ## These are a hash map to variables that are used as the weights and biases in \n",
    "    ## the neural network layers. Note that the layers have differnet number of filters\n",
    "    ## and that the input passes through these layers and take the shape of the layers.\n",
    "    ## It helps to think of a network as a pipline.\n",
    "    weights = {\n",
    "        'encoder_h1': tf.Variable(tf.random_normal([num_input, num_hidden],mean=0.0, stddev=wscale)),\n",
    "        'decoder_h1': tf.Variable(tf.random_normal([num_hidden, num_input],mean=0.0, stddev=wscale))\n",
    "    }\n",
    "\n",
    "    ## Building the encoder This layer has two layers that are concatinated into a \n",
    "    ## a single layer. This layer should reduce the input vector to a matrix of diffrent\n",
    "    ## deminsions.\n",
    "    def encoder(x):\n",
    "        # Encoder Hidden layer with sigmoid activation #1\n",
    "        layer_1 = tf.nn.relu(tf.matmul(x, weights['encoder_h1']))\n",
    "        return layer_1\n",
    "\n",
    "    ## Building the decoder This is simular to the process that is happening in the\n",
    "    ## encoder code but this layer further reduces the demensionality.\n",
    "    def decoder(x):\n",
    "        # Decoder Hidden layer with sigmoid activation #1\n",
    "        layer_1 = tf.matmul(x, weights['decoder_h1'])\n",
    "        #layer_1 =  tf.matmul(x, tf.transpose(weights['encoder_h1']))\n",
    "        return layer_1\n",
    "\n",
    "    ## Construct model by forming the encoded input and the decoded encoder\n",
    "    encoder_op = encoder(X)\n",
    "    decoder_op = decoder(encoder_op)\n",
    "\n",
    "    ## Prediction \n",
    "    y_pred = decoder_op\n",
    "    # Targets (Labels) are the input data.\n",
    "    y_true = X\n",
    "\n",
    "    ## Define loss and optimizer, minimize the squared error\n",
    "\n",
    "\n",
    "    errorterm   = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "    sparseterm  = tf.reduce_mean(tf.abs(encoder_op))\n",
    "\n",
    "    loss        = errorterm + beta*sparseterm \n",
    "\n",
    "    ## Note this is not the only optimizer and infact we should look into which one we\n",
    "    ## should use for our project!!!!! ADAMOptimizer is a well known and utilize option. \n",
    "    optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    #\n",
    "    ## Initialize the variables (i.e. assign their default value)\n",
    "    init    = tf.global_variables_initializer()\n",
    "    saver   = tf.train.Saver()\n",
    "\n",
    "    ###############################################################################\n",
    "    # Start Training\n",
    "    ###############################################################################\n",
    "\n",
    "    # Start a new TF session\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "\n",
    "        # Training\n",
    "        for i in range(1, num_steps+1):\n",
    "            # Prepare Data\n",
    "            temp    = np.random.permutation(num_train)\n",
    "            bat_ind = temp[0:batch_size]\n",
    "            batch_x = tdata[bat_ind,:]\n",
    "\n",
    "\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, l , e, s = sess.run([optimizer, loss, errorterm, sparseterm], feed_dict={X: batch_x})\n",
    "            # Display logs per step\n",
    "            if i % display_step == 0 or i == 1:\n",
    "                print('Step %i: Minibatch Loss: %f Error: %f Sparse: %f' % (i, l, e, s))\n",
    "                exp.metric(\"Loss \", e);\n",
    "                exp.metric(\"Sparse \", s);\n",
    "\n",
    "        final_W = weights['encoder_h1'].eval()\n",
    "\n",
    "        # Testing\n",
    "        # Encode and decode images from test set and visualize their reconstruction.\n",
    "        n = examples_to_show\n",
    "        k1 = 1\n",
    "        k2 = 1\n",
    "        canvas_orig = np.empty((pixel_size * n, pixel_size * n))\n",
    "        canvas_recon = np.empty((pixel_size * n, pixel_size * n))\n",
    "        for i in range(n):\n",
    "\n",
    "            g = sess.run(decoder_op, feed_dict={X: batch_x})\n",
    "            # Display original images\n",
    "            for j in range(n):\n",
    "                # Draw the original digits\n",
    "                canvas_orig[i * pixel_size:(i + 1) * pixel_size, j * pixel_size:(j + 1) * pixel_size] = \\\n",
    "                    batch_x[k1].reshape([pixel_size, pixel_size])\n",
    "                k1 = k1 + 1\n",
    "            # Display reconstructed images\n",
    "            for j in range(n):\n",
    "                # Draw the reconstructed digits\n",
    "                canvas_recon[i * pixel_size:(i + 1) * pixel_size, j * pixel_size:(j + 1) * pixel_size] = \\\n",
    "                    g[k2].reshape([pixel_size, pixel_size])\n",
    "                k2 = k2 + 1\n",
    "\n",
    "        #print(\"Original Images\")\n",
    "        #plt.figure(figsize=(n, n))\n",
    "        #plt.imshow(canvas_orig, origin=\"upper\", cmap=\"gray\")\n",
    "        #plt.show()\n",
    "\n",
    "        #print(\"Reconstructed Images\")\n",
    "        #plt.figure(figsize=(n, n))\n",
    "        #plt.imshow(canvas_recon, origin=\"upper\", cmap=\"gray\")\n",
    "        #plt.show()\n",
    "        #print(canvas_orig)\n",
    "        \n",
    "        \n",
    "        save_path = saver.save(sess,\"./epochCkpt/sparseLinTrainMiniBatFinal\" + str(counter) + \".ckpt\")\n",
    "        \n",
    "\n",
    "    \n",
    "    Z = final_W\n",
    "    nHid            = Z.shape[1]            # get the number of hidden units\n",
    "    imSz            = int(np.sqrt(Z.shape[0]))   # size of each image patch\n",
    "    plotGap2        = 1                     # half the gap to leave between each image patch\n",
    "    imSzPlot        = int(imSz + 2*plotGap2)    \n",
    "\n",
    "    layoutSqDim     = int(np.ceil(np.sqrt(nHid)))  # find dimensions for layout\n",
    "    arrayImage      = np.zeros((layoutSqDim*imSzPlot,layoutSqDim*imSzPlot))\n",
    "\n",
    "    k = 0\n",
    "    for i in range(layoutSqDim):\n",
    "        for j in range(layoutSqDim):\n",
    "            if k < nHid:    \n",
    "                littleImPlot = np.zeros((imSzPlot,imSzPlot))\n",
    "                littleImPlot[(plotGap2):(imSzPlot-plotGap2),(plotGap2):(imSzPlot-plotGap2)] = np.reshape(Z[:,k],(imSz,imSz)) \n",
    "                arrayImage[ (i*imSzPlot ):((i+1)*imSzPlot) , (j*imSzPlot ):((j+1)*imSzPlot) ] = littleImPlot;\n",
    "                k = k + 1\n",
    "\n",
    "    output = [l, e, s, canvas_orig, canvas_recon, arrayImage]\n",
    "    plt.imshow(arrayImage,  cmap='Greys')\n",
    "    #plt.show()\n",
    "    \n",
    "    sess.close()\n",
    "    exp.end()\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 64)\n",
      "Step 1: Minibatch Loss: 0.020222 Error: 0.018280 Sparse: 0.038852\n",
      "| Loss :   0.018280 |\n",
      "| Sparse :   0.038852 |\n",
      "Step 100: Minibatch Loss: 0.003420 Error: 0.002434 Sparse: 0.019704\n",
      "| Loss :   0.002434 |\n",
      "| Sparse :   0.019704 |\n",
      "Step 200: Minibatch Loss: 0.003720 Error: 0.002890 Sparse: 0.016605\n",
      "| Loss :   0.002890 |\n",
      "| Sparse :   0.016605 |\n",
      "Step 300: Minibatch Loss: 0.003156 Error: 0.002394 Sparse: 0.015246\n",
      "| Loss :   0.002394 |\n",
      "| Sparse :   0.015246 |\n",
      "Step 400: Minibatch Loss: 0.002785 Error: 0.002101 Sparse: 0.013679\n",
      "| Loss :   0.002101 |\n",
      "| Sparse :   0.013679 |\n",
      "Step 500: Minibatch Loss: 0.002717 Error: 0.002055 Sparse: 0.013244\n",
      "| Loss :   0.002055 |\n",
      "| Sparse :   0.013244 |\n",
      "Step 600: Minibatch Loss: 0.002706 Error: 0.002038 Sparse: 0.013365\n",
      "| Loss :   0.002038 |\n",
      "| Sparse :   0.013365 |\n",
      "Step 700: Minibatch Loss: 0.002893 Error: 0.002238 Sparse: 0.013114\n",
      "| Loss :   0.002238 |\n",
      "| Sparse :   0.013114 |\n",
      "Step 800: Minibatch Loss: 0.002838 Error: 0.002220 Sparse: 0.012368\n",
      "| Loss :   0.002220 |\n",
      "| Sparse :   0.012368 |\n",
      "Step 900: Minibatch Loss: 0.002848 Error: 0.002153 Sparse: 0.013911\n",
      "| Loss :   0.002153 |\n",
      "| Sparse :   0.013911 |\n",
      "Step 1000: Minibatch Loss: 0.003431 Error: 0.002816 Sparse: 0.012302\n",
      "| Loss :   0.002816 |\n",
      "| Sparse :   0.012302 |\n",
      "This run of SparceLinAutoEncoderRELU8 ran for 0:00:16 and logs are available locally at: C:\\Users\\cjhamilton4176\\.hyperdash\\logs\\sparcelinautoencoderrelu8\\sparcelinautoencoderrelu8_2018-11-07t11-56-54-668695.log\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tdata = data_loader('RawData/TrainingData8')\n",
    "out = sparseLinAutoEncoder(.1, .05, 1000, .05 ,555, tdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 64)\n",
      "Step 1: Minibatch Loss: 0.015425 Error: 0.015425 Sparse: 0.000392\n",
      "| Loss :   0.015425 |\n",
      "| Sparse :   0.000392 |\n",
      "Step 100: Minibatch Loss: 0.009915 Error: 0.009911 Sparse: 0.040146\n",
      "| Loss :   0.009911 |\n",
      "| Sparse :   0.040146 |\n",
      "Step 200: Minibatch Loss: 0.006877 Error: 0.006872 Sparse: 0.050970\n",
      "| Loss :   0.006872 |\n",
      "| Sparse :   0.050970 |\n",
      "Step 300: Minibatch Loss: 0.005377 Error: 0.005371 Sparse: 0.055521\n",
      "| Loss :   0.005371 |\n",
      "| Sparse :   0.055521 |\n",
      "Step 400: Minibatch Loss: 0.004381 Error: 0.004376 Sparse: 0.058512\n",
      "| Loss :   0.004376 |\n",
      "| Sparse :   0.058512 |\n",
      "Step 500: Minibatch Loss: 0.003721 Error: 0.003715 Sparse: 0.059905\n",
      "| Loss :   0.003715 |\n",
      "| Sparse :   0.059905 |\n",
      "Step 600: Minibatch Loss: 0.003212 Error: 0.003206 Sparse: 0.061392\n",
      "| Loss :   0.003206 |\n",
      "| Sparse :   0.061392 |\n",
      "Step 700: Minibatch Loss: 0.002870 Error: 0.002863 Sparse: 0.061906\n",
      "| Loss :   0.002863 |\n",
      "| Sparse :   0.061906 |\n",
      "Step 800: Minibatch Loss: 0.002644 Error: 0.002638 Sparse: 0.062645\n",
      "| Loss :   0.002638 |\n",
      "| Sparse :   0.062645 |\n",
      "Step 900: Minibatch Loss: 0.002457 Error: 0.002451 Sparse: 0.062321\n",
      "| Loss :   0.002451 |\n",
      "| Sparse :   0.062321 |\n",
      "Step 1000: Minibatch Loss: 0.002367 Error: 0.002361 Sparse: 0.061896\n",
      "| Loss :   0.002361 |\n",
      "| Sparse :   0.061896 |\n",
      "This run of SparceLinAutoEncoderRELU8 ran for 0:00:16 and logs are available locally at: C:\\Users\\cjhamilton4176\\.hyperdash\\logs\\sparcelinautoencoderrelu8\\sparcelinautoencoderrelu8_2018-11-07t11-57-12-420773.log\n",
      "Step 1: Minibatch Loss: 0.015375 Error: 0.015375 Sparse: 0.000397\n",
      "| Loss :   0.015375 |\n",
      "| Sparse :   0.000397 |\n",
      "Step 100: Minibatch Loss: 0.009998 Error: 0.009958 Sparse: 0.040005\n",
      "| Loss :   0.009958 |\n",
      "| Sparse :   0.040005 |\n",
      "Step 200: Minibatch Loss: 0.006811 Error: 0.006760 Sparse: 0.050424\n",
      "| Loss :   0.006760 |\n",
      "| Sparse :   0.050424 |\n",
      "Step 300: Minibatch Loss: 0.005502 Error: 0.005448 Sparse: 0.054615\n",
      "| Loss :   0.005448 |\n",
      "| Sparse :   0.054615 |\n",
      "Step 400: Minibatch Loss: 0.004483 Error: 0.004425 Sparse: 0.057531\n",
      "| Loss :   0.004425 |\n",
      "| Sparse :   0.057531 |\n",
      "Step 500: Minibatch Loss: 0.003814 Error: 0.003755 Sparse: 0.058965\n",
      "| Loss :   0.003755 |\n",
      "| Sparse :   0.058965 |\n",
      "Step 600: Minibatch Loss: 0.003347 Error: 0.003287 Sparse: 0.060027\n",
      "| Loss :   0.003287 |\n",
      "| Sparse :   0.060027 |\n",
      "Step 700: Minibatch Loss: 0.002980 Error: 0.002920 Sparse: 0.060389\n",
      "| Loss :   0.002920 |\n",
      "| Sparse :   0.060389 |\n",
      "Step 800: Minibatch Loss: 0.002715 Error: 0.002654 Sparse: 0.060805\n",
      "| Loss :   0.002654 |\n",
      "| Sparse :   0.060805 |\n",
      "Step 900: Minibatch Loss: 0.002516 Error: 0.002455 Sparse: 0.060553\n",
      "| Loss :   0.002455 |\n",
      "| Sparse :   0.060553 |\n",
      "Step 1000: Minibatch Loss: 0.002404 Error: 0.002344 Sparse: 0.060125\n",
      "| Loss :   0.002344 |\n",
      "| Sparse :   0.060125 |\n",
      "This run of SparceLinAutoEncoderRELU8 ran for 0:00:16 and logs are available locally at: C:\\Users\\cjhamilton4176\\.hyperdash\\logs\\sparcelinautoencoderrelu8\\sparcelinautoencoderrelu8_2018-11-07t11-57-29-420006.log\n",
      "Step 1: Minibatch Loss: 0.015429 Error: 0.015425 Sparse: 0.000393\n",
      "| Loss :   0.015425 |\n",
      "| Sparse :   0.000393 |\n",
      "Step 100: Minibatch Loss: 0.010843 Error: 0.010492 Sparse: 0.035117\n",
      "| Loss :   0.010492 |\n",
      "| Sparse :   0.035117 |\n",
      "Step 200: Minibatch Loss: 0.007388 Error: 0.006935 Sparse: 0.045290\n",
      "| Loss :   0.006935 |\n",
      "| Sparse :   0.045290 |\n",
      "Step 300: Minibatch Loss: 0.005930 Error: 0.005450 Sparse: 0.048045\n",
      "| Loss :   0.005450 |\n",
      "| Sparse :   0.048045 |\n",
      "Step 400: Minibatch Loss: 0.004912 Error: 0.004416 Sparse: 0.049660\n",
      "| Loss :   0.004416 |\n",
      "| Sparse :   0.049660 |\n",
      "Step 500: Minibatch Loss: 0.004235 Error: 0.003735 Sparse: 0.049987\n",
      "| Loss :   0.003735 |\n",
      "| Sparse :   0.049987 |\n",
      "Step 600: Minibatch Loss: 0.003687 Error: 0.003186 Sparse: 0.050059\n",
      "| Loss :   0.003186 |\n",
      "| Sparse :   0.050059 |\n",
      "Step 700: Minibatch Loss: 0.003333 Error: 0.002834 Sparse: 0.049918\n",
      "| Loss :   0.002834 |\n",
      "| Sparse :   0.049918 |\n",
      "Step 800: Minibatch Loss: 0.003072 Error: 0.002581 Sparse: 0.049150\n",
      "| Loss :   0.002581 |\n",
      "| Sparse :   0.049150 |\n",
      "Step 900: Minibatch Loss: 0.002941 Error: 0.002458 Sparse: 0.048282\n",
      "| Loss :   0.002458 |\n",
      "| Sparse :   0.048282 |\n",
      "Step 1000: Minibatch Loss: 0.002823 Error: 0.002350 Sparse: 0.047304\n",
      "| Loss :   0.002350 |\n",
      "| Sparse :   0.047304 |\n",
      "This run of SparceLinAutoEncoderRELU8 ran for 0:00:16 and logs are available locally at: C:\\Users\\cjhamilton4176\\.hyperdash\\logs\\sparcelinautoencoderrelu8\\sparcelinautoencoderrelu8_2018-11-07t11-57-46-827742.log\n",
      "Step 1: Minibatch Loss: 0.015458 Error: 0.015419 Sparse: 0.000397\n",
      "| Loss :   0.015419 |\n",
      "| Sparse :   0.000397 |\n",
      "Step 100: Minibatch Loss: 0.015420 Error: 0.015415 Sparse: 0.000044\n",
      "| Loss :   0.015415 |\n",
      "| Sparse :   0.000044 |\n",
      "Step 200: Minibatch Loss: 0.014314 Error: 0.013794 Sparse: 0.005199\n",
      "| Loss :   0.013794 |\n",
      "| Sparse :   0.005199 |\n",
      "Step 300: Minibatch Loss: 0.011982 Error: 0.010934 Sparse: 0.010488\n",
      "| Loss :   0.010934 |\n",
      "| Sparse :   0.010488 |\n",
      "Step 400: Minibatch Loss: 0.010179 Error: 0.008899 Sparse: 0.012792\n",
      "| Loss :   0.008899 |\n",
      "| Sparse :   0.012792 |\n",
      "Step 500: Minibatch Loss: 0.008948 Error: 0.007486 Sparse: 0.014617\n",
      "| Loss :   0.007486 |\n",
      "| Sparse :   0.014617 |\n",
      "Step 600: Minibatch Loss: 0.008467 Error: 0.007015 Sparse: 0.014521\n",
      "| Loss :   0.007015 |\n",
      "| Sparse :   0.014521 |\n",
      "Step 700: Minibatch Loss: 0.008169 Error: 0.006765 Sparse: 0.014044\n",
      "| Loss :   0.006765 |\n",
      "| Sparse :   0.014044 |\n",
      "Step 800: Minibatch Loss: 0.007874 Error: 0.006510 Sparse: 0.013639\n",
      "| Loss :   0.006510 |\n",
      "| Sparse :   0.013639 |\n",
      "Step 900: Minibatch Loss: 0.007642 Error: 0.006341 Sparse: 0.013006\n",
      "| Loss :   0.006341 |\n",
      "| Sparse :   0.013006 |\n",
      "Step 1000: Minibatch Loss: 0.007479 Error: 0.006191 Sparse: 0.012881\n",
      "| Loss :   0.006191 |\n",
      "| Sparse :   0.012881 |\n",
      "This run of SparceLinAutoEncoderRELU8 ran for 0:00:16 and logs are available locally at: C:\\Users\\cjhamilton4176\\.hyperdash\\logs\\sparcelinautoencoderrelu8\\sparcelinautoencoderrelu8_2018-11-07t11-58-03-470245.log\n",
      "Step 1: Minibatch Loss: 0.015752 Error: 0.015366 Sparse: 0.000387\n",
      "| Loss :   0.015366 |\n",
      "| Sparse :   0.000387 |\n",
      "Step 100: Minibatch Loss: 0.015458 Error: 0.015416 Sparse: 0.000042\n",
      "| Loss :   0.015416 |\n",
      "| Sparse :   0.000042 |\n",
      "Step 200: Minibatch Loss: 0.015434 Error: 0.015393 Sparse: 0.000041\n",
      "| Loss :   0.015393 |\n",
      "| Sparse :   0.000041 |\n",
      "Step 300: Minibatch Loss: 0.015451 Error: 0.015413 Sparse: 0.000039\n",
      "| Loss :   0.015413 |\n",
      "| Sparse :   0.000039 |\n",
      "Step 400: Minibatch Loss: 0.015437 Error: 0.015399 Sparse: 0.000038\n",
      "| Loss :   0.015399 |\n",
      "| Sparse :   0.000038 |\n",
      "Step 500: Minibatch Loss: 0.015457 Error: 0.015422 Sparse: 0.000035\n",
      "| Loss :   0.015422 |\n",
      "| Sparse :   0.000035 |\n",
      "Step 600: Minibatch Loss: 0.015468 Error: 0.015427 Sparse: 0.000041\n",
      "| Loss :   0.015427 |\n",
      "| Sparse :   0.000041 |\n",
      "Step 700: Minibatch Loss: 0.015470 Error: 0.015432 Sparse: 0.000038\n",
      "| Loss :   0.015432 |\n",
      "| Sparse :   0.000038 |\n",
      "Step 800: Minibatch Loss: 0.015463 Error: 0.015426 Sparse: 0.000037\n",
      "| Loss :   0.015426 |\n",
      "| Sparse :   0.000037 |\n",
      "Step 900: Minibatch Loss: 0.015448 Error: 0.015409 Sparse: 0.000038\n",
      "| Loss :   0.015409 |\n",
      "| Sparse :   0.000038 |\n",
      "Step 1000: Minibatch Loss: 0.015413 Error: 0.015362 Sparse: 0.000050\n",
      "| Loss :   0.015362 |\n",
      "| Sparse :   0.000050 |\n",
      "This run of SparceLinAutoEncoderRELU8 ran for 0:00:16 and logs are available locally at: C:\\Users\\cjhamilton4176\\.hyperdash\\logs\\sparcelinautoencoderrelu8\\sparcelinautoencoderrelu8_2018-11-07t11-58-19-861643.log\n",
      "Step 1: Minibatch Loss: 0.015397 Error: 0.015397 Sparse: 0.000397\n",
      "| Loss :   0.015397 |\n",
      "| Sparse :   0.000397 |\n",
      "Step 100: Minibatch Loss: 0.009967 Error: 0.009963 Sparse: 0.039974\n",
      "| Loss :   0.009963 |\n",
      "| Sparse :   0.039974 |\n",
      "Step 200: Minibatch Loss: 0.006827 Error: 0.006822 Sparse: 0.051039\n",
      "| Loss :   0.006822 |\n",
      "| Sparse :   0.051039 |\n",
      "Step 300: Minibatch Loss: 0.005375 Error: 0.005369 Sparse: 0.055687\n",
      "| Loss :   0.005369 |\n",
      "| Sparse :   0.055687 |\n",
      "Step 400: Minibatch Loss: 0.004445 Error: 0.004439 Sparse: 0.058718\n",
      "| Loss :   0.004439 |\n",
      "| Sparse :   0.058718 |\n",
      "Step 500: Minibatch Loss: 0.003805 Error: 0.003799 Sparse: 0.060416\n",
      "| Loss :   0.003799 |\n",
      "| Sparse :   0.060416 |\n",
      "Step 600: Minibatch Loss: 0.003294 Error: 0.003288 Sparse: 0.061792\n",
      "| Loss :   0.003288 |\n",
      "| Sparse :   0.061792 |\n",
      "Step 700: Minibatch Loss: 0.002900 Error: 0.002893 Sparse: 0.062767\n",
      "| Loss :   0.002893 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Sparse :   0.062767 |\n",
      "Step 800: Minibatch Loss: 0.002664 Error: 0.002658 Sparse: 0.063072\n",
      "| Loss :   0.002658 |\n",
      "| Sparse :   0.063072 |\n",
      "Step 900: Minibatch Loss: 0.002466 Error: 0.002460 Sparse: 0.063292\n",
      "| Loss :   0.002460 |\n",
      "| Sparse :   0.063292 |\n",
      "Step 1000: Minibatch Loss: 0.002405 Error: 0.002399 Sparse: 0.063112\n",
      "| Loss :   0.002399 |\n",
      "| Sparse :   0.063112 |\n",
      "Step 1100: Minibatch Loss: 0.002300 Error: 0.002294 Sparse: 0.062368\n",
      "| Loss :   0.002294 |\n",
      "| Sparse :   0.062368 |\n",
      "Step 1200: Minibatch Loss: 0.002271 Error: 0.002264 Sparse: 0.061706\n",
      "| Loss :   0.002264 |\n",
      "| Sparse :   0.061706 |\n",
      "Step 1300: Minibatch Loss: 0.002180 Error: 0.002174 Sparse: 0.061347\n",
      "| Loss :   0.002174 |\n",
      "| Sparse :   0.061347 |\n",
      "Step 1400: Minibatch Loss: 0.002131 Error: 0.002125 Sparse: 0.060947\n",
      "| Loss :   0.002125 |\n",
      "| Sparse :   0.060947 |\n",
      "Step 1500: Minibatch Loss: 0.002103 Error: 0.002097 Sparse: 0.060730\n",
      "| Loss :   0.002097 |\n",
      "| Sparse :   0.060730 |\n",
      "Step 1600: Minibatch Loss: 0.002099 Error: 0.002093 Sparse: 0.060454\n",
      "| Loss :   0.002093 |\n",
      "| Sparse :   0.060454 |\n",
      "Step 1700: Minibatch Loss: 0.002023 Error: 0.002017 Sparse: 0.060229\n",
      "| Loss :   0.002017 |\n",
      "| Sparse :   0.060229 |\n",
      "Step 1800: Minibatch Loss: 0.002069 Error: 0.002063 Sparse: 0.060412\n",
      "| Loss :   0.002063 |\n",
      "| Sparse :   0.060412 |\n",
      "Step 1900: Minibatch Loss: 0.002032 Error: 0.002026 Sparse: 0.059907\n",
      "| Loss :   0.002026 |\n",
      "| Sparse :   0.059907 |\n",
      "Step 2000: Minibatch Loss: 0.002032 Error: 0.002026 Sparse: 0.059966\n",
      "| Loss :   0.002026 |\n",
      "| Sparse :   0.059966 |\n",
      "This run of SparceLinAutoEncoderRELU8 ran for 0:00:31 and logs are available locally at: C:\\Users\\cjhamilton4176\\.hyperdash\\logs\\sparcelinautoencoderrelu8\\sparcelinautoencoderrelu8_2018-11-07t11-58-37-073514.log\n",
      "Step 1: Minibatch Loss: 0.015419 Error: 0.015419 Sparse: 0.000391\n",
      "| Loss :   0.015419 |\n",
      "| Sparse :   0.000391 |\n",
      "Step 100: Minibatch Loss: 0.010096 Error: 0.010057 Sparse: 0.039684\n",
      "| Loss :   0.010057 |\n",
      "| Sparse :   0.039684 |\n",
      "Step 200: Minibatch Loss: 0.006989 Error: 0.006939 Sparse: 0.050259\n",
      "| Loss :   0.006939 |\n",
      "| Sparse :   0.050259 |\n",
      "Step 300: Minibatch Loss: 0.005528 Error: 0.005473 Sparse: 0.054366\n",
      "| Loss :   0.005473 |\n",
      "| Sparse :   0.054366 |\n",
      "Step 400: Minibatch Loss: 0.004499 Error: 0.004441 Sparse: 0.057118\n",
      "| Loss :   0.004441 |\n",
      "| Sparse :   0.057118 |\n",
      "Step 500: Minibatch Loss: 0.003811 Error: 0.003753 Sparse: 0.058773\n",
      "| Loss :   0.003753 |\n",
      "| Sparse :   0.058773 |\n",
      "Step 600: Minibatch Loss: 0.003278 Error: 0.003218 Sparse: 0.059880\n",
      "| Loss :   0.003218 |\n",
      "| Sparse :   0.059880 |\n",
      "Step 700: Minibatch Loss: 0.002941 Error: 0.002881 Sparse: 0.060334\n",
      "| Loss :   0.002881 |\n",
      "| Sparse :   0.060334 |\n",
      "Step 800: Minibatch Loss: 0.002709 Error: 0.002649 Sparse: 0.060515\n",
      "| Loss :   0.002649 |\n",
      "| Sparse :   0.060515 |\n",
      "Step 900: Minibatch Loss: 0.002535 Error: 0.002475 Sparse: 0.060265\n",
      "| Loss :   0.002475 |\n",
      "| Sparse :   0.060265 |\n",
      "Step 1000: Minibatch Loss: 0.002377 Error: 0.002317 Sparse: 0.059585\n",
      "| Loss :   0.002317 |\n",
      "| Sparse :   0.059585 |\n",
      "Step 1100: Minibatch Loss: 0.002353 Error: 0.002294 Sparse: 0.059053\n",
      "| Loss :   0.002294 |\n",
      "| Sparse :   0.059053 |\n",
      "Step 1200: Minibatch Loss: 0.002278 Error: 0.002219 Sparse: 0.058530\n",
      "| Loss :   0.002219 |\n",
      "| Sparse :   0.058530 |\n",
      "Step 1300: Minibatch Loss: 0.002210 Error: 0.002152 Sparse: 0.058327\n",
      "| Loss :   0.002152 |\n",
      "| Sparse :   0.058327 |\n",
      "Step 1400: Minibatch Loss: 0.002157 Error: 0.002100 Sparse: 0.057537\n",
      "| Loss :   0.002100 |\n",
      "| Sparse :   0.057537 |\n",
      "Step 1500: Minibatch Loss: 0.002158 Error: 0.002101 Sparse: 0.057176\n",
      "| Loss :   0.002101 |\n",
      "| Sparse :   0.057176 |\n",
      "Step 1600: Minibatch Loss: 0.002142 Error: 0.002085 Sparse: 0.057040\n",
      "| Loss :   0.002085 |\n",
      "| Sparse :   0.057040 |\n",
      "Step 1700: Minibatch Loss: 0.002125 Error: 0.002068 Sparse: 0.056754\n",
      "| Loss :   0.002068 |\n",
      "| Sparse :   0.056754 |\n",
      "Step 1800: Minibatch Loss: 0.002094 Error: 0.002038 Sparse: 0.056224\n",
      "| Loss :   0.002038 |\n",
      "| Sparse :   0.056224 |\n",
      "Step 1900: Minibatch Loss: 0.002105 Error: 0.002049 Sparse: 0.055696\n",
      "| Loss :   0.002049 |\n",
      "| Sparse :   0.055696 |\n",
      "Step 2000: Minibatch Loss: 0.002113 Error: 0.002057 Sparse: 0.055464\n",
      "| Loss :   0.002057 |\n",
      "| Sparse :   0.055464 |\n",
      "This run of SparceLinAutoEncoderRELU8 ran for 0:00:31 and logs are available locally at: C:\\Users\\cjhamilton4176\\.hyperdash\\logs\\sparcelinautoencoderrelu8\\sparcelinautoencoderrelu8_2018-11-07t11-59-09-118767.log\n",
      "Step 1: Minibatch Loss: 0.015419 Error: 0.015415 Sparse: 0.000391\n",
      "| Loss :   0.015415 |\n",
      "| Sparse :   0.000391 |\n",
      "Step 100: Minibatch Loss: 0.010515 Error: 0.010152 Sparse: 0.036230\n",
      "| Loss :   0.010152 |\n",
      "| Sparse :   0.036230 |\n",
      "Step 200: Minibatch Loss: 0.007305 Error: 0.006851 Sparse: 0.045414\n",
      "| Loss :   0.006851 |\n",
      "| Sparse :   0.045414 |\n",
      "Step 300: Minibatch Loss: 0.005702 Error: 0.005219 Sparse: 0.048349\n",
      "| Loss :   0.005219 |\n",
      "| Sparse :   0.048349 |\n",
      "Step 400: Minibatch Loss: 0.004804 Error: 0.004304 Sparse: 0.049966\n",
      "| Loss :   0.004304 |\n",
      "| Sparse :   0.049966 |\n",
      "Step 500: Minibatch Loss: 0.004167 Error: 0.003667 Sparse: 0.050065\n",
      "| Loss :   0.003667 |\n",
      "| Sparse :   0.050065 |\n",
      "Step 600: Minibatch Loss: 0.003704 Error: 0.003206 Sparse: 0.049737\n",
      "| Loss :   0.003206 |\n",
      "| Sparse :   0.049737 |\n",
      "Step 700: Minibatch Loss: 0.003405 Error: 0.002908 Sparse: 0.049625\n",
      "| Loss :   0.002908 |\n",
      "| Sparse :   0.049625 |\n",
      "Step 800: Minibatch Loss: 0.003082 Error: 0.002593 Sparse: 0.048926\n",
      "| Loss :   0.002593 |\n",
      "| Sparse :   0.048926 |\n",
      "Step 900: Minibatch Loss: 0.002957 Error: 0.002476 Sparse: 0.048093\n",
      "| Loss :   0.002476 |\n",
      "| Sparse :   0.048093 |\n",
      "Step 1000: Minibatch Loss: 0.002826 Error: 0.002354 Sparse: 0.047267\n",
      "| Loss :   0.002354 |\n",
      "| Sparse :   0.047267 |\n",
      "Step 1100: Minibatch Loss: 0.002714 Error: 0.002254 Sparse: 0.046011\n",
      "| Loss :   0.002254 |\n",
      "| Sparse :   0.046011 |\n",
      "Step 1200: Minibatch Loss: 0.002643 Error: 0.002195 Sparse: 0.044815\n",
      "| Loss :   0.002195 |\n",
      "| Sparse :   0.044815 |\n",
      "Step 1300: Minibatch Loss: 0.002584 Error: 0.002146 Sparse: 0.043805\n",
      "| Loss :   0.002146 |\n",
      "| Sparse :   0.043805 |\n",
      "Step 1400: Minibatch Loss: 0.002593 Error: 0.002164 Sparse: 0.042898\n",
      "| Loss :   0.002164 |\n",
      "| Sparse :   0.042898 |\n",
      "Step 1500: Minibatch Loss: 0.002489 Error: 0.002072 Sparse: 0.041733\n",
      "| Loss :   0.002072 |\n",
      "| Sparse :   0.041733 |\n",
      "Step 1600: Minibatch Loss: 0.002503 Error: 0.002099 Sparse: 0.040460\n",
      "| Loss :   0.002099 |\n",
      "| Sparse :   0.040460 |\n",
      "Step 1700: Minibatch Loss: 0.002439 Error: 0.002043 Sparse: 0.039630\n",
      "| Loss :   0.002043 |\n",
      "| Sparse :   0.039630 |\n",
      "Step 1800: Minibatch Loss: 0.002440 Error: 0.002054 Sparse: 0.038651\n",
      "| Loss :   0.002054 |\n",
      "| Sparse :   0.038651 |\n",
      "Step 1900: Minibatch Loss: 0.002394 Error: 0.002017 Sparse: 0.037669\n",
      "| Loss :   0.002017 |\n",
      "| Sparse :   0.037669 |\n",
      "Step 2000: Minibatch Loss: 0.002395 Error: 0.002027 Sparse: 0.036818\n",
      "| Loss :   0.002027 |\n",
      "| Sparse :   0.036818 |\n",
      "This run of SparceLinAutoEncoderRELU8 ran for 0:00:32 and logs are available locally at: C:\\Users\\cjhamilton4176\\.hyperdash\\logs\\sparcelinautoencoderrelu8\\sparcelinautoencoderrelu8_2018-11-07t11-59-41-129007.log\n",
      "Step 1: Minibatch Loss: 0.015433 Error: 0.015394 Sparse: 0.000396\n",
      "| Loss :   0.015394 |\n",
      "| Sparse :   0.000396 |\n",
      "Step 100: Minibatch Loss: 0.015432 Error: 0.015427 Sparse: 0.000042\n",
      "| Loss :   0.015427 |\n",
      "| Sparse :   0.000042 |\n",
      "Step 200: Minibatch Loss: 0.014308 Error: 0.013808 Sparse: 0.005002\n",
      "| Loss :   0.013808 |\n",
      "| Sparse :   0.005002 |\n",
      "Step 300: Minibatch Loss: 0.011914 Error: 0.010845 Sparse: 0.010687\n",
      "| Loss :   0.010845 |\n",
      "| Sparse :   0.010687 |\n",
      "Step 400: Minibatch Loss: 0.009927 Error: 0.008557 Sparse: 0.013701\n",
      "| Loss :   0.008557 |\n",
      "| Sparse :   0.013701 |\n",
      "Step 500: Minibatch Loss: 0.008918 Error: 0.007440 Sparse: 0.014782\n",
      "| Loss :   0.007440 |\n",
      "| Sparse :   0.014782 |\n",
      "Step 600: Minibatch Loss: 0.008311 Error: 0.006827 Sparse: 0.014838\n",
      "| Loss :   0.006827 |\n",
      "| Sparse :   0.014838 |\n",
      "Step 700: Minibatch Loss: 0.007966 Error: 0.006513 Sparse: 0.014531\n",
      "| Loss :   0.006513 |\n",
      "| Sparse :   0.014531 |\n",
      "Step 800: Minibatch Loss: 0.007638 Error: 0.006239 Sparse: 0.013988\n",
      "| Loss :   0.006239 |\n",
      "| Sparse :   0.013988 |\n",
      "Step 900: Minibatch Loss: 0.007557 Error: 0.006222 Sparse: 0.013346\n",
      "| Loss :   0.006222 |\n",
      "| Sparse :   0.013346 |\n",
      "Step 1000: Minibatch Loss: 0.007402 Error: 0.006127 Sparse: 0.012746\n",
      "| Loss :   0.006127 |\n",
      "| Sparse :   0.012746 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1100: Minibatch Loss: 0.007129 Error: 0.005883 Sparse: 0.012464\n",
      "| Loss :   0.005883 |\n",
      "| Sparse :   0.012464 |\n",
      "Step 1200: Minibatch Loss: 0.006913 Error: 0.005629 Sparse: 0.012841\n",
      "| Loss :   0.005629 |\n",
      "| Sparse :   0.012841 |\n",
      "Step 1300: Minibatch Loss: 0.006410 Error: 0.005079 Sparse: 0.013309\n",
      "| Loss :   0.005079 |\n",
      "| Sparse :   0.013309 |\n",
      "Step 1400: Minibatch Loss: 0.006000 Error: 0.004643 Sparse: 0.013573\n",
      "| Loss :   0.004643 |\n",
      "| Sparse :   0.013573 |\n",
      "Step 1500: Minibatch Loss: 0.005783 Error: 0.004448 Sparse: 0.013349\n",
      "| Loss :   0.004448 |\n",
      "| Sparse :   0.013349 |\n",
      "Step 1600: Minibatch Loss: 0.005601 Error: 0.004270 Sparse: 0.013303\n",
      "| Loss :   0.004270 |\n",
      "| Sparse :   0.013303 |\n",
      "Step 1700: Minibatch Loss: 0.005210 Error: 0.003854 Sparse: 0.013560\n",
      "| Loss :   0.003854 |\n",
      "| Sparse :   0.013560 |\n",
      "Step 1800: Minibatch Loss: 0.005078 Error: 0.003738 Sparse: 0.013409\n",
      "| Loss :   0.003738 |\n",
      "| Sparse :   0.013409 |\n",
      "Step 1900: Minibatch Loss: 0.004752 Error: 0.003388 Sparse: 0.013647\n",
      "| Loss :   0.003388 |\n",
      "| Sparse :   0.013647 |\n",
      "Step 2000: Minibatch Loss: 0.004466 Error: 0.003066 Sparse: 0.013992\n",
      "| Loss :   0.003066 |\n",
      "| Sparse :   0.013992 |\n",
      "This run of SparceLinAutoEncoderRELU8 ran for 0:00:31 and logs are available locally at: C:\\Users\\cjhamilton4176\\.hyperdash\\logs\\sparcelinautoencoderrelu8\\sparcelinautoencoderrelu8_2018-11-07t12-00-14-295636.log\n",
      "Step 1: Minibatch Loss: 0.015802 Error: 0.015403 Sparse: 0.000399\n",
      "| Loss :   0.015403 |\n",
      "| Sparse :   0.000399 |\n",
      "Step 100: Minibatch Loss: 0.015455 Error: 0.015416 Sparse: 0.000039\n",
      "| Loss :   0.015416 |\n",
      "| Sparse :   0.000039 |\n",
      "Step 200: Minibatch Loss: 0.015434 Error: 0.015390 Sparse: 0.000044\n",
      "| Loss :   0.015390 |\n",
      "| Sparse :   0.000044 |\n",
      "Step 300: Minibatch Loss: 0.015441 Error: 0.015400 Sparse: 0.000042\n",
      "| Loss :   0.015400 |\n",
      "| Sparse :   0.000042 |\n",
      "Step 400: Minibatch Loss: 0.015437 Error: 0.015396 Sparse: 0.000041\n",
      "| Loss :   0.015396 |\n",
      "| Sparse :   0.000041 |\n",
      "Step 500: Minibatch Loss: 0.015455 Error: 0.015421 Sparse: 0.000034\n",
      "| Loss :   0.015421 |\n",
      "| Sparse :   0.000034 |\n",
      "Step 600: Minibatch Loss: 0.015421 Error: 0.015387 Sparse: 0.000035\n",
      "| Loss :   0.015387 |\n",
      "| Sparse :   0.000035 |\n",
      "Step 700: Minibatch Loss: 0.015432 Error: 0.015391 Sparse: 0.000041\n",
      "| Loss :   0.015391 |\n",
      "| Sparse :   0.000041 |\n",
      "Step 800: Minibatch Loss: 0.015480 Error: 0.015432 Sparse: 0.000048\n",
      "| Loss :   0.015432 |\n",
      "| Sparse :   0.000048 |\n",
      "Step 900: Minibatch Loss: 0.015450 Error: 0.015407 Sparse: 0.000043\n",
      "| Loss :   0.015407 |\n",
      "| Sparse :   0.000043 |\n",
      "Step 1000: Minibatch Loss: 0.015323 Error: 0.015042 Sparse: 0.000281\n",
      "| Loss :   0.015042 |\n",
      "| Sparse :   0.000281 |\n",
      "Step 1100: Minibatch Loss: 0.015063 Error: 0.014585 Sparse: 0.000478\n",
      "| Loss :   0.014585 |\n",
      "| Sparse :   0.000478 |\n",
      "Step 1200: Minibatch Loss: 0.014896 Error: 0.014398 Sparse: 0.000498\n",
      "| Loss :   0.014398 |\n",
      "| Sparse :   0.000498 |\n",
      "Step 1300: Minibatch Loss: 0.014730 Error: 0.014166 Sparse: 0.000564\n",
      "| Loss :   0.014166 |\n",
      "| Sparse :   0.000564 |\n",
      "Step 1400: Minibatch Loss: 0.014499 Error: 0.013860 Sparse: 0.000639\n",
      "| Loss :   0.013860 |\n",
      "| Sparse :   0.000639 |\n",
      "Step 1500: Minibatch Loss: 0.014489 Error: 0.013848 Sparse: 0.000641\n",
      "| Loss :   0.013848 |\n",
      "| Sparse :   0.000641 |\n",
      "Step 1600: Minibatch Loss: 0.014328 Error: 0.013667 Sparse: 0.000660\n",
      "| Loss :   0.013667 |\n",
      "| Sparse :   0.000660 |\n",
      "Step 1700: Minibatch Loss: 0.014297 Error: 0.013638 Sparse: 0.000659\n",
      "| Loss :   0.013638 |\n",
      "| Sparse :   0.000659 |\n",
      "Step 1800: Minibatch Loss: 0.014087 Error: 0.013226 Sparse: 0.000862\n",
      "| Loss :   0.013226 |\n",
      "| Sparse :   0.000862 |\n",
      "Step 1900: Minibatch Loss: 0.013661 Error: 0.012539 Sparse: 0.001122\n",
      "| Loss :   0.012539 |\n",
      "| Sparse :   0.001122 |\n",
      "Step 2000: Minibatch Loss: 0.013202 Error: 0.011744 Sparse: 0.001458\n",
      "| Loss :   0.011744 |\n",
      "| Sparse :   0.001458 |\n",
      "This run of SparceLinAutoEncoderRELU8 ran for 0:00:32 and logs are available locally at: C:\\Users\\cjhamilton4176\\.hyperdash\\logs\\sparcelinautoencoderrelu8\\sparcelinautoencoderrelu8_2018-11-07t12-00-46-707612.log\n",
      "Step 1: Minibatch Loss: 0.015406 Error: 0.015406 Sparse: 0.000393\n",
      "| Loss :   0.015406 |\n",
      "| Sparse :   0.000393 |\n",
      "Step 100: Minibatch Loss: 0.010169 Error: 0.010165 Sparse: 0.039747\n",
      "| Loss :   0.010165 |\n",
      "| Sparse :   0.039747 |\n",
      "Step 200: Minibatch Loss: 0.006900 Error: 0.006895 Sparse: 0.050835\n",
      "| Loss :   0.006895 |\n",
      "| Sparse :   0.050835 |\n",
      "Step 300: Minibatch Loss: 0.005482 Error: 0.005476 Sparse: 0.055193\n",
      "| Loss :   0.005476 |\n",
      "| Sparse :   0.055193 |\n",
      "Step 400: Minibatch Loss: 0.004552 Error: 0.004546 Sparse: 0.058325\n",
      "| Loss :   0.004546 |\n",
      "| Sparse :   0.058325 |\n",
      "Step 500: Minibatch Loss: 0.003904 Error: 0.003898 Sparse: 0.059634\n",
      "| Loss :   0.003898 |\n",
      "| Sparse :   0.059634 |\n",
      "Step 600: Minibatch Loss: 0.003346 Error: 0.003340 Sparse: 0.060570\n",
      "| Loss :   0.003340 |\n",
      "| Sparse :   0.060570 |\n",
      "Step 700: Minibatch Loss: 0.002966 Error: 0.002960 Sparse: 0.061211\n",
      "| Loss :   0.002960 |\n",
      "| Sparse :   0.061211 |\n",
      "Step 800: Minibatch Loss: 0.002733 Error: 0.002727 Sparse: 0.061504\n",
      "| Loss :   0.002727 |\n",
      "| Sparse :   0.061504 |\n",
      "Step 900: Minibatch Loss: 0.002548 Error: 0.002542 Sparse: 0.061341\n",
      "| Loss :   0.002542 |\n",
      "| Sparse :   0.061341 |\n",
      "Step 1000: Minibatch Loss: 0.002439 Error: 0.002433 Sparse: 0.061220\n",
      "| Loss :   0.002433 |\n",
      "| Sparse :   0.061220 |\n",
      "Step 1100: Minibatch Loss: 0.002309 Error: 0.002303 Sparse: 0.060790\n",
      "| Loss :   0.002303 |\n",
      "| Sparse :   0.060790 |\n",
      "Step 1200: Minibatch Loss: 0.002258 Error: 0.002252 Sparse: 0.060564\n",
      "| Loss :   0.002252 |\n",
      "| Sparse :   0.060564 |\n",
      "Step 1300: Minibatch Loss: 0.002233 Error: 0.002227 Sparse: 0.060189\n",
      "| Loss :   0.002227 |\n",
      "| Sparse :   0.060189 |\n",
      "Step 1400: Minibatch Loss: 0.002195 Error: 0.002189 Sparse: 0.060244\n",
      "| Loss :   0.002189 |\n",
      "| Sparse :   0.060244 |\n",
      "Step 1500: Minibatch Loss: 0.002104 Error: 0.002098 Sparse: 0.059859\n",
      "| Loss :   0.002098 |\n",
      "| Sparse :   0.059859 |\n",
      "Step 1600: Minibatch Loss: 0.002084 Error: 0.002078 Sparse: 0.059900\n",
      "| Loss :   0.002078 |\n",
      "| Sparse :   0.059900 |\n",
      "Step 1700: Minibatch Loss: 0.002046 Error: 0.002040 Sparse: 0.059721\n",
      "| Loss :   0.002040 |\n",
      "| Sparse :   0.059721 |\n",
      "Step 1800: Minibatch Loss: 0.002044 Error: 0.002038 Sparse: 0.059764\n",
      "| Loss :   0.002038 |\n",
      "| Sparse :   0.059764 |\n",
      "Step 1900: Minibatch Loss: 0.001988 Error: 0.001982 Sparse: 0.059712\n",
      "| Loss :   0.001982 |\n",
      "| Sparse :   0.059712 |\n",
      "Step 2000: Minibatch Loss: 0.001991 Error: 0.001985 Sparse: 0.059691\n",
      "| Loss :   0.001985 |\n",
      "| Sparse :   0.059691 |\n",
      "Step 2100: Minibatch Loss: 0.001978 Error: 0.001972 Sparse: 0.059736\n",
      "| Loss :   0.001972 |\n",
      "| Sparse :   0.059736 |\n",
      "Step 2200: Minibatch Loss: 0.001928 Error: 0.001922 Sparse: 0.059676\n",
      "| Loss :   0.001922 |\n",
      "| Sparse :   0.059676 |\n",
      "Step 2300: Minibatch Loss: 0.001919 Error: 0.001913 Sparse: 0.059735\n",
      "| Loss :   0.001913 |\n",
      "| Sparse :   0.059735 |\n",
      "Step 2400: Minibatch Loss: 0.001908 Error: 0.001902 Sparse: 0.059652\n",
      "| Loss :   0.001902 |\n",
      "| Sparse :   0.059652 |\n",
      "Step 2500: Minibatch Loss: 0.001899 Error: 0.001893 Sparse: 0.059401\n",
      "| Loss :   0.001893 |\n",
      "| Sparse :   0.059401 |\n",
      "Step 2600: Minibatch Loss: 0.001884 Error: 0.001879 Sparse: 0.059310\n",
      "| Loss :   0.001879 |\n",
      "| Sparse :   0.059310 |\n",
      "Step 2700: Minibatch Loss: 0.001910 Error: 0.001905 Sparse: 0.059336\n",
      "| Loss :   0.001905 |\n",
      "| Sparse :   0.059336 |\n",
      "Step 2800: Minibatch Loss: 0.001905 Error: 0.001899 Sparse: 0.059352\n",
      "| Loss :   0.001899 |\n",
      "| Sparse :   0.059352 |\n",
      "Step 2900: Minibatch Loss: 0.001868 Error: 0.001862 Sparse: 0.059411\n",
      "| Loss :   0.001862 |\n",
      "| Sparse :   0.059411 |\n",
      "Step 3000: Minibatch Loss: 0.001887 Error: 0.001881 Sparse: 0.059106\n",
      "| Loss :   0.001881 |\n",
      "| Sparse :   0.059106 |\n",
      "This run of SparceLinAutoEncoderRELU8 ran for 0:00:47 and logs are available locally at: C:\\Users\\cjhamilton4176\\.hyperdash\\logs\\sparcelinautoencoderrelu8\\sparcelinautoencoderrelu8_2018-11-07t12-01-19-702947.log\n",
      "Step 1: Minibatch Loss: 0.015397 Error: 0.015397 Sparse: 0.000403\n",
      "| Loss :   0.015397 |\n",
      "| Sparse :   0.000403 |\n",
      "Step 100: Minibatch Loss: 0.010147 Error: 0.010108 Sparse: 0.039450\n",
      "| Loss :   0.010108 |\n",
      "| Sparse :   0.039450 |\n",
      "Step 200: Minibatch Loss: 0.007093 Error: 0.007043 Sparse: 0.049799\n",
      "| Loss :   0.007043 |\n",
      "| Sparse :   0.049799 |\n",
      "Step 300: Minibatch Loss: 0.005484 Error: 0.005430 Sparse: 0.054636\n",
      "| Loss :   0.005430 |\n",
      "| Sparse :   0.054636 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 400: Minibatch Loss: 0.004493 Error: 0.004435 Sparse: 0.057600\n",
      "| Loss :   0.004435 |\n",
      "| Sparse :   0.057600 |\n",
      "Step 500: Minibatch Loss: 0.003810 Error: 0.003751 Sparse: 0.059413\n",
      "| Loss :   0.003751 |\n",
      "| Sparse :   0.059413 |\n",
      "Step 600: Minibatch Loss: 0.003341 Error: 0.003281 Sparse: 0.060120\n",
      "| Loss :   0.003281 |\n",
      "| Sparse :   0.060120 |\n",
      "Step 700: Minibatch Loss: 0.003021 Error: 0.002960 Sparse: 0.060954\n",
      "| Loss :   0.002960 |\n",
      "| Sparse :   0.060954 |\n",
      "Step 800: Minibatch Loss: 0.002762 Error: 0.002701 Sparse: 0.061085\n",
      "| Loss :   0.002701 |\n",
      "| Sparse :   0.061085 |\n",
      "Step 900: Minibatch Loss: 0.002609 Error: 0.002548 Sparse: 0.060927\n",
      "| Loss :   0.002548 |\n",
      "| Sparse :   0.060927 |\n",
      "Step 1000: Minibatch Loss: 0.002519 Error: 0.002458 Sparse: 0.060504\n",
      "| Loss :   0.002458 |\n",
      "| Sparse :   0.060504 |\n",
      "Step 1100: Minibatch Loss: 0.002410 Error: 0.002350 Sparse: 0.059647\n",
      "| Loss :   0.002350 |\n",
      "| Sparse :   0.059647 |\n",
      "Step 1200: Minibatch Loss: 0.002292 Error: 0.002233 Sparse: 0.059304\n",
      "| Loss :   0.002233 |\n",
      "| Sparse :   0.059304 |\n",
      "Step 1300: Minibatch Loss: 0.002294 Error: 0.002235 Sparse: 0.058707\n",
      "| Loss :   0.002235 |\n",
      "| Sparse :   0.058707 |\n",
      "Step 1400: Minibatch Loss: 0.002221 Error: 0.002162 Sparse: 0.058294\n",
      "| Loss :   0.002162 |\n",
      "| Sparse :   0.058294 |\n",
      "Step 1500: Minibatch Loss: 0.002175 Error: 0.002117 Sparse: 0.057576\n",
      "| Loss :   0.002117 |\n",
      "| Sparse :   0.057576 |\n",
      "Step 1600: Minibatch Loss: 0.002113 Error: 0.002056 Sparse: 0.057321\n",
      "| Loss :   0.002056 |\n",
      "| Sparse :   0.057321 |\n",
      "Step 1700: Minibatch Loss: 0.002139 Error: 0.002082 Sparse: 0.056975\n",
      "| Loss :   0.002082 |\n",
      "| Sparse :   0.056975 |\n",
      "Step 1800: Minibatch Loss: 0.002061 Error: 0.002004 Sparse: 0.056587\n",
      "| Loss :   0.002004 |\n",
      "| Sparse :   0.056587 |\n",
      "Step 1900: Minibatch Loss: 0.002033 Error: 0.001977 Sparse: 0.056267\n",
      "| Loss :   0.001977 |\n",
      "| Sparse :   0.056267 |\n",
      "Step 2000: Minibatch Loss: 0.002059 Error: 0.002003 Sparse: 0.055979\n",
      "| Loss :   0.002003 |\n",
      "| Sparse :   0.055979 |\n",
      "Step 2100: Minibatch Loss: 0.001970 Error: 0.001915 Sparse: 0.055458\n",
      "| Loss :   0.001915 |\n",
      "| Sparse :   0.055458 |\n",
      "Step 2200: Minibatch Loss: 0.001996 Error: 0.001941 Sparse: 0.054981\n",
      "| Loss :   0.001941 |\n",
      "| Sparse :   0.054981 |\n",
      "Step 2300: Minibatch Loss: 0.002016 Error: 0.001961 Sparse: 0.054700\n",
      "| Loss :   0.001961 |\n",
      "| Sparse :   0.054700 |\n",
      "Step 2400: Minibatch Loss: 0.001981 Error: 0.001926 Sparse: 0.054256\n",
      "| Loss :   0.001926 |\n",
      "| Sparse :   0.054256 |\n",
      "Step 2500: Minibatch Loss: 0.001966 Error: 0.001912 Sparse: 0.053900\n",
      "| Loss :   0.001912 |\n",
      "| Sparse :   0.053900 |\n",
      "Step 2600: Minibatch Loss: 0.001950 Error: 0.001897 Sparse: 0.053583\n",
      "| Loss :   0.001897 |\n",
      "| Sparse :   0.053583 |\n",
      "Step 2700: Minibatch Loss: 0.001947 Error: 0.001894 Sparse: 0.052967\n",
      "| Loss :   0.001894 |\n",
      "| Sparse :   0.052967 |\n",
      "Step 2800: Minibatch Loss: 0.001971 Error: 0.001919 Sparse: 0.052718\n",
      "| Loss :   0.001919 |\n",
      "| Sparse :   0.052718 |\n",
      "Step 2900: Minibatch Loss: 0.002002 Error: 0.001950 Sparse: 0.052130\n",
      "| Loss :   0.001950 |\n",
      "| Sparse :   0.052130 |\n",
      "Step 3000: Minibatch Loss: 0.001943 Error: 0.001891 Sparse: 0.052014\n",
      "| Loss :   0.001891 |\n",
      "| Sparse :   0.052014 |\n",
      "This run of SparceLinAutoEncoderRELU8 ran for 0:00:47 and logs are available locally at: C:\\Users\\cjhamilton4176\\.hyperdash\\logs\\sparcelinautoencoderrelu8\\sparcelinautoencoderrelu8_2018-11-07t12-02-08-174544.log\n",
      "Step 1: Minibatch Loss: 0.015410 Error: 0.015406 Sparse: 0.000401\n",
      "| Loss :   0.015406 |\n",
      "| Sparse :   0.000401 |\n",
      "Step 100: Minibatch Loss: 0.010673 Error: 0.010317 Sparse: 0.035603\n",
      "| Loss :   0.010317 |\n",
      "| Sparse :   0.035603 |\n",
      "Step 200: Minibatch Loss: 0.007208 Error: 0.006751 Sparse: 0.045677\n",
      "| Loss :   0.006751 |\n",
      "| Sparse :   0.045677 |\n",
      "Step 300: Minibatch Loss: 0.005871 Error: 0.005389 Sparse: 0.048254\n",
      "| Loss :   0.005389 |\n",
      "| Sparse :   0.048254 |\n",
      "Step 400: Minibatch Loss: 0.004819 Error: 0.004322 Sparse: 0.049782\n",
      "| Loss :   0.004322 |\n",
      "| Sparse :   0.049782 |\n",
      "Step 500: Minibatch Loss: 0.004206 Error: 0.003704 Sparse: 0.050185\n",
      "| Loss :   0.003704 |\n",
      "| Sparse :   0.050185 |\n",
      "Step 600: Minibatch Loss: 0.003682 Error: 0.003179 Sparse: 0.050281\n",
      "| Loss :   0.003179 |\n",
      "| Sparse :   0.050281 |\n",
      "Step 700: Minibatch Loss: 0.003328 Error: 0.002826 Sparse: 0.050139\n",
      "| Loss :   0.002826 |\n",
      "| Sparse :   0.050139 |\n",
      "Step 800: Minibatch Loss: 0.003091 Error: 0.002597 Sparse: 0.049470\n",
      "| Loss :   0.002597 |\n",
      "| Sparse :   0.049470 |\n",
      "Step 900: Minibatch Loss: 0.002947 Error: 0.002460 Sparse: 0.048633\n",
      "| Loss :   0.002460 |\n",
      "| Sparse :   0.048633 |\n",
      "Step 1000: Minibatch Loss: 0.002824 Error: 0.002350 Sparse: 0.047391\n",
      "| Loss :   0.002350 |\n",
      "| Sparse :   0.047391 |\n",
      "Step 1100: Minibatch Loss: 0.002736 Error: 0.002272 Sparse: 0.046351\n",
      "| Loss :   0.002272 |\n",
      "| Sparse :   0.046351 |\n",
      "Step 1200: Minibatch Loss: 0.002684 Error: 0.002232 Sparse: 0.045206\n",
      "| Loss :   0.002232 |\n",
      "| Sparse :   0.045206 |\n",
      "Step 1300: Minibatch Loss: 0.002631 Error: 0.002189 Sparse: 0.044126\n",
      "| Loss :   0.002189 |\n",
      "| Sparse :   0.044126 |\n",
      "Step 1400: Minibatch Loss: 0.002566 Error: 0.002136 Sparse: 0.042991\n",
      "| Loss :   0.002136 |\n",
      "| Sparse :   0.042991 |\n",
      "Step 1500: Minibatch Loss: 0.002588 Error: 0.002168 Sparse: 0.041999\n",
      "| Loss :   0.002168 |\n",
      "| Sparse :   0.041999 |\n",
      "Step 1600: Minibatch Loss: 0.002501 Error: 0.002093 Sparse: 0.040785\n",
      "| Loss :   0.002093 |\n",
      "| Sparse :   0.040785 |\n",
      "Step 1700: Minibatch Loss: 0.002445 Error: 0.002047 Sparse: 0.039702\n",
      "| Loss :   0.002047 |\n",
      "| Sparse :   0.039702 |\n",
      "Step 1800: Minibatch Loss: 0.002411 Error: 0.002024 Sparse: 0.038681\n",
      "| Loss :   0.002024 |\n",
      "| Sparse :   0.038681 |\n",
      "Step 1900: Minibatch Loss: 0.002404 Error: 0.002025 Sparse: 0.037912\n",
      "| Loss :   0.002025 |\n",
      "| Sparse :   0.037912 |\n",
      "Step 2000: Minibatch Loss: 0.002352 Error: 0.001983 Sparse: 0.036864\n",
      "| Loss :   0.001983 |\n",
      "| Sparse :   0.036864 |\n",
      "Step 2100: Minibatch Loss: 0.002351 Error: 0.001993 Sparse: 0.035813\n",
      "| Loss :   0.001993 |\n",
      "| Sparse :   0.035813 |\n",
      "Step 2200: Minibatch Loss: 0.002348 Error: 0.001996 Sparse: 0.035207\n",
      "| Loss :   0.001996 |\n",
      "| Sparse :   0.035207 |\n",
      "Step 2300: Minibatch Loss: 0.002327 Error: 0.001984 Sparse: 0.034337\n",
      "| Loss :   0.001984 |\n",
      "| Sparse :   0.034337 |\n",
      "Step 2400: Minibatch Loss: 0.002286 Error: 0.001950 Sparse: 0.033539\n",
      "| Loss :   0.001950 |\n",
      "| Sparse :   0.033539 |\n",
      "Step 2500: Minibatch Loss: 0.002325 Error: 0.001996 Sparse: 0.032887\n",
      "| Loss :   0.001996 |\n",
      "| Sparse :   0.032887 |\n",
      "Step 2600: Minibatch Loss: 0.002280 Error: 0.001960 Sparse: 0.031951\n",
      "| Loss :   0.001960 |\n",
      "| Sparse :   0.031951 |\n",
      "Step 2700: Minibatch Loss: 0.002239 Error: 0.001926 Sparse: 0.031356\n",
      "| Loss :   0.001926 |\n",
      "| Sparse :   0.031356 |\n",
      "Step 2800: Minibatch Loss: 0.002259 Error: 0.001952 Sparse: 0.030680\n",
      "| Loss :   0.001952 |\n",
      "| Sparse :   0.030680 |\n",
      "Step 2900: Minibatch Loss: 0.002248 Error: 0.001947 Sparse: 0.030151\n",
      "| Loss :   0.001947 |\n",
      "| Sparse :   0.030151 |\n",
      "Step 3000: Minibatch Loss: 0.002220 Error: 0.001928 Sparse: 0.029257\n",
      "| Loss :   0.001928 |\n",
      "| Sparse :   0.029257 |\n",
      "This run of SparceLinAutoEncoderRELU8 ran for 0:00:48 and logs are available locally at: C:\\Users\\cjhamilton4176\\.hyperdash\\logs\\sparcelinautoencoderrelu8\\sparcelinautoencoderrelu8_2018-11-07t12-02-55-886515.log\n",
      "Step 1: Minibatch Loss: 0.015425 Error: 0.015384 Sparse: 0.000405\n",
      "| Loss :   0.015384 |\n",
      "| Sparse :   0.000405 |\n",
      "Step 100: Minibatch Loss: 0.015460 Error: 0.015456 Sparse: 0.000037\n",
      "| Loss :   0.015456 |\n",
      "| Sparse :   0.000037 |\n",
      "Step 200: Minibatch Loss: 0.014559 Error: 0.014145 Sparse: 0.004143\n",
      "| Loss :   0.014145 |\n",
      "| Sparse :   0.004143 |\n",
      "Step 300: Minibatch Loss: 0.011844 Error: 0.010791 Sparse: 0.010532\n",
      "| Loss :   0.010791 |\n",
      "| Sparse :   0.010532 |\n",
      "Step 400: Minibatch Loss: 0.010073 Error: 0.008786 Sparse: 0.012871\n",
      "| Loss :   0.008786 |\n",
      "| Sparse :   0.012871 |\n",
      "Step 500: Minibatch Loss: 0.008755 Error: 0.007248 Sparse: 0.015073\n",
      "| Loss :   0.007248 |\n",
      "| Sparse :   0.015073 |\n",
      "Step 600: Minibatch Loss: 0.008167 Error: 0.006676 Sparse: 0.014906\n",
      "| Loss :   0.006676 |\n",
      "| Sparse :   0.014906 |\n",
      "Step 700: Minibatch Loss: 0.007931 Error: 0.006486 Sparse: 0.014449\n",
      "| Loss :   0.006486 |\n",
      "| Sparse :   0.014449 |\n",
      "Step 800: Minibatch Loss: 0.007616 Error: 0.006227 Sparse: 0.013898\n",
      "| Loss :   0.006227 |\n",
      "| Sparse :   0.013898 |\n",
      "Step 900: Minibatch Loss: 0.007478 Error: 0.006147 Sparse: 0.013309\n",
      "| Loss :   0.006147 |\n",
      "| Sparse :   0.013309 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000: Minibatch Loss: 0.007217 Error: 0.005911 Sparse: 0.013066\n",
      "| Loss :   0.005911 |\n",
      "| Sparse :   0.013066 |\n",
      "Step 1100: Minibatch Loss: 0.006820 Error: 0.005453 Sparse: 0.013670\n",
      "| Loss :   0.005453 |\n",
      "| Sparse :   0.013670 |\n",
      "Step 1200: Minibatch Loss: 0.006368 Error: 0.004957 Sparse: 0.014107\n",
      "| Loss :   0.004957 |\n",
      "| Sparse :   0.014107 |\n",
      "Step 1300: Minibatch Loss: 0.006073 Error: 0.004676 Sparse: 0.013973\n",
      "| Loss :   0.004676 |\n",
      "| Sparse :   0.013973 |\n",
      "Step 1400: Minibatch Loss: 0.005860 Error: 0.004498 Sparse: 0.013619\n",
      "| Loss :   0.004498 |\n",
      "| Sparse :   0.013619 |\n",
      "Step 1500: Minibatch Loss: 0.005706 Error: 0.004369 Sparse: 0.013363\n",
      "| Loss :   0.004369 |\n",
      "| Sparse :   0.013363 |\n",
      "Step 1600: Minibatch Loss: 0.005468 Error: 0.004134 Sparse: 0.013336\n",
      "| Loss :   0.004134 |\n",
      "| Sparse :   0.013336 |\n",
      "Step 1700: Minibatch Loss: 0.005250 Error: 0.003909 Sparse: 0.013408\n",
      "| Loss :   0.003909 |\n",
      "| Sparse :   0.013408 |\n",
      "Step 1800: Minibatch Loss: 0.004996 Error: 0.003661 Sparse: 0.013352\n",
      "| Loss :   0.003661 |\n",
      "| Sparse :   0.013352 |\n",
      "Step 1900: Minibatch Loss: 0.004790 Error: 0.003444 Sparse: 0.013455\n",
      "| Loss :   0.003444 |\n",
      "| Sparse :   0.013455 |\n",
      "Step 2000: Minibatch Loss: 0.004581 Error: 0.003242 Sparse: 0.013392\n",
      "| Loss :   0.003242 |\n",
      "| Sparse :   0.013392 |\n",
      "Step 2100: Minibatch Loss: 0.004458 Error: 0.003142 Sparse: 0.013152\n",
      "| Loss :   0.003142 |\n",
      "| Sparse :   0.013152 |\n",
      "Step 2200: Minibatch Loss: 0.004380 Error: 0.003087 Sparse: 0.012939\n",
      "| Loss :   0.003087 |\n",
      "| Sparse :   0.012939 |\n",
      "Step 2300: Minibatch Loss: 0.004201 Error: 0.002911 Sparse: 0.012903\n",
      "| Loss :   0.002911 |\n",
      "| Sparse :   0.012903 |\n",
      "Step 2400: Minibatch Loss: 0.004013 Error: 0.002705 Sparse: 0.013080\n",
      "| Loss :   0.002705 |\n",
      "| Sparse :   0.013080 |\n",
      "Step 2500: Minibatch Loss: 0.003880 Error: 0.002585 Sparse: 0.012957\n",
      "| Loss :   0.002585 |\n",
      "| Sparse :   0.012957 |\n",
      "Step 2600: Minibatch Loss: 0.003786 Error: 0.002502 Sparse: 0.012840\n",
      "| Loss :   0.002502 |\n",
      "| Sparse :   0.012840 |\n",
      "Step 2700: Minibatch Loss: 0.003661 Error: 0.002391 Sparse: 0.012701\n",
      "| Loss :   0.002391 |\n",
      "| Sparse :   0.012701 |\n",
      "Step 2800: Minibatch Loss: 0.003569 Error: 0.002323 Sparse: 0.012461\n",
      "| Loss :   0.002323 |\n",
      "| Sparse :   0.012461 |\n",
      "Step 2900: Minibatch Loss: 0.003491 Error: 0.002265 Sparse: 0.012254\n",
      "| Loss :   0.002265 |\n",
      "| Sparse :   0.012254 |\n",
      "Step 3000: Minibatch Loss: 0.003423 Error: 0.002219 Sparse: 0.012041\n",
      "| Loss :   0.002219 |\n",
      "| Sparse :   0.012041 |\n",
      "This run of SparceLinAutoEncoderRELU8 ran for 0:00:47 and logs are available locally at: C:\\Users\\cjhamilton4176\\.hyperdash\\logs\\sparcelinautoencoderrelu8\\sparcelinautoencoderrelu8_2018-11-07t12-03-45-022488.log\n",
      "Step 1: Minibatch Loss: 0.015834 Error: 0.015437 Sparse: 0.000397\n",
      "| Loss :   0.015437 |\n",
      "| Sparse :   0.000397 |\n",
      "Step 100: Minibatch Loss: 0.015436 Error: 0.015397 Sparse: 0.000039\n",
      "| Loss :   0.015397 |\n",
      "| Sparse :   0.000039 |\n",
      "Step 200: Minibatch Loss: 0.015474 Error: 0.015434 Sparse: 0.000040\n",
      "| Loss :   0.015434 |\n",
      "| Sparse :   0.000040 |\n",
      "Step 300: Minibatch Loss: 0.015428 Error: 0.015387 Sparse: 0.000040\n",
      "| Loss :   0.015387 |\n",
      "| Sparse :   0.000040 |\n",
      "Step 400: Minibatch Loss: 0.015450 Error: 0.015412 Sparse: 0.000038\n",
      "| Loss :   0.015412 |\n",
      "| Sparse :   0.000038 |\n",
      "Step 500: Minibatch Loss: 0.015446 Error: 0.015409 Sparse: 0.000038\n",
      "| Loss :   0.015409 |\n",
      "| Sparse :   0.000038 |\n",
      "Step 600: Minibatch Loss: 0.015449 Error: 0.015415 Sparse: 0.000034\n",
      "| Loss :   0.015415 |\n",
      "| Sparse :   0.000034 |\n",
      "Step 700: Minibatch Loss: 0.015455 Error: 0.015412 Sparse: 0.000043\n",
      "| Loss :   0.015412 |\n",
      "| Sparse :   0.000043 |\n",
      "Step 800: Minibatch Loss: 0.015466 Error: 0.015423 Sparse: 0.000043\n",
      "| Loss :   0.015423 |\n",
      "| Sparse :   0.000043 |\n",
      "Step 900: Minibatch Loss: 0.015394 Error: 0.015350 Sparse: 0.000044\n",
      "| Loss :   0.015350 |\n",
      "| Sparse :   0.000044 |\n",
      "Step 1000: Minibatch Loss: 0.015429 Error: 0.015356 Sparse: 0.000073\n",
      "| Loss :   0.015356 |\n",
      "| Sparse :   0.000073 |\n",
      "Step 1100: Minibatch Loss: 0.015081 Error: 0.014592 Sparse: 0.000489\n",
      "| Loss :   0.014592 |\n",
      "| Sparse :   0.000489 |\n",
      "Step 1200: Minibatch Loss: 0.014767 Error: 0.014135 Sparse: 0.000632\n",
      "| Loss :   0.014135 |\n",
      "| Sparse :   0.000632 |\n",
      "Step 1300: Minibatch Loss: 0.014518 Error: 0.013794 Sparse: 0.000724\n",
      "| Loss :   0.013794 |\n",
      "| Sparse :   0.000724 |\n",
      "Step 1400: Minibatch Loss: 0.014415 Error: 0.013688 Sparse: 0.000727\n",
      "| Loss :   0.013688 |\n",
      "| Sparse :   0.000727 |\n",
      "Step 1500: Minibatch Loss: 0.014362 Error: 0.013626 Sparse: 0.000735\n",
      "| Loss :   0.013626 |\n",
      "| Sparse :   0.000735 |\n",
      "Step 1600: Minibatch Loss: 0.014140 Error: 0.013288 Sparse: 0.000852\n",
      "| Loss :   0.013288 |\n",
      "| Sparse :   0.000852 |\n",
      "Step 1700: Minibatch Loss: 0.013830 Error: 0.012753 Sparse: 0.001077\n",
      "| Loss :   0.012753 |\n",
      "| Sparse :   0.001077 |\n",
      "Step 1800: Minibatch Loss: 0.013623 Error: 0.012532 Sparse: 0.001091\n",
      "| Loss :   0.012532 |\n",
      "| Sparse :   0.001091 |\n",
      "Step 1900: Minibatch Loss: 0.013528 Error: 0.012442 Sparse: 0.001086\n",
      "| Loss :   0.012442 |\n",
      "| Sparse :   0.001086 |\n",
      "Step 2000: Minibatch Loss: 0.013462 Error: 0.012378 Sparse: 0.001083\n",
      "| Loss :   0.012378 |\n",
      "| Sparse :   0.001083 |\n",
      "Step 2100: Minibatch Loss: 0.013335 Error: 0.012255 Sparse: 0.001080\n",
      "| Loss :   0.012255 |\n",
      "| Sparse :   0.001080 |\n",
      "Step 2200: Minibatch Loss: 0.013306 Error: 0.012247 Sparse: 0.001059\n",
      "| Loss :   0.012247 |\n",
      "| Sparse :   0.001059 |\n",
      "Step 2300: Minibatch Loss: 0.013242 Error: 0.012209 Sparse: 0.001033\n",
      "| Loss :   0.012209 |\n",
      "| Sparse :   0.001033 |\n",
      "Step 2400: Minibatch Loss: 0.013146 Error: 0.012117 Sparse: 0.001029\n",
      "| Loss :   0.012117 |\n",
      "| Sparse :   0.001029 |\n",
      "Step 2500: Minibatch Loss: 0.013002 Error: 0.011883 Sparse: 0.001118\n",
      "| Loss :   0.011883 |\n",
      "| Sparse :   0.001118 |\n",
      "Step 2600: Minibatch Loss: 0.012973 Error: 0.011866 Sparse: 0.001107\n",
      "| Loss :   0.011866 |\n",
      "| Sparse :   0.001107 |\n",
      "Step 2700: Minibatch Loss: 0.012868 Error: 0.011772 Sparse: 0.001096\n",
      "| Loss :   0.011772 |\n",
      "| Sparse :   0.001096 |\n",
      "Step 2800: Minibatch Loss: 0.012791 Error: 0.011707 Sparse: 0.001085\n",
      "| Loss :   0.011707 |\n",
      "| Sparse :   0.001085 |\n",
      "Step 2900: Minibatch Loss: 0.012715 Error: 0.011643 Sparse: 0.001072\n",
      "| Loss :   0.011643 |\n",
      "| Sparse :   0.001072 |\n",
      "Step 3000: Minibatch Loss: 0.012536 Error: 0.011253 Sparse: 0.001283\n",
      "| Loss :   0.011253 |\n",
      "| Sparse :   0.001283 |\n",
      "This run of SparceLinAutoEncoderRELU8 ran for 0:00:47 and logs are available locally at: C:\\Users\\cjhamilton4176\\.hyperdash\\logs\\sparcelinautoencoderrelu8\\sparcelinautoencoderrelu8_2018-11-07t12-04-32-900819.log\n",
      "Step 1: Minibatch Loss: 0.015422 Error: 0.015422 Sparse: 0.000404\n",
      "| Loss :   0.015422 |\n",
      "| Sparse :   0.000404 |\n",
      "Step 100: Minibatch Loss: 0.010081 Error: 0.010077 Sparse: 0.039602\n",
      "| Loss :   0.010077 |\n",
      "| Sparse :   0.039602 |\n",
      "Step 200: Minibatch Loss: 0.006853 Error: 0.006848 Sparse: 0.050795\n",
      "| Loss :   0.006848 |\n",
      "| Sparse :   0.050795 |\n",
      "Step 300: Minibatch Loss: 0.005466 Error: 0.005461 Sparse: 0.055587\n",
      "| Loss :   0.005461 |\n",
      "| Sparse :   0.055587 |\n",
      "Step 400: Minibatch Loss: 0.004468 Error: 0.004463 Sparse: 0.058610\n",
      "| Loss :   0.004463 |\n",
      "| Sparse :   0.058610 |\n",
      "Step 500: Minibatch Loss: 0.003860 Error: 0.003854 Sparse: 0.060361\n",
      "| Loss :   0.003854 |\n",
      "| Sparse :   0.060361 |\n",
      "Step 600: Minibatch Loss: 0.003344 Error: 0.003337 Sparse: 0.061665\n",
      "| Loss :   0.003337 |\n",
      "| Sparse :   0.061665 |\n",
      "Step 700: Minibatch Loss: 0.002955 Error: 0.002949 Sparse: 0.062705\n",
      "| Loss :   0.002949 |\n",
      "| Sparse :   0.062705 |\n",
      "Step 800: Minibatch Loss: 0.002740 Error: 0.002733 Sparse: 0.063198\n",
      "| Loss :   0.002733 |\n",
      "| Sparse :   0.063198 |\n",
      "Step 900: Minibatch Loss: 0.002569 Error: 0.002563 Sparse: 0.063259\n",
      "| Loss :   0.002563 |\n",
      "| Sparse :   0.063259 |\n",
      "Step 1000: Minibatch Loss: 0.002447 Error: 0.002441 Sparse: 0.062511\n",
      "| Loss :   0.002441 |\n",
      "| Sparse :   0.062511 |\n",
      "Step 1100: Minibatch Loss: 0.002357 Error: 0.002351 Sparse: 0.061796\n",
      "| Loss :   0.002351 |\n",
      "| Sparse :   0.061796 |\n",
      "Step 1200: Minibatch Loss: 0.002308 Error: 0.002302 Sparse: 0.061309\n",
      "| Loss :   0.002302 |\n",
      "| Sparse :   0.061309 |\n",
      "Step 1300: Minibatch Loss: 0.002222 Error: 0.002216 Sparse: 0.060751\n",
      "| Loss :   0.002216 |\n",
      "| Sparse :   0.060751 |\n",
      "Step 1400: Minibatch Loss: 0.002151 Error: 0.002145 Sparse: 0.060319\n",
      "| Loss :   0.002145 |\n",
      "| Sparse :   0.060319 |\n",
      "Step 1500: Minibatch Loss: 0.002145 Error: 0.002139 Sparse: 0.060352\n",
      "| Loss :   0.002139 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Sparse :   0.060352 |\n",
      "Step 1600: Minibatch Loss: 0.002077 Error: 0.002071 Sparse: 0.059956\n",
      "| Loss :   0.002071 |\n",
      "| Sparse :   0.059956 |\n",
      "Step 1700: Minibatch Loss: 0.002060 Error: 0.002054 Sparse: 0.059876\n",
      "| Loss :   0.002054 |\n",
      "| Sparse :   0.059876 |\n",
      "Step 1800: Minibatch Loss: 0.002049 Error: 0.002043 Sparse: 0.059693\n",
      "| Loss :   0.002043 |\n",
      "| Sparse :   0.059693 |\n",
      "Step 1900: Minibatch Loss: 0.002016 Error: 0.002010 Sparse: 0.059817\n",
      "| Loss :   0.002010 |\n",
      "| Sparse :   0.059817 |\n",
      "Step 2000: Minibatch Loss: 0.001991 Error: 0.001985 Sparse: 0.059810\n",
      "| Loss :   0.001985 |\n",
      "| Sparse :   0.059810 |\n",
      "Step 2100: Minibatch Loss: 0.001974 Error: 0.001968 Sparse: 0.059828\n",
      "| Loss :   0.001968 |\n",
      "| Sparse :   0.059828 |\n",
      "Step 2200: Minibatch Loss: 0.001963 Error: 0.001957 Sparse: 0.059698\n",
      "| Loss :   0.001957 |\n",
      "| Sparse :   0.059698 |\n",
      "Step 2300: Minibatch Loss: 0.001966 Error: 0.001960 Sparse: 0.059601\n",
      "| Loss :   0.001960 |\n",
      "| Sparse :   0.059601 |\n",
      "Step 2400: Minibatch Loss: 0.001928 Error: 0.001922 Sparse: 0.059417\n",
      "| Loss :   0.001922 |\n",
      "| Sparse :   0.059417 |\n",
      "Step 2500: Minibatch Loss: 0.001905 Error: 0.001899 Sparse: 0.059630\n",
      "| Loss :   0.001899 |\n",
      "| Sparse :   0.059630 |\n",
      "Step 2600: Minibatch Loss: 0.001919 Error: 0.001913 Sparse: 0.059661\n",
      "| Loss :   0.001913 |\n",
      "| Sparse :   0.059661 |\n",
      "Step 2700: Minibatch Loss: 0.001890 Error: 0.001884 Sparse: 0.059513\n",
      "| Loss :   0.001884 |\n",
      "| Sparse :   0.059513 |\n",
      "Step 2800: Minibatch Loss: 0.001912 Error: 0.001906 Sparse: 0.059422\n",
      "| Loss :   0.001906 |\n",
      "| Sparse :   0.059422 |\n",
      "Step 2900: Minibatch Loss: 0.001895 Error: 0.001889 Sparse: 0.059389\n",
      "| Loss :   0.001889 |\n",
      "| Sparse :   0.059389 |\n",
      "Step 3000: Minibatch Loss: 0.001905 Error: 0.001899 Sparse: 0.059101\n",
      "| Loss :   0.001899 |\n",
      "| Sparse :   0.059101 |\n",
      "Step 3100: Minibatch Loss: 0.001878 Error: 0.001872 Sparse: 0.059120\n",
      "| Loss :   0.001872 |\n",
      "| Sparse :   0.059120 |\n",
      "Step 3200: Minibatch Loss: 0.001881 Error: 0.001875 Sparse: 0.058796\n",
      "| Loss :   0.001875 |\n",
      "| Sparse :   0.058796 |\n",
      "Step 3300: Minibatch Loss: 0.001900 Error: 0.001894 Sparse: 0.058930\n",
      "| Loss :   0.001894 |\n",
      "| Sparse :   0.058930 |\n",
      "Step 3400: Minibatch Loss: 0.001880 Error: 0.001874 Sparse: 0.058661\n",
      "| Loss :   0.001874 |\n",
      "| Sparse :   0.058661 |\n",
      "Step 3500: Minibatch Loss: 0.001897 Error: 0.001891 Sparse: 0.058730\n",
      "| Loss :   0.001891 |\n",
      "| Sparse :   0.058730 |\n",
      "Step 3600: Minibatch Loss: 0.001872 Error: 0.001866 Sparse: 0.058522\n",
      "| Loss :   0.001866 |\n",
      "| Sparse :   0.058522 |\n",
      "Step 3700: Minibatch Loss: 0.001870 Error: 0.001864 Sparse: 0.058607\n",
      "| Loss :   0.001864 |\n",
      "| Sparse :   0.058607 |\n",
      "Step 3800: Minibatch Loss: 0.001865 Error: 0.001859 Sparse: 0.058314\n",
      "| Loss :   0.001859 |\n",
      "| Sparse :   0.058314 |\n",
      "Step 3900: Minibatch Loss: 0.001883 Error: 0.001877 Sparse: 0.058171\n",
      "| Loss :   0.001877 |\n",
      "| Sparse :   0.058171 |\n",
      "Step 4000: Minibatch Loss: 0.001867 Error: 0.001862 Sparse: 0.058076\n",
      "| Loss :   0.001862 |\n",
      "| Sparse :   0.058076 |\n",
      "This run of SparceLinAutoEncoderRELU8 ran for 0:01:02 and logs are available locally at: C:\\Users\\cjhamilton4176\\.hyperdash\\logs\\sparcelinautoencoderrelu8\\sparcelinautoencoderrelu8_2018-11-07t12-05-20-969014.log\n",
      "Step 1: Minibatch Loss: 0.015435 Error: 0.015434 Sparse: 0.000403\n",
      "| Loss :   0.015434 |\n",
      "| Sparse :   0.000403 |\n",
      "Step 100: Minibatch Loss: 0.010039 Error: 0.009999 Sparse: 0.039468\n",
      "| Loss :   0.009999 |\n",
      "| Sparse :   0.039468 |\n",
      "Step 200: Minibatch Loss: 0.006711 Error: 0.006660 Sparse: 0.051045\n",
      "| Loss :   0.006660 |\n",
      "| Sparse :   0.051045 |\n",
      "Step 300: Minibatch Loss: 0.005361 Error: 0.005306 Sparse: 0.054898\n",
      "| Loss :   0.005306 |\n",
      "| Sparse :   0.054898 |\n",
      "Step 400: Minibatch Loss: 0.004438 Error: 0.004381 Sparse: 0.057575\n",
      "| Loss :   0.004381 |\n",
      "| Sparse :   0.057575 |\n",
      "Step 500: Minibatch Loss: 0.003757 Error: 0.003698 Sparse: 0.058723\n",
      "| Loss :   0.003698 |\n",
      "| Sparse :   0.058723 |\n",
      "Step 600: Minibatch Loss: 0.003269 Error: 0.003210 Sparse: 0.059852\n",
      "| Loss :   0.003210 |\n",
      "| Sparse :   0.059852 |\n",
      "Step 700: Minibatch Loss: 0.002869 Error: 0.002808 Sparse: 0.060507\n",
      "| Loss :   0.002808 |\n",
      "| Sparse :   0.060507 |\n",
      "Step 800: Minibatch Loss: 0.002667 Error: 0.002606 Sparse: 0.060530\n",
      "| Loss :   0.002606 |\n",
      "| Sparse :   0.060530 |\n",
      "Step 900: Minibatch Loss: 0.002516 Error: 0.002455 Sparse: 0.060204\n",
      "| Loss :   0.002455 |\n",
      "| Sparse :   0.060204 |\n",
      "Step 1000: Minibatch Loss: 0.002441 Error: 0.002381 Sparse: 0.059767\n",
      "| Loss :   0.002381 |\n",
      "| Sparse :   0.059767 |\n",
      "Step 1100: Minibatch Loss: 0.002347 Error: 0.002288 Sparse: 0.059075\n",
      "| Loss :   0.002288 |\n",
      "| Sparse :   0.059075 |\n",
      "Step 1200: Minibatch Loss: 0.002309 Error: 0.002251 Sparse: 0.058413\n",
      "| Loss :   0.002251 |\n",
      "| Sparse :   0.058413 |\n",
      "Step 1300: Minibatch Loss: 0.002214 Error: 0.002156 Sparse: 0.057809\n",
      "| Loss :   0.002156 |\n",
      "| Sparse :   0.057809 |\n",
      "Step 1400: Minibatch Loss: 0.002201 Error: 0.002143 Sparse: 0.057513\n",
      "| Loss :   0.002143 |\n",
      "| Sparse :   0.057513 |\n",
      "Step 1500: Minibatch Loss: 0.002139 Error: 0.002082 Sparse: 0.057216\n",
      "| Loss :   0.002082 |\n",
      "| Sparse :   0.057216 |\n",
      "Step 1600: Minibatch Loss: 0.002107 Error: 0.002050 Sparse: 0.056760\n",
      "| Loss :   0.002050 |\n",
      "| Sparse :   0.056760 |\n",
      "Step 1700: Minibatch Loss: 0.002058 Error: 0.002001 Sparse: 0.056501\n",
      "| Loss :   0.002001 |\n",
      "| Sparse :   0.056501 |\n",
      "Step 1800: Minibatch Loss: 0.002081 Error: 0.002025 Sparse: 0.056145\n",
      "| Loss :   0.002025 |\n",
      "| Sparse :   0.056145 |\n",
      "Step 1900: Minibatch Loss: 0.002035 Error: 0.001979 Sparse: 0.055710\n",
      "| Loss :   0.001979 |\n",
      "| Sparse :   0.055710 |\n",
      "Step 2000: Minibatch Loss: 0.002016 Error: 0.001960 Sparse: 0.055525\n",
      "| Loss :   0.001960 |\n",
      "| Sparse :   0.055525 |\n",
      "Step 2100: Minibatch Loss: 0.002053 Error: 0.001997 Sparse: 0.055485\n",
      "| Loss :   0.001997 |\n",
      "| Sparse :   0.055485 |\n",
      "Step 2200: Minibatch Loss: 0.002006 Error: 0.001951 Sparse: 0.054819\n",
      "| Loss :   0.001951 |\n",
      "| Sparse :   0.054819 |\n",
      "Step 2300: Minibatch Loss: 0.002016 Error: 0.001962 Sparse: 0.054371\n",
      "| Loss :   0.001962 |\n",
      "| Sparse :   0.054371 |\n",
      "Step 2400: Minibatch Loss: 0.002017 Error: 0.001962 Sparse: 0.054299\n",
      "| Loss :   0.001962 |\n",
      "| Sparse :   0.054299 |\n",
      "Step 2500: Minibatch Loss: 0.001969 Error: 0.001915 Sparse: 0.053849\n",
      "| Loss :   0.001915 |\n",
      "| Sparse :   0.053849 |\n",
      "Step 2600: Minibatch Loss: 0.001989 Error: 0.001936 Sparse: 0.053295\n",
      "| Loss :   0.001936 |\n",
      "| Sparse :   0.053295 |\n",
      "Step 2700: Minibatch Loss: 0.001957 Error: 0.001904 Sparse: 0.053053\n",
      "| Loss :   0.001904 |\n",
      "| Sparse :   0.053053 |\n",
      "Step 2800: Minibatch Loss: 0.001938 Error: 0.001885 Sparse: 0.052501\n",
      "| Loss :   0.001885 |\n",
      "| Sparse :   0.052501 |\n",
      "Step 2900: Minibatch Loss: 0.001983 Error: 0.001931 Sparse: 0.052244\n",
      "| Loss :   0.001931 |\n",
      "| Sparse :   0.052244 |\n",
      "Step 3000: Minibatch Loss: 0.001919 Error: 0.001867 Sparse: 0.051885\n",
      "| Loss :   0.001867 |\n",
      "| Sparse :   0.051885 |\n",
      "Step 3100: Minibatch Loss: 0.001943 Error: 0.001891 Sparse: 0.051342\n",
      "| Loss :   0.001891 |\n",
      "| Sparse :   0.051342 |\n",
      "Step 3200: Minibatch Loss: 0.001940 Error: 0.001889 Sparse: 0.050912\n",
      "| Loss :   0.001889 |\n",
      "| Sparse :   0.050912 |\n",
      "Step 3300: Minibatch Loss: 0.001910 Error: 0.001859 Sparse: 0.050718\n",
      "| Loss :   0.001859 |\n",
      "| Sparse :   0.050718 |\n",
      "Step 3400: Minibatch Loss: 0.001948 Error: 0.001898 Sparse: 0.050313\n",
      "| Loss :   0.001898 |\n",
      "| Sparse :   0.050313 |\n",
      "Step 3500: Minibatch Loss: 0.001927 Error: 0.001878 Sparse: 0.049743\n",
      "| Loss :   0.001878 |\n",
      "| Sparse :   0.049743 |\n",
      "Step 3600: Minibatch Loss: 0.001919 Error: 0.001869 Sparse: 0.049295\n",
      "| Loss :   0.001869 |\n",
      "| Sparse :   0.049295 |\n",
      "Step 3700: Minibatch Loss: 0.001922 Error: 0.001873 Sparse: 0.049148\n",
      "| Loss :   0.001873 |\n",
      "| Sparse :   0.049148 |\n",
      "Step 3800: Minibatch Loss: 0.001900 Error: 0.001851 Sparse: 0.048709\n",
      "| Loss :   0.001851 |\n",
      "| Sparse :   0.048709 |\n",
      "Step 3900: Minibatch Loss: 0.001883 Error: 0.001835 Sparse: 0.048252\n",
      "| Loss :   0.001835 |\n",
      "| Sparse :   0.048252 |\n",
      "Step 4000: Minibatch Loss: 0.001906 Error: 0.001858 Sparse: 0.048069\n",
      "| Loss :   0.001858 |\n",
      "| Sparse :   0.048069 |\n",
      "This run of SparceLinAutoEncoderRELU8 ran for 0:01:03 and logs are available locally at: C:\\Users\\cjhamilton4176\\.hyperdash\\logs\\sparcelinautoencoderrelu8\\sparcelinautoencoderrelu8_2018-11-07t12-06-24-057904.log\n",
      "Step 1: Minibatch Loss: 0.015376 Error: 0.015372 Sparse: 0.000375\n",
      "| Loss :   0.015372 |\n",
      "| Sparse :   0.000375 |\n",
      "Step 100: Minibatch Loss: 0.010726 Error: 0.010374 Sparse: 0.035272\n",
      "| Loss :   0.010374 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Sparse :   0.035272 |\n",
      "Step 200: Minibatch Loss: 0.007339 Error: 0.006887 Sparse: 0.045261\n",
      "| Loss :   0.006887 |\n",
      "| Sparse :   0.045261 |\n",
      "Step 300: Minibatch Loss: 0.005871 Error: 0.005392 Sparse: 0.047907\n",
      "| Loss :   0.005392 |\n",
      "| Sparse :   0.047907 |\n",
      "Step 400: Minibatch Loss: 0.005000 Error: 0.004505 Sparse: 0.049446\n",
      "| Loss :   0.004505 |\n",
      "| Sparse :   0.049446 |\n",
      "Step 500: Minibatch Loss: 0.004195 Error: 0.003696 Sparse: 0.049901\n",
      "| Loss :   0.003696 |\n",
      "| Sparse :   0.049901 |\n",
      "Step 600: Minibatch Loss: 0.003681 Error: 0.003181 Sparse: 0.050074\n",
      "| Loss :   0.003181 |\n",
      "| Sparse :   0.050074 |\n",
      "Step 700: Minibatch Loss: 0.003318 Error: 0.002819 Sparse: 0.049866\n",
      "| Loss :   0.002819 |\n",
      "| Sparse :   0.049866 |\n",
      "Step 800: Minibatch Loss: 0.003110 Error: 0.002619 Sparse: 0.049159\n",
      "| Loss :   0.002619 |\n",
      "| Sparse :   0.049159 |\n",
      "Step 900: Minibatch Loss: 0.002992 Error: 0.002509 Sparse: 0.048350\n",
      "| Loss :   0.002509 |\n",
      "| Sparse :   0.048350 |\n",
      "Step 1000: Minibatch Loss: 0.002808 Error: 0.002337 Sparse: 0.047026\n",
      "| Loss :   0.002337 |\n",
      "| Sparse :   0.047026 |\n",
      "Step 1100: Minibatch Loss: 0.002779 Error: 0.002318 Sparse: 0.046138\n",
      "| Loss :   0.002318 |\n",
      "| Sparse :   0.046138 |\n",
      "Step 1200: Minibatch Loss: 0.002668 Error: 0.002217 Sparse: 0.045043\n",
      "| Loss :   0.002217 |\n",
      "| Sparse :   0.045043 |\n",
      "Step 1300: Minibatch Loss: 0.002581 Error: 0.002143 Sparse: 0.043799\n",
      "| Loss :   0.002143 |\n",
      "| Sparse :   0.043799 |\n",
      "Step 1400: Minibatch Loss: 0.002568 Error: 0.002141 Sparse: 0.042718\n",
      "| Loss :   0.002141 |\n",
      "| Sparse :   0.042718 |\n",
      "Step 1500: Minibatch Loss: 0.002522 Error: 0.002105 Sparse: 0.041725\n",
      "| Loss :   0.002105 |\n",
      "| Sparse :   0.041725 |\n",
      "Step 1600: Minibatch Loss: 0.002504 Error: 0.002098 Sparse: 0.040541\n",
      "| Loss :   0.002098 |\n",
      "| Sparse :   0.040541 |\n",
      "Step 1700: Minibatch Loss: 0.002428 Error: 0.002032 Sparse: 0.039530\n",
      "| Loss :   0.002032 |\n",
      "| Sparse :   0.039530 |\n",
      "Step 1800: Minibatch Loss: 0.002445 Error: 0.002059 Sparse: 0.038606\n",
      "| Loss :   0.002059 |\n",
      "| Sparse :   0.038606 |\n",
      "Step 1900: Minibatch Loss: 0.002385 Error: 0.002010 Sparse: 0.037506\n",
      "| Loss :   0.002010 |\n",
      "| Sparse :   0.037506 |\n",
      "Step 2000: Minibatch Loss: 0.002377 Error: 0.002009 Sparse: 0.036831\n",
      "| Loss :   0.002009 |\n",
      "| Sparse :   0.036831 |\n",
      "Step 2100: Minibatch Loss: 0.002330 Error: 0.001973 Sparse: 0.035763\n",
      "| Loss :   0.001973 |\n",
      "| Sparse :   0.035763 |\n",
      "Step 2200: Minibatch Loss: 0.002335 Error: 0.001985 Sparse: 0.035017\n",
      "| Loss :   0.001985 |\n",
      "| Sparse :   0.035017 |\n",
      "Step 2300: Minibatch Loss: 0.002280 Error: 0.001938 Sparse: 0.034225\n",
      "| Loss :   0.001938 |\n",
      "| Sparse :   0.034225 |\n",
      "Step 2400: Minibatch Loss: 0.002334 Error: 0.002000 Sparse: 0.033374\n",
      "| Loss :   0.002000 |\n",
      "| Sparse :   0.033374 |\n",
      "Step 2500: Minibatch Loss: 0.002271 Error: 0.001944 Sparse: 0.032707\n",
      "| Loss :   0.001944 |\n",
      "| Sparse :   0.032707 |\n",
      "Step 2600: Minibatch Loss: 0.002214 Error: 0.001895 Sparse: 0.031900\n",
      "| Loss :   0.001895 |\n",
      "| Sparse :   0.031900 |\n",
      "Step 2700: Minibatch Loss: 0.002260 Error: 0.001947 Sparse: 0.031311\n",
      "| Loss :   0.001947 |\n",
      "| Sparse :   0.031311 |\n",
      "Step 2800: Minibatch Loss: 0.002194 Error: 0.001890 Sparse: 0.030445\n",
      "| Loss :   0.001890 |\n",
      "| Sparse :   0.030445 |\n",
      "Step 2900: Minibatch Loss: 0.002200 Error: 0.001901 Sparse: 0.029892\n",
      "| Loss :   0.001901 |\n",
      "| Sparse :   0.029892 |\n",
      "Step 3000: Minibatch Loss: 0.002197 Error: 0.001904 Sparse: 0.029311\n",
      "| Loss :   0.001904 |\n",
      "| Sparse :   0.029311 |\n",
      "Step 3100: Minibatch Loss: 0.002193 Error: 0.001906 Sparse: 0.028745\n",
      "| Loss :   0.001906 |\n",
      "| Sparse :   0.028745 |\n",
      "Step 3200: Minibatch Loss: 0.002169 Error: 0.001888 Sparse: 0.028126\n",
      "| Loss :   0.001888 |\n",
      "| Sparse :   0.028126 |\n",
      "Step 3300: Minibatch Loss: 0.002160 Error: 0.001884 Sparse: 0.027549\n",
      "| Loss :   0.001884 |\n",
      "| Sparse :   0.027549 |\n",
      "Step 3400: Minibatch Loss: 0.002175 Error: 0.001906 Sparse: 0.026923\n",
      "| Loss :   0.001906 |\n",
      "| Sparse :   0.026923 |\n",
      "Step 3500: Minibatch Loss: 0.002171 Error: 0.001906 Sparse: 0.026477\n",
      "| Loss :   0.001906 |\n",
      "| Sparse :   0.026477 |\n",
      "Step 3600: Minibatch Loss: 0.002161 Error: 0.001899 Sparse: 0.026139\n",
      "| Loss :   0.001899 |\n",
      "| Sparse :   0.026139 |\n",
      "Step 3700: Minibatch Loss: 0.002135 Error: 0.001878 Sparse: 0.025649\n",
      "| Loss :   0.001878 |\n",
      "| Sparse :   0.025649 |\n",
      "Step 3800: Minibatch Loss: 0.002131 Error: 0.001880 Sparse: 0.025062\n",
      "| Loss :   0.001880 |\n",
      "| Sparse :   0.025062 |\n",
      "Step 3900: Minibatch Loss: 0.002112 Error: 0.001866 Sparse: 0.024630\n",
      "| Loss :   0.001866 |\n",
      "| Sparse :   0.024630 |\n",
      "Step 4000: Minibatch Loss: 0.002151 Error: 0.001908 Sparse: 0.024231\n",
      "| Loss :   0.001908 |\n",
      "| Sparse :   0.024231 |\n",
      "This run of SparceLinAutoEncoderRELU8 ran for 0:01:03 and logs are available locally at: C:\\Users\\cjhamilton4176\\.hyperdash\\logs\\sparcelinautoencoderrelu8\\sparcelinautoencoderrelu8_2018-11-07t12-07-27-986722.log\n",
      "Step 1: Minibatch Loss: 0.015495 Error: 0.015456 Sparse: 0.000390\n",
      "| Loss :   0.015456 |\n",
      "| Sparse :   0.000390 |\n",
      "Step 100: Minibatch Loss: 0.015429 Error: 0.015425 Sparse: 0.000040\n",
      "| Loss :   0.015425 |\n",
      "| Sparse :   0.000040 |\n",
      "Step 200: Minibatch Loss: 0.014696 Error: 0.014291 Sparse: 0.004047\n",
      "| Loss :   0.014291 |\n",
      "| Sparse :   0.004047 |\n",
      "Step 300: Minibatch Loss: 0.011855 Error: 0.010736 Sparse: 0.011183\n",
      "| Loss :   0.010736 |\n",
      "| Sparse :   0.011183 |\n",
      "Step 400: Minibatch Loss: 0.010143 Error: 0.008785 Sparse: 0.013575\n",
      "| Loss :   0.008785 |\n",
      "| Sparse :   0.013575 |\n",
      "Step 500: Minibatch Loss: 0.008802 Error: 0.007227 Sparse: 0.015755\n",
      "| Loss :   0.007227 |\n",
      "| Sparse :   0.015755 |\n",
      "Step 600: Minibatch Loss: 0.008093 Error: 0.006506 Sparse: 0.015869\n",
      "| Loss :   0.006506 |\n",
      "| Sparse :   0.015869 |\n",
      "Step 700: Minibatch Loss: 0.007711 Error: 0.006162 Sparse: 0.015491\n",
      "| Loss :   0.006162 |\n",
      "| Sparse :   0.015491 |\n",
      "Step 800: Minibatch Loss: 0.007382 Error: 0.005885 Sparse: 0.014971\n",
      "| Loss :   0.005885 |\n",
      "| Sparse :   0.014971 |\n",
      "Step 900: Minibatch Loss: 0.007168 Error: 0.005716 Sparse: 0.014522\n",
      "| Loss :   0.005716 |\n",
      "| Sparse :   0.014522 |\n",
      "Step 1000: Minibatch Loss: 0.006954 Error: 0.005563 Sparse: 0.013908\n",
      "| Loss :   0.005563 |\n",
      "| Sparse :   0.013908 |\n",
      "Step 1100: Minibatch Loss: 0.006882 Error: 0.005547 Sparse: 0.013351\n",
      "| Loss :   0.005547 |\n",
      "| Sparse :   0.013351 |\n",
      "Step 1200: Minibatch Loss: 0.006628 Error: 0.005297 Sparse: 0.013308\n",
      "| Loss :   0.005297 |\n",
      "| Sparse :   0.013308 |\n",
      "Step 1300: Minibatch Loss: 0.006255 Error: 0.004902 Sparse: 0.013531\n",
      "| Loss :   0.004902 |\n",
      "| Sparse :   0.013531 |\n",
      "Step 1400: Minibatch Loss: 0.006040 Error: 0.004700 Sparse: 0.013402\n",
      "| Loss :   0.004700 |\n",
      "| Sparse :   0.013402 |\n",
      "Step 1500: Minibatch Loss: 0.005825 Error: 0.004491 Sparse: 0.013349\n",
      "| Loss :   0.004491 |\n",
      "| Sparse :   0.013349 |\n",
      "Step 1600: Minibatch Loss: 0.005448 Error: 0.004088 Sparse: 0.013601\n",
      "| Loss :   0.004088 |\n",
      "| Sparse :   0.013601 |\n",
      "Step 1700: Minibatch Loss: 0.005182 Error: 0.003802 Sparse: 0.013790\n",
      "| Loss :   0.003802 |\n",
      "| Sparse :   0.013790 |\n",
      "Step 1800: Minibatch Loss: 0.004887 Error: 0.003495 Sparse: 0.013925\n",
      "| Loss :   0.003495 |\n",
      "| Sparse :   0.013925 |\n",
      "Step 1900: Minibatch Loss: 0.004682 Error: 0.003293 Sparse: 0.013889\n",
      "| Loss :   0.003293 |\n",
      "| Sparse :   0.013889 |\n",
      "Step 2000: Minibatch Loss: 0.004519 Error: 0.003139 Sparse: 0.013798\n",
      "| Loss :   0.003139 |\n",
      "| Sparse :   0.013798 |\n",
      "Step 2100: Minibatch Loss: 0.004345 Error: 0.002971 Sparse: 0.013737\n",
      "| Loss :   0.002971 |\n",
      "| Sparse :   0.013737 |\n",
      "Step 2200: Minibatch Loss: 0.004156 Error: 0.002788 Sparse: 0.013678\n",
      "| Loss :   0.002788 |\n",
      "| Sparse :   0.013678 |\n",
      "Step 2300: Minibatch Loss: 0.003995 Error: 0.002644 Sparse: 0.013515\n",
      "| Loss :   0.002644 |\n",
      "| Sparse :   0.013515 |\n",
      "Step 2400: Minibatch Loss: 0.003905 Error: 0.002568 Sparse: 0.013377\n",
      "| Loss :   0.002568 |\n",
      "| Sparse :   0.013377 |\n",
      "Step 2500: Minibatch Loss: 0.003806 Error: 0.002486 Sparse: 0.013201\n",
      "| Loss :   0.002486 |\n",
      "| Sparse :   0.013201 |\n",
      "Step 2600: Minibatch Loss: 0.003679 Error: 0.002355 Sparse: 0.013235\n",
      "| Loss :   0.002355 |\n",
      "| Sparse :   0.013235 |\n",
      "Step 2700: Minibatch Loss: 0.003510 Error: 0.002194 Sparse: 0.013160\n",
      "| Loss :   0.002194 |\n",
      "| Sparse :   0.013160 |\n",
      "Step 2800: Minibatch Loss: 0.003388 Error: 0.002093 Sparse: 0.012954\n",
      "| Loss :   0.002093 |\n",
      "| Sparse :   0.012954 |\n",
      "Step 2900: Minibatch Loss: 0.003383 Error: 0.002106 Sparse: 0.012769\n",
      "| Loss :   0.002106 |\n",
      "| Sparse :   0.012769 |\n",
      "Step 3000: Minibatch Loss: 0.003267 Error: 0.002024 Sparse: 0.012435\n",
      "| Loss :   0.002024 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Sparse :   0.012435 |\n",
      "Step 3100: Minibatch Loss: 0.003224 Error: 0.002006 Sparse: 0.012178\n",
      "| Loss :   0.002006 |\n",
      "| Sparse :   0.012178 |\n",
      "Step 3200: Minibatch Loss: 0.003187 Error: 0.002001 Sparse: 0.011856\n",
      "| Loss :   0.002001 |\n",
      "| Sparse :   0.011856 |\n",
      "Step 3300: Minibatch Loss: 0.003220 Error: 0.002053 Sparse: 0.011671\n",
      "| Loss :   0.002053 |\n",
      "| Sparse :   0.011671 |\n",
      "Step 3400: Minibatch Loss: 0.003116 Error: 0.001980 Sparse: 0.011361\n",
      "| Loss :   0.001980 |\n",
      "| Sparse :   0.011361 |\n",
      "Step 3500: Minibatch Loss: 0.003127 Error: 0.002015 Sparse: 0.011119\n",
      "| Loss :   0.002015 |\n",
      "| Sparse :   0.011119 |\n",
      "Step 3600: Minibatch Loss: 0.003076 Error: 0.001980 Sparse: 0.010955\n",
      "| Loss :   0.001980 |\n",
      "| Sparse :   0.010955 |\n",
      "Step 3700: Minibatch Loss: 0.003050 Error: 0.001978 Sparse: 0.010719\n",
      "| Loss :   0.001978 |\n",
      "| Sparse :   0.010719 |\n",
      "Step 3800: Minibatch Loss: 0.003026 Error: 0.001974 Sparse: 0.010513\n",
      "| Loss :   0.001974 |\n",
      "| Sparse :   0.010513 |\n",
      "Step 3900: Minibatch Loss: 0.002976 Error: 0.001949 Sparse: 0.010274\n",
      "| Loss :   0.001949 |\n",
      "| Sparse :   0.010274 |\n",
      "Step 4000: Minibatch Loss: 0.002964 Error: 0.001952 Sparse: 0.010126\n",
      "| Loss :   0.001952 |\n",
      "| Sparse :   0.010126 |\n",
      "This run of SparceLinAutoEncoderRELU8 ran for 0:01:04 and logs are available locally at: C:\\Users\\cjhamilton4176\\.hyperdash\\logs\\sparcelinautoencoderrelu8\\sparcelinautoencoderrelu8_2018-11-07t12-08-31-716092.log\n",
      "Step 1: Minibatch Loss: 0.015793 Error: 0.015400 Sparse: 0.000393\n",
      "| Loss :   0.015400 |\n",
      "| Sparse :   0.000393 |\n",
      "Step 100: Minibatch Loss: 0.015472 Error: 0.015428 Sparse: 0.000044\n",
      "| Loss :   0.015428 |\n",
      "| Sparse :   0.000044 |\n",
      "Step 200: Minibatch Loss: 0.015478 Error: 0.015434 Sparse: 0.000044\n",
      "| Loss :   0.015434 |\n",
      "| Sparse :   0.000044 |\n",
      "Step 300: Minibatch Loss: 0.015462 Error: 0.015425 Sparse: 0.000037\n",
      "| Loss :   0.015425 |\n",
      "| Sparse :   0.000037 |\n",
      "Step 400: Minibatch Loss: 0.015424 Error: 0.015387 Sparse: 0.000037\n",
      "| Loss :   0.015387 |\n",
      "| Sparse :   0.000037 |\n",
      "Step 500: Minibatch Loss: 0.015499 Error: 0.015459 Sparse: 0.000040\n",
      "| Loss :   0.015459 |\n",
      "| Sparse :   0.000040 |\n",
      "Step 600: Minibatch Loss: 0.015451 Error: 0.015417 Sparse: 0.000034\n",
      "| Loss :   0.015417 |\n",
      "| Sparse :   0.000034 |\n",
      "Step 700: Minibatch Loss: 0.015429 Error: 0.015390 Sparse: 0.000039\n",
      "| Loss :   0.015390 |\n",
      "| Sparse :   0.000039 |\n",
      "Step 800: Minibatch Loss: 0.015484 Error: 0.015444 Sparse: 0.000040\n",
      "| Loss :   0.015444 |\n",
      "| Sparse :   0.000040 |\n",
      "Step 900: Minibatch Loss: 0.015429 Error: 0.015385 Sparse: 0.000044\n",
      "| Loss :   0.015385 |\n",
      "| Sparse :   0.000044 |\n",
      "Step 1000: Minibatch Loss: 0.015377 Error: 0.015267 Sparse: 0.000110\n",
      "| Loss :   0.015267 |\n",
      "| Sparse :   0.000110 |\n",
      "Step 1100: Minibatch Loss: 0.015081 Error: 0.014594 Sparse: 0.000488\n",
      "| Loss :   0.014594 |\n",
      "| Sparse :   0.000488 |\n",
      "Step 1200: Minibatch Loss: 0.014973 Error: 0.014464 Sparse: 0.000508\n",
      "| Loss :   0.014464 |\n",
      "| Sparse :   0.000508 |\n",
      "Step 1300: Minibatch Loss: 0.014694 Error: 0.014127 Sparse: 0.000567\n",
      "| Loss :   0.014127 |\n",
      "| Sparse :   0.000567 |\n",
      "Step 1400: Minibatch Loss: 0.014582 Error: 0.013960 Sparse: 0.000622\n",
      "| Loss :   0.013960 |\n",
      "| Sparse :   0.000622 |\n",
      "Step 1500: Minibatch Loss: 0.014428 Error: 0.013763 Sparse: 0.000665\n",
      "| Loss :   0.013763 |\n",
      "| Sparse :   0.000665 |\n",
      "Step 1600: Minibatch Loss: 0.014369 Error: 0.013684 Sparse: 0.000685\n",
      "| Loss :   0.013684 |\n",
      "| Sparse :   0.000685 |\n",
      "Step 1700: Minibatch Loss: 0.014348 Error: 0.013677 Sparse: 0.000670\n",
      "| Loss :   0.013677 |\n",
      "| Sparse :   0.000670 |\n",
      "Step 1800: Minibatch Loss: 0.014134 Error: 0.013446 Sparse: 0.000688\n",
      "| Loss :   0.013446 |\n",
      "| Sparse :   0.000688 |\n",
      "Step 1900: Minibatch Loss: 0.013914 Error: 0.012951 Sparse: 0.000962\n",
      "| Loss :   0.012951 |\n",
      "| Sparse :   0.000962 |\n",
      "Step 2000: Minibatch Loss: 0.013290 Error: 0.011832 Sparse: 0.001458\n",
      "| Loss :   0.011832 |\n",
      "| Sparse :   0.001458 |\n",
      "Step 2100: Minibatch Loss: 0.013113 Error: 0.011714 Sparse: 0.001399\n",
      "| Loss :   0.011714 |\n",
      "| Sparse :   0.001399 |\n",
      "Step 2200: Minibatch Loss: 0.013006 Error: 0.011669 Sparse: 0.001337\n",
      "| Loss :   0.011669 |\n",
      "| Sparse :   0.001337 |\n",
      "Step 2300: Minibatch Loss: 0.012835 Error: 0.011545 Sparse: 0.001290\n",
      "| Loss :   0.011545 |\n",
      "| Sparse :   0.001290 |\n",
      "Step 2400: Minibatch Loss: 0.012815 Error: 0.011556 Sparse: 0.001259\n",
      "| Loss :   0.011556 |\n",
      "| Sparse :   0.001259 |\n",
      "Step 2500: Minibatch Loss: 0.012794 Error: 0.011591 Sparse: 0.001202\n",
      "| Loss :   0.011591 |\n",
      "| Sparse :   0.001202 |\n",
      "Step 2600: Minibatch Loss: 0.012609 Error: 0.011426 Sparse: 0.001183\n",
      "| Loss :   0.011426 |\n",
      "| Sparse :   0.001183 |\n",
      "Step 2700: Minibatch Loss: 0.012689 Error: 0.011552 Sparse: 0.001138\n",
      "| Loss :   0.011552 |\n",
      "| Sparse :   0.001138 |\n",
      "Step 2800: Minibatch Loss: 0.012693 Error: 0.011594 Sparse: 0.001099\n",
      "| Loss :   0.011594 |\n",
      "| Sparse :   0.001099 |\n",
      "Step 2900: Minibatch Loss: 0.012573 Error: 0.011472 Sparse: 0.001101\n",
      "| Loss :   0.011472 |\n",
      "| Sparse :   0.001101 |\n",
      "Step 3000: Minibatch Loss: 0.012538 Error: 0.011463 Sparse: 0.001076\n",
      "| Loss :   0.011463 |\n",
      "| Sparse :   0.001076 |\n",
      "Step 3100: Minibatch Loss: 0.012168 Error: 0.010686 Sparse: 0.001482\n",
      "| Loss :   0.010686 |\n",
      "| Sparse :   0.001482 |\n",
      "Step 3200: Minibatch Loss: 0.011937 Error: 0.010383 Sparse: 0.001554\n",
      "| Loss :   0.010383 |\n",
      "| Sparse :   0.001554 |\n",
      "Step 3300: Minibatch Loss: 0.011803 Error: 0.010233 Sparse: 0.001570\n",
      "| Loss :   0.010233 |\n",
      "| Sparse :   0.001570 |\n",
      "Step 3400: Minibatch Loss: 0.011693 Error: 0.010132 Sparse: 0.001561\n",
      "| Loss :   0.010132 |\n",
      "| Sparse :   0.001561 |\n",
      "Step 3500: Minibatch Loss: 0.011639 Error: 0.010113 Sparse: 0.001526\n",
      "| Loss :   0.010113 |\n",
      "| Sparse :   0.001526 |\n",
      "Step 3600: Minibatch Loss: 0.011531 Error: 0.010013 Sparse: 0.001518\n",
      "| Loss :   0.010013 |\n",
      "| Sparse :   0.001518 |\n",
      "Step 3700: Minibatch Loss: 0.011488 Error: 0.009966 Sparse: 0.001522\n",
      "| Loss :   0.009966 |\n",
      "| Sparse :   0.001522 |\n",
      "Step 3800: Minibatch Loss: 0.011273 Error: 0.009622 Sparse: 0.001650\n",
      "| Loss :   0.009622 |\n",
      "| Sparse :   0.001650 |\n",
      "Step 3900: Minibatch Loss: 0.011215 Error: 0.009493 Sparse: 0.001721\n",
      "| Loss :   0.009493 |\n",
      "| Sparse :   0.001721 |\n",
      "Step 4000: Minibatch Loss: 0.011025 Error: 0.009286 Sparse: 0.001739\n",
      "| Loss :   0.009286 |\n",
      "| Sparse :   0.001739 |\n",
      "This run of SparceLinAutoEncoderRELU8 ran for 0:01:04 and logs are available locally at: C:\\Users\\cjhamilton4176\\.hyperdash\\logs\\sparcelinautoencoderrelu8\\sparcelinautoencoderrelu8_2018-11-07t12-09-37-175675.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cjhamilton4176\\AppData\\Local\\conda\\conda\\envs\\DeepLearner\\lib\\site-packages\\matplotlib\\pyplot.py:522: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Minibatch Loss: 0.015459 Error: 0.015459 Sparse: 0.000408\n",
      "| Loss :   0.015459 |\n",
      "| Sparse :   0.000408 |\n",
      "Step 100: Minibatch Loss: 0.010079 Error: 0.010075 Sparse: 0.039604\n",
      "| Loss :   0.010075 |\n",
      "| Sparse :   0.039604 |\n",
      "Step 200: Minibatch Loss: 0.006744 Error: 0.006739 Sparse: 0.050961\n",
      "| Loss :   0.006739 |\n",
      "| Sparse :   0.050961 |\n",
      "Step 300: Minibatch Loss: 0.005310 Error: 0.005305 Sparse: 0.055911\n",
      "| Loss :   0.005305 |\n",
      "| Sparse :   0.055911 |\n",
      "Step 400: Minibatch Loss: 0.004402 Error: 0.004397 Sparse: 0.058650\n",
      "| Loss :   0.004397 |\n",
      "| Sparse :   0.058650 |\n",
      "Step 500: Minibatch Loss: 0.003817 Error: 0.003811 Sparse: 0.060306\n",
      "| Loss :   0.003811 |\n",
      "| Sparse :   0.060306 |\n",
      "Step 600: Minibatch Loss: 0.003252 Error: 0.003246 Sparse: 0.061489\n",
      "| Loss :   0.003246 |\n",
      "| Sparse :   0.061489 |\n",
      "Step 700: Minibatch Loss: 0.002886 Error: 0.002880 Sparse: 0.062679\n",
      "| Loss :   0.002880 |\n",
      "| Sparse :   0.062679 |\n",
      "Step 800: Minibatch Loss: 0.002658 Error: 0.002651 Sparse: 0.062799\n",
      "| Loss :   0.002651 |\n",
      "| Sparse :   0.062799 |\n",
      "Step 900: Minibatch Loss: 0.002486 Error: 0.002480 Sparse: 0.062655\n",
      "| Loss :   0.002480 |\n",
      "| Sparse :   0.062655 |\n",
      "Step 1000: Minibatch Loss: 0.002408 Error: 0.002401 Sparse: 0.062433\n",
      "| Loss :   0.002401 |\n",
      "| Sparse :   0.062433 |\n",
      "Step 1100: Minibatch Loss: 0.002348 Error: 0.002342 Sparse: 0.061574\n",
      "| Loss :   0.002342 |\n",
      "| Sparse :   0.061574 |\n",
      "Step 1200: Minibatch Loss: 0.002275 Error: 0.002269 Sparse: 0.060980\n",
      "| Loss :   0.002269 |\n",
      "| Sparse :   0.060980 |\n",
      "Step 1300: Minibatch Loss: 0.002231 Error: 0.002225 Sparse: 0.060756\n",
      "| Loss :   0.002225 |\n",
      "| Sparse :   0.060756 |\n",
      "Step 1400: Minibatch Loss: 0.002176 Error: 0.002170 Sparse: 0.060168\n",
      "| Loss :   0.002170 |\n",
      "| Sparse :   0.060168 |\n",
      "Step 1500: Minibatch Loss: 0.002119 Error: 0.002113 Sparse: 0.060339\n",
      "| Loss :   0.002113 |\n",
      "| Sparse :   0.060339 |\n",
      "Step 1600: Minibatch Loss: 0.002070 Error: 0.002064 Sparse: 0.059980\n",
      "| Loss :   0.002064 |\n",
      "| Sparse :   0.059980 |\n",
      "Step 1700: Minibatch Loss: 0.002002 Error: 0.001996 Sparse: 0.059869\n",
      "| Loss :   0.001996 |\n",
      "| Sparse :   0.059869 |\n",
      "Step 1800: Minibatch Loss: 0.002012 Error: 0.002006 Sparse: 0.059907\n",
      "| Loss :   0.002006 |\n",
      "| Sparse :   0.059907 |\n",
      "Step 1900: Minibatch Loss: 0.001985 Error: 0.001979 Sparse: 0.059790\n",
      "| Loss :   0.001979 |\n",
      "| Sparse :   0.059790 |\n",
      "Step 2000: Minibatch Loss: 0.002016 Error: 0.002010 Sparse: 0.059864\n",
      "| Loss :   0.002010 |\n",
      "| Sparse :   0.059864 |\n",
      "Step 2100: Minibatch Loss: 0.001991 Error: 0.001985 Sparse: 0.059973\n",
      "| Loss :   0.001985 |\n",
      "| Sparse :   0.059973 |\n",
      "Step 2200: Minibatch Loss: 0.001950 Error: 0.001944 Sparse: 0.059777\n",
      "| Loss :   0.001944 |\n",
      "| Sparse :   0.059777 |\n",
      "Step 2300: Minibatch Loss: 0.001948 Error: 0.001942 Sparse: 0.059444\n",
      "| Loss :   0.001942 |\n",
      "| Sparse :   0.059444 |\n",
      "Step 2400: Minibatch Loss: 0.001970 Error: 0.001964 Sparse: 0.059635\n",
      "| Loss :   0.001964 |\n",
      "| Sparse :   0.059635 |\n",
      "Step 2500: Minibatch Loss: 0.001926 Error: 0.001920 Sparse: 0.059588\n",
      "| Loss :   0.001920 |\n",
      "| Sparse :   0.059588 |\n",
      "Step 2600: Minibatch Loss: 0.001933 Error: 0.001927 Sparse: 0.059716\n",
      "| Loss :   0.001927 |\n",
      "| Sparse :   0.059716 |\n",
      "Step 2700: Minibatch Loss: 0.001921 Error: 0.001915 Sparse: 0.059427\n",
      "| Loss :   0.001915 |\n",
      "| Sparse :   0.059427 |\n",
      "Step 2800: Minibatch Loss: 0.001882 Error: 0.001876 Sparse: 0.059432\n",
      "| Loss :   0.001876 |\n",
      "| Sparse :   0.059432 |\n",
      "Step 2900: Minibatch Loss: 0.001905 Error: 0.001899 Sparse: 0.059399\n",
      "| Loss :   0.001899 |\n",
      "| Sparse :   0.059399 |\n",
      "Step 3000: Minibatch Loss: 0.001939 Error: 0.001933 Sparse: 0.059162\n",
      "| Loss :   0.001933 |\n",
      "| Sparse :   0.059162 |\n",
      "Step 3100: Minibatch Loss: 0.001909 Error: 0.001903 Sparse: 0.059065\n",
      "| Loss :   0.001903 |\n",
      "| Sparse :   0.059065 |\n",
      "Step 3200: Minibatch Loss: 0.001904 Error: 0.001898 Sparse: 0.058891\n",
      "| Loss :   0.001898 |\n",
      "| Sparse :   0.058891 |\n",
      "Step 3300: Minibatch Loss: 0.001904 Error: 0.001898 Sparse: 0.058873\n",
      "| Loss :   0.001898 |\n",
      "| Sparse :   0.058873 |\n",
      "Step 3400: Minibatch Loss: 0.001911 Error: 0.001905 Sparse: 0.058716\n",
      "| Loss :   0.001905 |\n",
      "| Sparse :   0.058716 |\n",
      "Step 3500: Minibatch Loss: 0.001901 Error: 0.001895 Sparse: 0.058575\n",
      "| Loss :   0.001895 |\n",
      "| Sparse :   0.058575 |\n",
      "Step 3600: Minibatch Loss: 0.001886 Error: 0.001880 Sparse: 0.058359\n",
      "| Loss :   0.001880 |\n",
      "| Sparse :   0.058359 |\n",
      "Step 3700: Minibatch Loss: 0.001902 Error: 0.001896 Sparse: 0.058313\n",
      "| Loss :   0.001896 |\n",
      "| Sparse :   0.058313 |\n",
      "Step 3800: Minibatch Loss: 0.001858 Error: 0.001852 Sparse: 0.058385\n",
      "| Loss :   0.001852 |\n",
      "| Sparse :   0.058385 |\n",
      "Step 3900: Minibatch Loss: 0.001896 Error: 0.001890 Sparse: 0.058174\n",
      "| Loss :   0.001890 |\n",
      "| Sparse :   0.058174 |\n",
      "Step 4000: Minibatch Loss: 0.001896 Error: 0.001890 Sparse: 0.058121\n",
      "| Loss :   0.001890 |\n",
      "| Sparse :   0.058121 |\n",
      "Step 4100: Minibatch Loss: 0.001884 Error: 0.001878 Sparse: 0.057994\n",
      "| Loss :   0.001878 |\n",
      "| Sparse :   0.057994 |\n",
      "Step 4200: Minibatch Loss: 0.001907 Error: 0.001901 Sparse: 0.057940\n",
      "| Loss :   0.001901 |\n",
      "| Sparse :   0.057940 |\n",
      "Step 4300: Minibatch Loss: 0.001905 Error: 0.001899 Sparse: 0.057904\n",
      "| Loss :   0.001899 |\n",
      "| Sparse :   0.057904 |\n",
      "Step 4400: Minibatch Loss: 0.001887 Error: 0.001881 Sparse: 0.057584\n",
      "| Loss :   0.001881 |\n",
      "| Sparse :   0.057584 |\n",
      "Step 4500: Minibatch Loss: 0.001901 Error: 0.001896 Sparse: 0.057716\n",
      "| Loss :   0.001896 |\n",
      "| Sparse :   0.057716 |\n",
      "Step 4600: Minibatch Loss: 0.001883 Error: 0.001877 Sparse: 0.057485\n",
      "| Loss :   0.001877 |\n",
      "| Sparse :   0.057485 |\n",
      "Step 4700: Minibatch Loss: 0.001879 Error: 0.001874 Sparse: 0.057206\n",
      "| Loss :   0.001874 |\n",
      "| Sparse :   0.057206 |\n",
      "Step 4800: Minibatch Loss: 0.001880 Error: 0.001875 Sparse: 0.057206\n",
      "| Loss :   0.001875 |\n",
      "| Sparse :   0.057206 |\n",
      "Step 4900: Minibatch Loss: 0.001916 Error: 0.001910 Sparse: 0.057364\n",
      "| Loss :   0.001910 |\n",
      "| Sparse :   0.057364 |\n",
      "Step 5000: Minibatch Loss: 0.001917 Error: 0.001911 Sparse: 0.057093\n",
      "| Loss :   0.001911 |\n",
      "| Sparse :   0.057093 |\n",
      "This run of SparceLinAutoEncoderRELU8 ran for 0:01:19 and logs are available locally at: C:\\Users\\cjhamilton4176\\.hyperdash\\logs\\sparcelinautoencoderrelu8\\sparcelinautoencoderrelu8_2018-11-07t12-10-42-765085.log\n",
      "Step 1: Minibatch Loss: 0.015447 Error: 0.015447 Sparse: 0.000398\n",
      "| Loss :   0.015447 |\n",
      "| Sparse :   0.000398 |\n",
      "Step 100: Minibatch Loss: 0.010239 Error: 0.010200 Sparse: 0.038918\n",
      "| Loss :   0.010200 |\n",
      "| Sparse :   0.038918 |\n",
      "Step 200: Minibatch Loss: 0.006921 Error: 0.006871 Sparse: 0.050206\n",
      "| Loss :   0.006871 |\n",
      "| Sparse :   0.050206 |\n",
      "Step 300: Minibatch Loss: 0.005430 Error: 0.005375 Sparse: 0.054445\n",
      "| Loss :   0.005375 |\n",
      "| Sparse :   0.054445 |\n",
      "Step 400: Minibatch Loss: 0.004444 Error: 0.004387 Sparse: 0.057486\n",
      "| Loss :   0.004387 |\n",
      "| Sparse :   0.057486 |\n",
      "Step 500: Minibatch Loss: 0.003776 Error: 0.003717 Sparse: 0.058752\n",
      "| Loss :   0.003717 |\n",
      "| Sparse :   0.058752 |\n",
      "Step 600: Minibatch Loss: 0.003285 Error: 0.003225 Sparse: 0.059670\n",
      "| Loss :   0.003225 |\n",
      "| Sparse :   0.059670 |\n",
      "Step 700: Minibatch Loss: 0.002936 Error: 0.002876 Sparse: 0.060049\n",
      "| Loss :   0.002876 |\n",
      "| Sparse :   0.060049 |\n",
      "Step 800: Minibatch Loss: 0.002647 Error: 0.002587 Sparse: 0.060103\n",
      "| Loss :   0.002587 |\n",
      "| Sparse :   0.060103 |\n",
      "Step 900: Minibatch Loss: 0.002502 Error: 0.002442 Sparse: 0.060100\n",
      "| Loss :   0.002442 |\n",
      "| Sparse :   0.060100 |\n",
      "Step 1000: Minibatch Loss: 0.002417 Error: 0.002357 Sparse: 0.059495\n",
      "| Loss :   0.002357 |\n",
      "| Sparse :   0.059495 |\n",
      "Step 1100: Minibatch Loss: 0.002351 Error: 0.002293 Sparse: 0.058667\n",
      "| Loss :   0.002293 |\n",
      "| Sparse :   0.058667 |\n",
      "Step 1200: Minibatch Loss: 0.002258 Error: 0.002200 Sparse: 0.058413\n",
      "| Loss :   0.002200 |\n",
      "| Sparse :   0.058413 |\n",
      "Step 1300: Minibatch Loss: 0.002207 Error: 0.002149 Sparse: 0.058183\n",
      "| Loss :   0.002149 |\n",
      "| Sparse :   0.058183 |\n",
      "Step 1400: Minibatch Loss: 0.002188 Error: 0.002131 Sparse: 0.057212\n",
      "| Loss :   0.002131 |\n",
      "| Sparse :   0.057212 |\n",
      "Step 1500: Minibatch Loss: 0.002143 Error: 0.002086 Sparse: 0.057310\n",
      "| Loss :   0.002086 |\n",
      "| Sparse :   0.057310 |\n",
      "Step 1600: Minibatch Loss: 0.002131 Error: 0.002074 Sparse: 0.056915\n",
      "| Loss :   0.002074 |\n",
      "| Sparse :   0.056915 |\n",
      "Step 1700: Minibatch Loss: 0.002145 Error: 0.002088 Sparse: 0.056524\n",
      "| Loss :   0.002088 |\n",
      "| Sparse :   0.056524 |\n",
      "Step 1800: Minibatch Loss: 0.002089 Error: 0.002033 Sparse: 0.056146\n",
      "| Loss :   0.002033 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Sparse :   0.056146 |\n",
      "Step 1900: Minibatch Loss: 0.002060 Error: 0.002004 Sparse: 0.055838\n",
      "| Loss :   0.002004 |\n",
      "| Sparse :   0.055838 |\n",
      "Step 2000: Minibatch Loss: 0.002005 Error: 0.001950 Sparse: 0.055425\n",
      "| Loss :   0.001950 |\n",
      "| Sparse :   0.055425 |\n",
      "Step 2100: Minibatch Loss: 0.002046 Error: 0.001991 Sparse: 0.055039\n",
      "| Loss :   0.001991 |\n",
      "| Sparse :   0.055039 |\n",
      "Step 2200: Minibatch Loss: 0.002011 Error: 0.001956 Sparse: 0.054470\n",
      "| Loss :   0.001956 |\n",
      "| Sparse :   0.054470 |\n",
      "Step 2300: Minibatch Loss: 0.002005 Error: 0.001951 Sparse: 0.054074\n",
      "| Loss :   0.001951 |\n",
      "| Sparse :   0.054074 |\n",
      "Step 2400: Minibatch Loss: 0.002016 Error: 0.001962 Sparse: 0.053754\n",
      "| Loss :   0.001962 |\n",
      "| Sparse :   0.053754 |\n",
      "Step 2500: Minibatch Loss: 0.002013 Error: 0.001959 Sparse: 0.053283\n",
      "| Loss :   0.001959 |\n",
      "| Sparse :   0.053283 |\n",
      "Step 2600: Minibatch Loss: 0.001970 Error: 0.001917 Sparse: 0.053018\n",
      "| Loss :   0.001917 |\n",
      "| Sparse :   0.053018 |\n",
      "Step 2700: Minibatch Loss: 0.001956 Error: 0.001903 Sparse: 0.052687\n",
      "| Loss :   0.001903 |\n",
      "| Sparse :   0.052687 |\n",
      "Step 2800: Minibatch Loss: 0.001987 Error: 0.001935 Sparse: 0.052184\n",
      "| Loss :   0.001935 |\n",
      "| Sparse :   0.052184 |\n",
      "Step 2900: Minibatch Loss: 0.001984 Error: 0.001932 Sparse: 0.051794\n",
      "| Loss :   0.001932 |\n",
      "| Sparse :   0.051794 |\n",
      "Step 3000: Minibatch Loss: 0.001976 Error: 0.001924 Sparse: 0.051488\n",
      "| Loss :   0.001924 |\n",
      "| Sparse :   0.051488 |\n",
      "Step 3100: Minibatch Loss: 0.001942 Error: 0.001891 Sparse: 0.050856\n",
      "| Loss :   0.001891 |\n",
      "| Sparse :   0.050856 |\n",
      "Step 3200: Minibatch Loss: 0.002002 Error: 0.001951 Sparse: 0.050655\n",
      "| Loss :   0.001951 |\n",
      "| Sparse :   0.050655 |\n",
      "Step 3300: Minibatch Loss: 0.001924 Error: 0.001874 Sparse: 0.050362\n",
      "| Loss :   0.001874 |\n",
      "| Sparse :   0.050362 |\n",
      "Step 3400: Minibatch Loss: 0.001932 Error: 0.001882 Sparse: 0.049800\n",
      "| Loss :   0.001882 |\n",
      "| Sparse :   0.049800 |\n",
      "Step 3500: Minibatch Loss: 0.001948 Error: 0.001898 Sparse: 0.049627\n",
      "| Loss :   0.001898 |\n",
      "| Sparse :   0.049627 |\n",
      "Step 3600: Minibatch Loss: 0.001963 Error: 0.001913 Sparse: 0.049448\n",
      "| Loss :   0.001913 |\n",
      "| Sparse :   0.049448 |\n",
      "Step 3700: Minibatch Loss: 0.001941 Error: 0.001892 Sparse: 0.048656\n",
      "| Loss :   0.001892 |\n",
      "| Sparse :   0.048656 |\n",
      "Step 3800: Minibatch Loss: 0.001930 Error: 0.001881 Sparse: 0.048360\n",
      "| Loss :   0.001881 |\n",
      "| Sparse :   0.048360 |\n",
      "Step 3900: Minibatch Loss: 0.001956 Error: 0.001908 Sparse: 0.047959\n",
      "| Loss :   0.001908 |\n",
      "| Sparse :   0.047959 |\n",
      "Step 4000: Minibatch Loss: 0.001951 Error: 0.001903 Sparse: 0.047485\n",
      "| Loss :   0.001903 |\n",
      "| Sparse :   0.047485 |\n",
      "Step 4100: Minibatch Loss: 0.001943 Error: 0.001895 Sparse: 0.047352\n",
      "| Loss :   0.001895 |\n",
      "| Sparse :   0.047352 |\n",
      "Step 4200: Minibatch Loss: 0.001953 Error: 0.001906 Sparse: 0.046728\n",
      "| Loss :   0.001906 |\n",
      "| Sparse :   0.046728 |\n",
      "Step 4300: Minibatch Loss: 0.001953 Error: 0.001906 Sparse: 0.046622\n",
      "| Loss :   0.001906 |\n",
      "| Sparse :   0.046622 |\n",
      "Step 4400: Minibatch Loss: 0.001909 Error: 0.001863 Sparse: 0.045901\n",
      "| Loss :   0.001863 |\n",
      "| Sparse :   0.045901 |\n",
      "Step 4500: Minibatch Loss: 0.001920 Error: 0.001874 Sparse: 0.045853\n",
      "| Loss :   0.001874 |\n",
      "| Sparse :   0.045853 |\n",
      "Step 4600: Minibatch Loss: 0.001953 Error: 0.001907 Sparse: 0.045480\n",
      "| Loss :   0.001907 |\n",
      "| Sparse :   0.045480 |\n",
      "Step 4700: Minibatch Loss: 0.001918 Error: 0.001873 Sparse: 0.045033\n",
      "| Loss :   0.001873 |\n",
      "| Sparse :   0.045033 |\n",
      "Step 4800: Minibatch Loss: 0.001946 Error: 0.001902 Sparse: 0.044645\n",
      "| Loss :   0.001902 |\n",
      "| Sparse :   0.044645 |\n",
      "Step 4900: Minibatch Loss: 0.001906 Error: 0.001861 Sparse: 0.044424\n",
      "| Loss :   0.001861 |\n",
      "| Sparse :   0.044424 |\n",
      "Step 5000: Minibatch Loss: 0.001942 Error: 0.001898 Sparse: 0.043945\n",
      "| Loss :   0.001898 |\n",
      "| Sparse :   0.043945 |\n",
      "This run of SparceLinAutoEncoderRELU8 ran for 0:01:19 and logs are available locally at: C:\\Users\\cjhamilton4176\\.hyperdash\\logs\\sparcelinautoencoderrelu8\\sparcelinautoencoderrelu8_2018-11-07t12-12-03-718645.log\n",
      "Step 1: Minibatch Loss: 0.015457 Error: 0.015453 Sparse: 0.000403\n",
      "| Loss :   0.015453 |\n",
      "| Sparse :   0.000403 |\n",
      "Step 100: Minibatch Loss: 0.010699 Error: 0.010342 Sparse: 0.035711\n",
      "| Loss :   0.010342 |\n",
      "| Sparse :   0.035711 |\n",
      "Step 200: Minibatch Loss: 0.007448 Error: 0.006996 Sparse: 0.045242\n",
      "| Loss :   0.006996 |\n",
      "| Sparse :   0.045242 |\n",
      "Step 300: Minibatch Loss: 0.005954 Error: 0.005474 Sparse: 0.048061\n",
      "| Loss :   0.005474 |\n",
      "| Sparse :   0.048061 |\n",
      "Step 400: Minibatch Loss: 0.004868 Error: 0.004371 Sparse: 0.049615\n",
      "| Loss :   0.004371 |\n",
      "| Sparse :   0.049615 |\n",
      "Step 500: Minibatch Loss: 0.004187 Error: 0.003684 Sparse: 0.050268\n",
      "| Loss :   0.003684 |\n",
      "| Sparse :   0.050268 |\n",
      "Step 600: Minibatch Loss: 0.003726 Error: 0.003227 Sparse: 0.049922\n",
      "| Loss :   0.003227 |\n",
      "| Sparse :   0.049922 |\n",
      "Step 700: Minibatch Loss: 0.003345 Error: 0.002848 Sparse: 0.049747\n",
      "| Loss :   0.002848 |\n",
      "| Sparse :   0.049747 |\n",
      "Step 800: Minibatch Loss: 0.003123 Error: 0.002632 Sparse: 0.049143\n",
      "| Loss :   0.002632 |\n",
      "| Sparse :   0.049143 |\n",
      "Step 900: Minibatch Loss: 0.002937 Error: 0.002455 Sparse: 0.048296\n",
      "| Loss :   0.002455 |\n",
      "| Sparse :   0.048296 |\n",
      "Step 1000: Minibatch Loss: 0.002820 Error: 0.002348 Sparse: 0.047230\n",
      "| Loss :   0.002348 |\n",
      "| Sparse :   0.047230 |\n",
      "Step 1100: Minibatch Loss: 0.002760 Error: 0.002300 Sparse: 0.046058\n",
      "| Loss :   0.002300 |\n",
      "| Sparse :   0.046058 |\n",
      "Step 1200: Minibatch Loss: 0.002662 Error: 0.002213 Sparse: 0.044882\n",
      "| Loss :   0.002213 |\n",
      "| Sparse :   0.044882 |\n",
      "Step 1300: Minibatch Loss: 0.002635 Error: 0.002197 Sparse: 0.043821\n",
      "| Loss :   0.002197 |\n",
      "| Sparse :   0.043821 |\n",
      "Step 1400: Minibatch Loss: 0.002522 Error: 0.002093 Sparse: 0.042830\n",
      "| Loss :   0.002093 |\n",
      "| Sparse :   0.042830 |\n",
      "Step 1500: Minibatch Loss: 0.002521 Error: 0.002104 Sparse: 0.041720\n",
      "| Loss :   0.002104 |\n",
      "| Sparse :   0.041720 |\n",
      "Step 1600: Minibatch Loss: 0.002480 Error: 0.002075 Sparse: 0.040535\n",
      "| Loss :   0.002075 |\n",
      "| Sparse :   0.040535 |\n",
      "Step 1700: Minibatch Loss: 0.002472 Error: 0.002076 Sparse: 0.039536\n",
      "| Loss :   0.002076 |\n",
      "| Sparse :   0.039536 |\n",
      "Step 1800: Minibatch Loss: 0.002438 Error: 0.002053 Sparse: 0.038541\n",
      "| Loss :   0.002053 |\n",
      "| Sparse :   0.038541 |\n",
      "Step 1900: Minibatch Loss: 0.002395 Error: 0.002019 Sparse: 0.037583\n",
      "| Loss :   0.002019 |\n",
      "| Sparse :   0.037583 |\n",
      "Step 2000: Minibatch Loss: 0.002378 Error: 0.002011 Sparse: 0.036659\n",
      "| Loss :   0.002011 |\n",
      "| Sparse :   0.036659 |\n",
      "Step 2100: Minibatch Loss: 0.002350 Error: 0.001992 Sparse: 0.035789\n",
      "| Loss :   0.001992 |\n",
      "| Sparse :   0.035789 |\n",
      "Step 2200: Minibatch Loss: 0.002313 Error: 0.001963 Sparse: 0.034974\n",
      "| Loss :   0.001963 |\n",
      "| Sparse :   0.034974 |\n",
      "Step 2300: Minibatch Loss: 0.002297 Error: 0.001955 Sparse: 0.034126\n",
      "| Loss :   0.001955 |\n",
      "| Sparse :   0.034126 |\n",
      "Step 2400: Minibatch Loss: 0.002295 Error: 0.001961 Sparse: 0.033421\n",
      "| Loss :   0.001961 |\n",
      "| Sparse :   0.033421 |\n",
      "Step 2500: Minibatch Loss: 0.002265 Error: 0.001938 Sparse: 0.032686\n",
      "| Loss :   0.001938 |\n",
      "| Sparse :   0.032686 |\n",
      "Step 2600: Minibatch Loss: 0.002260 Error: 0.001940 Sparse: 0.031939\n",
      "| Loss :   0.001940 |\n",
      "| Sparse :   0.031939 |\n",
      "Step 2700: Minibatch Loss: 0.002258 Error: 0.001945 Sparse: 0.031351\n",
      "| Loss :   0.001945 |\n",
      "| Sparse :   0.031351 |\n",
      "Step 2800: Minibatch Loss: 0.002235 Error: 0.001928 Sparse: 0.030709\n",
      "| Loss :   0.001928 |\n",
      "| Sparse :   0.030709 |\n",
      "Step 2900: Minibatch Loss: 0.002225 Error: 0.001925 Sparse: 0.030021\n",
      "| Loss :   0.001925 |\n",
      "| Sparse :   0.030021 |\n",
      "Step 3000: Minibatch Loss: 0.002212 Error: 0.001918 Sparse: 0.029340\n",
      "| Loss :   0.001918 |\n",
      "| Sparse :   0.029340 |\n",
      "Step 3100: Minibatch Loss: 0.002190 Error: 0.001904 Sparse: 0.028650\n",
      "| Loss :   0.001904 |\n",
      "| Sparse :   0.028650 |\n",
      "Step 3200: Minibatch Loss: 0.002194 Error: 0.001912 Sparse: 0.028171\n",
      "| Loss :   0.001912 |\n",
      "| Sparse :   0.028171 |\n",
      "Step 3300: Minibatch Loss: 0.002203 Error: 0.001927 Sparse: 0.027611\n",
      "| Loss :   0.001927 |\n",
      "| Sparse :   0.027611 |\n",
      "Step 3400: Minibatch Loss: 0.002157 Error: 0.001886 Sparse: 0.027101\n",
      "| Loss :   0.001886 |\n",
      "| Sparse :   0.027101 |\n",
      "Step 3500: Minibatch Loss: 0.002140 Error: 0.001874 Sparse: 0.026611\n",
      "| Loss :   0.001874 |\n",
      "| Sparse :   0.026611 |\n",
      "Step 3600: Minibatch Loss: 0.002146 Error: 0.001885 Sparse: 0.026010\n",
      "| Loss :   0.001885 |\n",
      "| Sparse :   0.026010 |\n",
      "Step 3700: Minibatch Loss: 0.002114 Error: 0.001858 Sparse: 0.025601\n",
      "| Loss :   0.001858 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Sparse :   0.025601 |\n",
      "Step 3800: Minibatch Loss: 0.002124 Error: 0.001873 Sparse: 0.025159\n",
      "| Loss :   0.001873 |\n",
      "| Sparse :   0.025159 |\n",
      "Step 3900: Minibatch Loss: 0.002133 Error: 0.001885 Sparse: 0.024809\n",
      "| Loss :   0.001885 |\n",
      "| Sparse :   0.024809 |\n",
      "Step 4000: Minibatch Loss: 0.002107 Error: 0.001863 Sparse: 0.024360\n",
      "| Loss :   0.001863 |\n",
      "| Sparse :   0.024360 |\n",
      "Step 4100: Minibatch Loss: 0.002149 Error: 0.001911 Sparse: 0.023852\n",
      "| Loss :   0.001911 |\n",
      "| Sparse :   0.023852 |\n",
      "Step 4200: Minibatch Loss: 0.002115 Error: 0.001882 Sparse: 0.023307\n",
      "| Loss :   0.001882 |\n",
      "| Sparse :   0.023307 |\n",
      "Step 4300: Minibatch Loss: 0.002083 Error: 0.001852 Sparse: 0.023108\n",
      "| Loss :   0.001852 |\n",
      "| Sparse :   0.023108 |\n",
      "Step 4400: Minibatch Loss: 0.002104 Error: 0.001877 Sparse: 0.022716\n",
      "| Loss :   0.001877 |\n",
      "| Sparse :   0.022716 |\n",
      "Step 4500: Minibatch Loss: 0.002100 Error: 0.001877 Sparse: 0.022270\n",
      "| Loss :   0.001877 |\n",
      "| Sparse :   0.022270 |\n",
      "Step 4600: Minibatch Loss: 0.002084 Error: 0.001864 Sparse: 0.022044\n",
      "| Loss :   0.001864 |\n",
      "| Sparse :   0.022044 |\n",
      "Step 4700: Minibatch Loss: 0.002078 Error: 0.001861 Sparse: 0.021690\n",
      "| Loss :   0.001861 |\n",
      "| Sparse :   0.021690 |\n",
      "Step 4800: Minibatch Loss: 0.002096 Error: 0.001882 Sparse: 0.021421\n",
      "| Loss :   0.001882 |\n",
      "| Sparse :   0.021421 |\n",
      "Step 4900: Minibatch Loss: 0.002081 Error: 0.001871 Sparse: 0.021007\n",
      "| Loss :   0.001871 |\n",
      "| Sparse :   0.021007 |\n",
      "Step 5000: Minibatch Loss: 0.002075 Error: 0.001868 Sparse: 0.020727\n",
      "| Loss :   0.001868 |\n",
      "| Sparse :   0.020727 |\n",
      "This run of SparceLinAutoEncoderRELU8 ran for 0:01:21 and logs are available locally at: C:\\Users\\cjhamilton4176\\.hyperdash\\logs\\sparcelinautoencoderrelu8\\sparcelinautoencoderrelu8_2018-11-07t12-13-24-315807.log\n",
      "Step 1: Minibatch Loss: 0.015431 Error: 0.015390 Sparse: 0.000402\n",
      "| Loss :   0.015390 |\n",
      "| Sparse :   0.000402 |\n",
      "Step 100: Minibatch Loss: 0.015435 Error: 0.015431 Sparse: 0.000038\n",
      "| Loss :   0.015431 |\n",
      "| Sparse :   0.000038 |\n",
      "Step 200: Minibatch Loss: 0.014343 Error: 0.013846 Sparse: 0.004965\n",
      "| Loss :   0.013846 |\n",
      "| Sparse :   0.004965 |\n",
      "Step 300: Minibatch Loss: 0.011730 Error: 0.010706 Sparse: 0.010243\n",
      "| Loss :   0.010706 |\n",
      "| Sparse :   0.010243 |\n",
      "Step 400: Minibatch Loss: 0.009904 Error: 0.008587 Sparse: 0.013170\n",
      "| Loss :   0.008587 |\n",
      "| Sparse :   0.013170 |\n",
      "Step 500: Minibatch Loss: 0.008950 Error: 0.007488 Sparse: 0.014618\n",
      "| Loss :   0.007488 |\n",
      "| Sparse :   0.014618 |\n",
      "Step 600: Minibatch Loss: 0.008343 Error: 0.006878 Sparse: 0.014653\n",
      "| Loss :   0.006878 |\n",
      "| Sparse :   0.014653 |\n",
      "Step 700: Minibatch Loss: 0.007969 Error: 0.006538 Sparse: 0.014303\n",
      "| Loss :   0.006538 |\n",
      "| Sparse :   0.014303 |\n",
      "Step 800: Minibatch Loss: 0.007720 Error: 0.006342 Sparse: 0.013782\n",
      "| Loss :   0.006342 |\n",
      "| Sparse :   0.013782 |\n",
      "Step 900: Minibatch Loss: 0.007558 Error: 0.006244 Sparse: 0.013143\n",
      "| Loss :   0.006244 |\n",
      "| Sparse :   0.013143 |\n",
      "Step 1000: Minibatch Loss: 0.007427 Error: 0.006163 Sparse: 0.012641\n",
      "| Loss :   0.006163 |\n",
      "| Sparse :   0.012641 |\n",
      "Step 1100: Minibatch Loss: 0.007130 Error: 0.005870 Sparse: 0.012599\n",
      "| Loss :   0.005870 |\n",
      "| Sparse :   0.012599 |\n",
      "Step 1200: Minibatch Loss: 0.006766 Error: 0.005477 Sparse: 0.012884\n",
      "| Loss :   0.005477 |\n",
      "| Sparse :   0.012884 |\n",
      "Step 1300: Minibatch Loss: 0.006468 Error: 0.005166 Sparse: 0.013026\n",
      "| Loss :   0.005166 |\n",
      "| Sparse :   0.013026 |\n",
      "Step 1400: Minibatch Loss: 0.006193 Error: 0.004887 Sparse: 0.013063\n",
      "| Loss :   0.004887 |\n",
      "| Sparse :   0.013063 |\n",
      "Step 1500: Minibatch Loss: 0.005878 Error: 0.004543 Sparse: 0.013354\n",
      "| Loss :   0.004543 |\n",
      "| Sparse :   0.013354 |\n",
      "Step 1600: Minibatch Loss: 0.005464 Error: 0.004096 Sparse: 0.013682\n",
      "| Loss :   0.004096 |\n",
      "| Sparse :   0.013682 |\n",
      "Step 1700: Minibatch Loss: 0.005217 Error: 0.003847 Sparse: 0.013701\n",
      "| Loss :   0.003847 |\n",
      "| Sparse :   0.013701 |\n",
      "Step 1800: Minibatch Loss: 0.005015 Error: 0.003666 Sparse: 0.013493\n",
      "| Loss :   0.003666 |\n",
      "| Sparse :   0.013493 |\n",
      "Step 1900: Minibatch Loss: 0.004807 Error: 0.003452 Sparse: 0.013552\n",
      "| Loss :   0.003452 |\n",
      "| Sparse :   0.013552 |\n",
      "Step 2000: Minibatch Loss: 0.004482 Error: 0.003104 Sparse: 0.013781\n",
      "| Loss :   0.003104 |\n",
      "| Sparse :   0.013781 |\n",
      "Step 2100: Minibatch Loss: 0.004231 Error: 0.002835 Sparse: 0.013963\n",
      "| Loss :   0.002835 |\n",
      "| Sparse :   0.013963 |\n",
      "Step 2200: Minibatch Loss: 0.004052 Error: 0.002674 Sparse: 0.013781\n",
      "| Loss :   0.002674 |\n",
      "| Sparse :   0.013781 |\n",
      "Step 2300: Minibatch Loss: 0.003989 Error: 0.002638 Sparse: 0.013513\n",
      "| Loss :   0.002638 |\n",
      "| Sparse :   0.013513 |\n",
      "Step 2400: Minibatch Loss: 0.003825 Error: 0.002479 Sparse: 0.013459\n",
      "| Loss :   0.002479 |\n",
      "| Sparse :   0.013459 |\n",
      "Step 2500: Minibatch Loss: 0.003676 Error: 0.002342 Sparse: 0.013346\n",
      "| Loss :   0.002342 |\n",
      "| Sparse :   0.013346 |\n",
      "Step 2600: Minibatch Loss: 0.003558 Error: 0.002240 Sparse: 0.013183\n",
      "| Loss :   0.002240 |\n",
      "| Sparse :   0.013183 |\n",
      "Step 2700: Minibatch Loss: 0.003445 Error: 0.002150 Sparse: 0.012955\n",
      "| Loss :   0.002150 |\n",
      "| Sparse :   0.012955 |\n",
      "Step 2800: Minibatch Loss: 0.003371 Error: 0.002104 Sparse: 0.012672\n",
      "| Loss :   0.002104 |\n",
      "| Sparse :   0.012672 |\n",
      "Step 2900: Minibatch Loss: 0.003323 Error: 0.002081 Sparse: 0.012415\n",
      "| Loss :   0.002081 |\n",
      "| Sparse :   0.012415 |\n",
      "Step 3000: Minibatch Loss: 0.003312 Error: 0.002099 Sparse: 0.012132\n",
      "| Loss :   0.002099 |\n",
      "| Sparse :   0.012132 |\n",
      "Step 3100: Minibatch Loss: 0.003242 Error: 0.002055 Sparse: 0.011870\n",
      "| Loss :   0.002055 |\n",
      "| Sparse :   0.011870 |\n",
      "Step 3200: Minibatch Loss: 0.003232 Error: 0.002065 Sparse: 0.011665\n",
      "| Loss :   0.002065 |\n",
      "| Sparse :   0.011665 |\n",
      "Step 3300: Minibatch Loss: 0.003193 Error: 0.002050 Sparse: 0.011434\n",
      "| Loss :   0.002050 |\n",
      "| Sparse :   0.011434 |\n",
      "Step 3400: Minibatch Loss: 0.003123 Error: 0.002005 Sparse: 0.011182\n",
      "| Loss :   0.002005 |\n",
      "| Sparse :   0.011182 |\n",
      "Step 3500: Minibatch Loss: 0.003129 Error: 0.002033 Sparse: 0.010953\n",
      "| Loss :   0.002033 |\n",
      "| Sparse :   0.010953 |\n",
      "Step 3600: Minibatch Loss: 0.003083 Error: 0.002004 Sparse: 0.010785\n",
      "| Loss :   0.002004 |\n",
      "| Sparse :   0.010785 |\n",
      "Step 3700: Minibatch Loss: 0.003039 Error: 0.001982 Sparse: 0.010568\n",
      "| Loss :   0.001982 |\n",
      "| Sparse :   0.010568 |\n",
      "Step 3800: Minibatch Loss: 0.003057 Error: 0.002018 Sparse: 0.010388\n",
      "| Loss :   0.002018 |\n",
      "| Sparse :   0.010388 |\n",
      "Step 3900: Minibatch Loss: 0.002983 Error: 0.001964 Sparse: 0.010182\n",
      "| Loss :   0.001964 |\n",
      "| Sparse :   0.010182 |\n",
      "Step 4000: Minibatch Loss: 0.002982 Error: 0.001985 Sparse: 0.009975\n",
      "| Loss :   0.001985 |\n",
      "| Sparse :   0.009975 |\n",
      "Step 4100: Minibatch Loss: 0.002961 Error: 0.001978 Sparse: 0.009838\n",
      "| Loss :   0.001978 |\n",
      "| Sparse :   0.009838 |\n",
      "Step 4200: Minibatch Loss: 0.002940 Error: 0.001976 Sparse: 0.009646\n",
      "| Loss :   0.001976 |\n",
      "| Sparse :   0.009646 |\n",
      "Step 4300: Minibatch Loss: 0.002915 Error: 0.001964 Sparse: 0.009505\n",
      "| Loss :   0.001964 |\n",
      "| Sparse :   0.009505 |\n",
      "Step 4400: Minibatch Loss: 0.002907 Error: 0.001975 Sparse: 0.009328\n",
      "| Loss :   0.001975 |\n",
      "| Sparse :   0.009328 |\n",
      "Step 4500: Minibatch Loss: 0.002906 Error: 0.001990 Sparse: 0.009166\n",
      "| Loss :   0.001990 |\n",
      "| Sparse :   0.009166 |\n",
      "Step 4600: Minibatch Loss: 0.002887 Error: 0.001983 Sparse: 0.009039\n",
      "| Loss :   0.001983 |\n",
      "| Sparse :   0.009039 |\n",
      "Step 4700: Minibatch Loss: 0.002839 Error: 0.001947 Sparse: 0.008916\n",
      "| Loss :   0.001947 |\n",
      "| Sparse :   0.008916 |\n",
      "Step 4800: Minibatch Loss: 0.002865 Error: 0.001990 Sparse: 0.008751\n",
      "| Loss :   0.001990 |\n",
      "| Sparse :   0.008751 |\n",
      "Step 4900: Minibatch Loss: 0.002801 Error: 0.001940 Sparse: 0.008609\n",
      "| Loss :   0.001940 |\n",
      "| Sparse :   0.008609 |\n",
      "Step 5000: Minibatch Loss: 0.002809 Error: 0.001958 Sparse: 0.008515\n",
      "| Loss :   0.001958 |\n",
      "| Sparse :   0.008515 |\n",
      "This run of SparceLinAutoEncoderRELU8 ran for 0:01:21 and logs are available locally at: C:\\Users\\cjhamilton4176\\.hyperdash\\logs\\sparcelinautoencoderrelu8\\sparcelinautoencoderrelu8_2018-11-07t12-14-46-743463.log\n",
      "Step 1: Minibatch Loss: 0.015795 Error: 0.015409 Sparse: 0.000386\n",
      "| Loss :   0.015409 |\n",
      "| Sparse :   0.000386 |\n",
      "Step 100: Minibatch Loss: 0.015469 Error: 0.015428 Sparse: 0.000041\n",
      "| Loss :   0.015428 |\n",
      "| Sparse :   0.000041 |\n",
      "Step 200: Minibatch Loss: 0.015420 Error: 0.015384 Sparse: 0.000036\n",
      "| Loss :   0.015384 |\n",
      "| Sparse :   0.000036 |\n",
      "Step 300: Minibatch Loss: 0.015453 Error: 0.015412 Sparse: 0.000041\n",
      "| Loss :   0.015412 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Sparse :   0.000041 |\n",
      "Step 400: Minibatch Loss: 0.015428 Error: 0.015390 Sparse: 0.000038\n",
      "| Loss :   0.015390 |\n",
      "| Sparse :   0.000038 |\n",
      "Step 500: Minibatch Loss: 0.015479 Error: 0.015443 Sparse: 0.000036\n",
      "| Loss :   0.015443 |\n",
      "| Sparse :   0.000036 |\n",
      "Step 600: Minibatch Loss: 0.015475 Error: 0.015436 Sparse: 0.000039\n",
      "| Loss :   0.015436 |\n",
      "| Sparse :   0.000039 |\n",
      "Step 700: Minibatch Loss: 0.015479 Error: 0.015441 Sparse: 0.000038\n",
      "| Loss :   0.015441 |\n",
      "| Sparse :   0.000038 |\n",
      "Step 800: Minibatch Loss: 0.015464 Error: 0.015418 Sparse: 0.000046\n",
      "| Loss :   0.015418 |\n",
      "| Sparse :   0.000046 |\n",
      "Step 900: Minibatch Loss: 0.015419 Error: 0.015375 Sparse: 0.000044\n",
      "| Loss :   0.015375 |\n",
      "| Sparse :   0.000044 |\n",
      "Step 1000: Minibatch Loss: 0.015171 Error: 0.014830 Sparse: 0.000341\n",
      "| Loss :   0.014830 |\n",
      "| Sparse :   0.000341 |\n",
      "Step 1100: Minibatch Loss: 0.015026 Error: 0.014569 Sparse: 0.000458\n",
      "| Loss :   0.014569 |\n",
      "| Sparse :   0.000458 |\n",
      "Step 1200: Minibatch Loss: 0.014910 Error: 0.014491 Sparse: 0.000419\n",
      "| Loss :   0.014491 |\n",
      "| Sparse :   0.000419 |\n",
      "Step 1300: Minibatch Loss: 0.014854 Error: 0.014407 Sparse: 0.000448\n",
      "| Loss :   0.014407 |\n",
      "| Sparse :   0.000448 |\n",
      "Step 1400: Minibatch Loss: 0.014518 Error: 0.013749 Sparse: 0.000769\n",
      "| Loss :   0.013749 |\n",
      "| Sparse :   0.000769 |\n",
      "Step 1500: Minibatch Loss: 0.014339 Error: 0.013478 Sparse: 0.000860\n",
      "| Loss :   0.013478 |\n",
      "| Sparse :   0.000860 |\n",
      "Step 1600: Minibatch Loss: 0.013995 Error: 0.012980 Sparse: 0.001016\n",
      "| Loss :   0.012980 |\n",
      "| Sparse :   0.001016 |\n",
      "Step 1700: Minibatch Loss: 0.013397 Error: 0.011914 Sparse: 0.001483\n",
      "| Loss :   0.011914 |\n",
      "| Sparse :   0.001483 |\n",
      "Step 1800: Minibatch Loss: 0.013168 Error: 0.011738 Sparse: 0.001430\n",
      "| Loss :   0.011738 |\n",
      "| Sparse :   0.001430 |\n",
      "Step 1900: Minibatch Loss: 0.013053 Error: 0.011679 Sparse: 0.001373\n",
      "| Loss :   0.011679 |\n",
      "| Sparse :   0.001373 |\n",
      "Step 2000: Minibatch Loss: 0.012928 Error: 0.011607 Sparse: 0.001321\n",
      "| Loss :   0.011607 |\n",
      "| Sparse :   0.001321 |\n",
      "Step 2100: Minibatch Loss: 0.012860 Error: 0.011586 Sparse: 0.001275\n",
      "| Loss :   0.011586 |\n",
      "| Sparse :   0.001275 |\n",
      "Step 2200: Minibatch Loss: 0.012770 Error: 0.011533 Sparse: 0.001237\n",
      "| Loss :   0.011533 |\n",
      "| Sparse :   0.001237 |\n",
      "Step 2300: Minibatch Loss: 0.012667 Error: 0.011466 Sparse: 0.001201\n",
      "| Loss :   0.011466 |\n",
      "| Sparse :   0.001201 |\n",
      "Step 2400: Minibatch Loss: 0.012692 Error: 0.011528 Sparse: 0.001164\n",
      "| Loss :   0.011528 |\n",
      "| Sparse :   0.001164 |\n",
      "Step 2500: Minibatch Loss: 0.012694 Error: 0.011562 Sparse: 0.001132\n",
      "| Loss :   0.011562 |\n",
      "| Sparse :   0.001132 |\n",
      "Step 2600: Minibatch Loss: 0.012619 Error: 0.011510 Sparse: 0.001109\n",
      "| Loss :   0.011510 |\n",
      "| Sparse :   0.001109 |\n",
      "Step 2700: Minibatch Loss: 0.012585 Error: 0.011508 Sparse: 0.001077\n",
      "| Loss :   0.011508 |\n",
      "| Sparse :   0.001077 |\n",
      "Step 2800: Minibatch Loss: 0.012461 Error: 0.011277 Sparse: 0.001184\n",
      "| Loss :   0.011277 |\n",
      "| Sparse :   0.001184 |\n",
      "Step 2900: Minibatch Loss: 0.012189 Error: 0.010837 Sparse: 0.001352\n",
      "| Loss :   0.010837 |\n",
      "| Sparse :   0.001352 |\n",
      "Step 3000: Minibatch Loss: 0.012031 Error: 0.010591 Sparse: 0.001440\n",
      "| Loss :   0.010591 |\n",
      "| Sparse :   0.001440 |\n",
      "Step 3100: Minibatch Loss: 0.011994 Error: 0.010590 Sparse: 0.001404\n",
      "| Loss :   0.010590 |\n",
      "| Sparse :   0.001404 |\n",
      "Step 3200: Minibatch Loss: 0.011917 Error: 0.010533 Sparse: 0.001384\n",
      "| Loss :   0.010533 |\n",
      "| Sparse :   0.001384 |\n",
      "Step 3300: Minibatch Loss: 0.011902 Error: 0.010567 Sparse: 0.001335\n",
      "| Loss :   0.010567 |\n",
      "| Sparse :   0.001335 |\n",
      "Step 3400: Minibatch Loss: 0.011772 Error: 0.010458 Sparse: 0.001315\n",
      "| Loss :   0.010458 |\n",
      "| Sparse :   0.001315 |\n",
      "Step 3500: Minibatch Loss: 0.011667 Error: 0.010352 Sparse: 0.001316\n",
      "| Loss :   0.010352 |\n",
      "| Sparse :   0.001316 |\n",
      "Step 3600: Minibatch Loss: 0.011537 Error: 0.010063 Sparse: 0.001473\n",
      "| Loss :   0.010063 |\n",
      "| Sparse :   0.001473 |\n",
      "Step 3700: Minibatch Loss: 0.011389 Error: 0.009810 Sparse: 0.001579\n",
      "| Loss :   0.009810 |\n",
      "| Sparse :   0.001579 |\n",
      "Step 3800: Minibatch Loss: 0.011246 Error: 0.009590 Sparse: 0.001656\n",
      "| Loss :   0.009590 |\n",
      "| Sparse :   0.001656 |\n",
      "Step 3900: Minibatch Loss: 0.011050 Error: 0.009369 Sparse: 0.001681\n",
      "| Loss :   0.009369 |\n",
      "| Sparse :   0.001681 |\n",
      "Step 4000: Minibatch Loss: 0.010952 Error: 0.009279 Sparse: 0.001673\n",
      "| Loss :   0.009279 |\n",
      "| Sparse :   0.001673 |\n",
      "Step 4100: Minibatch Loss: 0.010934 Error: 0.009241 Sparse: 0.001693\n",
      "| Loss :   0.009241 |\n",
      "| Sparse :   0.001693 |\n",
      "Step 4200: Minibatch Loss: 0.010726 Error: 0.008986 Sparse: 0.001740\n",
      "| Loss :   0.008986 |\n",
      "| Sparse :   0.001740 |\n",
      "Step 4300: Minibatch Loss: 0.010644 Error: 0.008909 Sparse: 0.001735\n",
      "| Loss :   0.008909 |\n",
      "| Sparse :   0.001735 |\n",
      "Step 4400: Minibatch Loss: 0.010533 Error: 0.008822 Sparse: 0.001711\n",
      "| Loss :   0.008822 |\n",
      "| Sparse :   0.001711 |\n",
      "Step 4500: Minibatch Loss: 0.010479 Error: 0.008739 Sparse: 0.001740\n",
      "| Loss :   0.008739 |\n",
      "| Sparse :   0.001740 |\n",
      "Step 4600: Minibatch Loss: 0.010372 Error: 0.008586 Sparse: 0.001786\n",
      "| Loss :   0.008586 |\n",
      "| Sparse :   0.001786 |\n",
      "Step 4700: Minibatch Loss: 0.010233 Error: 0.008392 Sparse: 0.001841\n",
      "| Loss :   0.008392 |\n",
      "| Sparse :   0.001841 |\n",
      "Step 4800: Minibatch Loss: 0.010119 Error: 0.008281 Sparse: 0.001838\n",
      "| Loss :   0.008281 |\n",
      "| Sparse :   0.001838 |\n",
      "Step 4900: Minibatch Loss: 0.010051 Error: 0.008203 Sparse: 0.001848\n",
      "| Loss :   0.008203 |\n",
      "| Sparse :   0.001848 |\n",
      "Step 5000: Minibatch Loss: 0.009864 Error: 0.007997 Sparse: 0.001867\n",
      "| Loss :   0.007997 |\n",
      "| Sparse :   0.001867 |\n",
      "This run of SparceLinAutoEncoderRELU8 ran for 0:01:20 and logs are available locally at: C:\\Users\\cjhamilton4176\\.hyperdash\\logs\\sparcelinautoencoderrelu8\\sparcelinautoencoderrelu8_2018-11-07t12-16-09-062181.log\n",
      "Step 1: Minibatch Loss: 0.015400 Error: 0.015400 Sparse: 0.000392\n",
      "| Loss :   0.015400 |\n",
      "| Sparse :   0.000392 |\n",
      "Step 100: Minibatch Loss: 0.003229 Error: 0.003223 Sparse: 0.062383\n",
      "| Loss :   0.003223 |\n",
      "| Sparse :   0.062383 |\n",
      "Step 200: Minibatch Loss: 0.002275 Error: 0.002269 Sparse: 0.061281\n",
      "| Loss :   0.002269 |\n",
      "| Sparse :   0.061281 |\n",
      "Step 300: Minibatch Loss: 0.002052 Error: 0.002046 Sparse: 0.060534\n",
      "| Loss :   0.002046 |\n",
      "| Sparse :   0.060534 |\n",
      "Step 400: Minibatch Loss: 0.001926 Error: 0.001920 Sparse: 0.060138\n",
      "| Loss :   0.001920 |\n",
      "| Sparse :   0.060138 |\n",
      "Step 500: Minibatch Loss: 0.001901 Error: 0.001895 Sparse: 0.059667\n",
      "| Loss :   0.001895 |\n",
      "| Sparse :   0.059667 |\n",
      "Step 600: Minibatch Loss: 0.001870 Error: 0.001864 Sparse: 0.058965\n",
      "| Loss :   0.001864 |\n",
      "| Sparse :   0.058965 |\n",
      "Step 700: Minibatch Loss: 0.001886 Error: 0.001880 Sparse: 0.058255\n",
      "| Loss :   0.001880 |\n",
      "| Sparse :   0.058255 |\n",
      "Step 800: Minibatch Loss: 0.001860 Error: 0.001854 Sparse: 0.057362\n",
      "| Loss :   0.001854 |\n",
      "| Sparse :   0.057362 |\n",
      "Step 900: Minibatch Loss: 0.001850 Error: 0.001845 Sparse: 0.056621\n",
      "| Loss :   0.001845 |\n",
      "| Sparse :   0.056621 |\n",
      "Step 1000: Minibatch Loss: 0.001852 Error: 0.001847 Sparse: 0.055897\n",
      "| Loss :   0.001847 |\n",
      "| Sparse :   0.055897 |\n",
      "This run of SparceLinAutoEncoderRELU8 ran for 0:00:21 and logs are available locally at: C:\\Users\\cjhamilton4176\\.hyperdash\\logs\\sparcelinautoencoderrelu8\\sparcelinautoencoderrelu8_2018-11-07t12-17-30-547760.log\n",
      "Step 1: Minibatch Loss: 0.015425 Error: 0.015425 Sparse: 0.000399\n",
      "| Loss :   0.015425 |\n",
      "| Sparse :   0.000399 |\n",
      "Step 100: Minibatch Loss: 0.003562 Error: 0.003502 Sparse: 0.059839\n",
      "| Loss :   0.003502 |\n",
      "| Sparse :   0.059839 |\n",
      "Step 200: Minibatch Loss: 0.002348 Error: 0.002290 Sparse: 0.058392\n",
      "| Loss :   0.002290 |\n",
      "| Sparse :   0.058392 |\n",
      "Step 300: Minibatch Loss: 0.002110 Error: 0.002055 Sparse: 0.055550\n",
      "| Loss :   0.002055 |\n",
      "| Sparse :   0.055550 |\n",
      "Step 400: Minibatch Loss: 0.001998 Error: 0.001944 Sparse: 0.053721\n",
      "| Loss :   0.001944 |\n",
      "| Sparse :   0.053721 |\n",
      "Step 500: Minibatch Loss: 0.001984 Error: 0.001932 Sparse: 0.051675\n",
      "| Loss :   0.001932 |\n",
      "| Sparse :   0.051675 |\n",
      "Step 600: Minibatch Loss: 0.001948 Error: 0.001899 Sparse: 0.049628\n",
      "| Loss :   0.001899 |\n",
      "| Sparse :   0.049628 |\n",
      "Step 700: Minibatch Loss: 0.001917 Error: 0.001869 Sparse: 0.047737\n",
      "| Loss :   0.001869 |\n",
      "| Sparse :   0.047737 |\n",
      "Step 800: Minibatch Loss: 0.001920 Error: 0.001874 Sparse: 0.045988\n",
      "| Loss :   0.001874 |\n",
      "| Sparse :   0.045988 |\n",
      "Step 900: Minibatch Loss: 0.001901 Error: 0.001857 Sparse: 0.044337\n",
      "| Loss :   0.001857 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Sparse :   0.044337 |\n",
      "Step 1000: Minibatch Loss: 0.001917 Error: 0.001874 Sparse: 0.042826\n",
      "| Loss :   0.001874 |\n",
      "| Sparse :   0.042826 |\n",
      "This run of SparceLinAutoEncoderRELU8 ran for 0:00:22 and logs are available locally at: C:\\Users\\cjhamilton4176\\.hyperdash\\logs\\sparcelinautoencoderrelu8\\sparcelinautoencoderrelu8_2018-11-07t12-17-52-452903.log\n",
      "Step 1: Minibatch Loss: 0.015422 Error: 0.015419 Sparse: 0.000387\n",
      "| Loss :   0.015419 |\n",
      "| Sparse :   0.000387 |\n",
      "Step 100: Minibatch Loss: 0.003944 Error: 0.003459 Sparse: 0.048566\n",
      "| Loss :   0.003459 |\n",
      "| Sparse :   0.048566 |\n",
      "Step 200: Minibatch Loss: 0.002699 Error: 0.002269 Sparse: 0.043075\n",
      "| Loss :   0.002269 |\n",
      "| Sparse :   0.043075 |\n",
      "Step 300: Minibatch Loss: 0.002438 Error: 0.002069 Sparse: 0.036882\n",
      "| Loss :   0.002069 |\n",
      "| Sparse :   0.036882 |\n",
      "Step 400: Minibatch Loss: 0.002300 Error: 0.001976 Sparse: 0.032440\n",
      "| Loss :   0.001976 |\n",
      "| Sparse :   0.032440 |\n",
      "Step 500: Minibatch Loss: 0.002238 Error: 0.001946 Sparse: 0.029236\n",
      "| Loss :   0.001946 |\n",
      "| Sparse :   0.029236 |\n",
      "Step 600: Minibatch Loss: 0.002188 Error: 0.001922 Sparse: 0.026648\n",
      "| Loss :   0.001922 |\n",
      "| Sparse :   0.026648 |\n",
      "Step 700: Minibatch Loss: 0.002144 Error: 0.001897 Sparse: 0.024643\n",
      "| Loss :   0.001897 |\n",
      "| Sparse :   0.024643 |\n",
      "Step 800: Minibatch Loss: 0.002093 Error: 0.001863 Sparse: 0.022952\n",
      "| Loss :   0.001863 |\n",
      "| Sparse :   0.022952 |\n",
      "Step 900: Minibatch Loss: 0.002120 Error: 0.001904 Sparse: 0.021628\n",
      "| Loss :   0.001904 |\n",
      "| Sparse :   0.021628 |\n",
      "Step 1000: Minibatch Loss: 0.002090 Error: 0.001885 Sparse: 0.020468\n",
      "| Loss :   0.001885 |\n",
      "| Sparse :   0.020468 |\n",
      "This run of SparceLinAutoEncoderRELU8 ran for 0:00:22 and logs are available locally at: C:\\Users\\cjhamilton4176\\.hyperdash\\logs\\sparcelinautoencoderrelu8\\sparcelinautoencoderrelu8_2018-11-07t12-18-15-762830.log\n",
      "Step 1: Minibatch Loss: 0.015464 Error: 0.015425 Sparse: 0.000393\n",
      "| Loss :   0.015425 |\n",
      "| Sparse :   0.000393 |\n",
      "Step 100: Minibatch Loss: 0.005982 Error: 0.003901 Sparse: 0.020808\n",
      "| Loss :   0.003901 |\n",
      "| Sparse :   0.020808 |\n",
      "Step 200: Minibatch Loss: 0.004336 Error: 0.002642 Sparse: 0.016947\n",
      "| Loss :   0.002642 |\n",
      "| Sparse :   0.016947 |\n",
      "Step 300: Minibatch Loss: 0.003755 Error: 0.002338 Sparse: 0.014171\n",
      "| Loss :   0.002338 |\n",
      "| Sparse :   0.014171 |\n",
      "Step 400: Minibatch Loss: 0.003484 Error: 0.002246 Sparse: 0.012382\n",
      "| Loss :   0.002246 |\n",
      "| Sparse :   0.012382 |\n",
      "Step 500: Minibatch Loss: 0.003314 Error: 0.002205 Sparse: 0.011087\n",
      "| Loss :   0.002205 |\n",
      "| Sparse :   0.011087 |\n",
      "Step 600: Minibatch Loss: 0.003104 Error: 0.002074 Sparse: 0.010296\n",
      "| Loss :   0.002074 |\n",
      "| Sparse :   0.010296 |\n",
      "Step 700: Minibatch Loss: 0.003023 Error: 0.002072 Sparse: 0.009502\n",
      "| Loss :   0.002072 |\n",
      "| Sparse :   0.009502 |\n",
      "Step 800: Minibatch Loss: 0.002910 Error: 0.002019 Sparse: 0.008917\n",
      "| Loss :   0.002019 |\n",
      "| Sparse :   0.008917 |\n",
      "Step 900: Minibatch Loss: 0.002866 Error: 0.002027 Sparse: 0.008394\n",
      "| Loss :   0.002027 |\n",
      "| Sparse :   0.008394 |\n",
      "Step 1000: Minibatch Loss: 0.002755 Error: 0.001960 Sparse: 0.007957\n",
      "| Loss :   0.001960 |\n",
      "| Sparse :   0.007957 |\n",
      "This run of SparceLinAutoEncoderRELU8 ran for 0:00:22 and logs are available locally at: C:\\Users\\cjhamilton4176\\.hyperdash\\logs\\sparcelinautoencoderrelu8\\sparcelinautoencoderrelu8_2018-11-07t12-18-38-537008.log\n",
      "Step 1: Minibatch Loss: 0.015820 Error: 0.015425 Sparse: 0.000395\n",
      "| Loss :   0.015425 |\n",
      "| Sparse :   0.000395 |\n",
      "Step 100: Minibatch Loss: 0.015717 Error: 0.015125 Sparse: 0.000592\n",
      "| Loss :   0.015125 |\n",
      "| Sparse :   0.000592 |\n",
      "Step 200: Minibatch Loss: 0.013084 Error: 0.011290 Sparse: 0.001794\n",
      "| Loss :   0.011290 |\n",
      "| Sparse :   0.001794 |\n",
      "Step 300: Minibatch Loss: 0.011628 Error: 0.009685 Sparse: 0.001943\n",
      "| Loss :   0.009685 |\n",
      "| Sparse :   0.001943 |\n",
      "Step 400: Minibatch Loss: 0.010333 Error: 0.008092 Sparse: 0.002241\n",
      "| Loss :   0.008092 |\n",
      "| Sparse :   0.002241 |\n",
      "Step 500: Minibatch Loss: 0.009775 Error: 0.007753 Sparse: 0.002022\n",
      "| Loss :   0.007753 |\n",
      "| Sparse :   0.002022 |\n",
      "Step 600: Minibatch Loss: 0.009123 Error: 0.007024 Sparse: 0.002099\n",
      "| Loss :   0.007024 |\n",
      "| Sparse :   0.002099 |\n",
      "Step 700: Minibatch Loss: 0.008498 Error: 0.006313 Sparse: 0.002185\n",
      "| Loss :   0.006313 |\n",
      "| Sparse :   0.002185 |\n",
      "Step 800: Minibatch Loss: 0.008046 Error: 0.005807 Sparse: 0.002239\n",
      "| Loss :   0.005807 |\n",
      "| Sparse :   0.002239 |\n",
      "Step 900: Minibatch Loss: 0.007442 Error: 0.005141 Sparse: 0.002302\n",
      "| Loss :   0.005141 |\n",
      "| Sparse :   0.002302 |\n",
      "Step 1000: Minibatch Loss: 0.007028 Error: 0.004790 Sparse: 0.002238\n",
      "| Loss :   0.004790 |\n",
      "| Sparse :   0.002238 |\n",
      "This run of SparceLinAutoEncoderRELU8 ran for 0:00:23 and logs are available locally at: C:\\Users\\cjhamilton4176\\.hyperdash\\logs\\sparcelinautoencoderrelu8\\sparcelinautoencoderrelu8_2018-11-07t12-19-01-282820.log\n",
      "Step 1: Minibatch Loss: 0.015434 Error: 0.015434 Sparse: 0.000385\n",
      "| Loss :   0.015434 |\n",
      "| Sparse :   0.000385 |\n",
      "Step 100: Minibatch Loss: 0.003208 Error: 0.003202 Sparse: 0.062346\n",
      "| Loss :   0.003202 |\n",
      "| Sparse :   0.062346 |\n",
      "Step 200: Minibatch Loss: 0.002283 Error: 0.002276 Sparse: 0.061183\n",
      "| Loss :   0.002276 |\n",
      "| Sparse :   0.061183 |\n",
      "Step 300: Minibatch Loss: 0.002088 Error: 0.002082 Sparse: 0.060412\n",
      "| Loss :   0.002082 |\n",
      "| Sparse :   0.060412 |\n",
      "Step 400: Minibatch Loss: 0.001970 Error: 0.001965 Sparse: 0.059300\n",
      "| Loss :   0.001965 |\n",
      "| Sparse :   0.059300 |\n",
      "Step 500: Minibatch Loss: 0.001905 Error: 0.001899 Sparse: 0.058921\n",
      "| Loss :   0.001899 |\n",
      "| Sparse :   0.058921 |\n",
      "Step 600: Minibatch Loss: 0.001936 Error: 0.001930 Sparse: 0.058040\n",
      "| Loss :   0.001930 |\n",
      "| Sparse :   0.058040 |\n",
      "Step 700: Minibatch Loss: 0.001896 Error: 0.001890 Sparse: 0.057195\n",
      "| Loss :   0.001890 |\n",
      "| Sparse :   0.057195 |\n",
      "Step 800: Minibatch Loss: 0.001908 Error: 0.001903 Sparse: 0.056370\n",
      "| Loss :   0.001903 |\n",
      "| Sparse :   0.056370 |\n",
      "Step 900: Minibatch Loss: 0.001929 Error: 0.001924 Sparse: 0.056204\n",
      "| Loss :   0.001924 |\n",
      "| Sparse :   0.056204 |\n",
      "Step 1000: Minibatch Loss: 0.001902 Error: 0.001896 Sparse: 0.055131\n",
      "| Loss :   0.001896 |\n",
      "| Sparse :   0.055131 |\n",
      "Step 1100: Minibatch Loss: 0.001890 Error: 0.001884 Sparse: 0.054576\n",
      "| Loss :   0.001884 |\n",
      "| Sparse :   0.054576 |\n",
      "Step 1200: Minibatch Loss: 0.001872 Error: 0.001866 Sparse: 0.053754\n",
      "| Loss :   0.001866 |\n",
      "| Sparse :   0.053754 |\n",
      "Step 1300: Minibatch Loss: 0.001871 Error: 0.001866 Sparse: 0.053142\n",
      "| Loss :   0.001866 |\n",
      "| Sparse :   0.053142 |\n",
      "Step 1400: Minibatch Loss: 0.001871 Error: 0.001865 Sparse: 0.052711\n",
      "| Loss :   0.001865 |\n",
      "| Sparse :   0.052711 |\n",
      "Step 1500: Minibatch Loss: 0.001865 Error: 0.001860 Sparse: 0.051995\n",
      "| Loss :   0.001860 |\n",
      "| Sparse :   0.051995 |\n",
      "Step 1600: Minibatch Loss: 0.001823 Error: 0.001818 Sparse: 0.051208\n",
      "| Loss :   0.001818 |\n",
      "| Sparse :   0.051208 |\n",
      "Step 1700: Minibatch Loss: 0.001853 Error: 0.001848 Sparse: 0.050720\n",
      "| Loss :   0.001848 |\n",
      "| Sparse :   0.050720 |\n",
      "Step 1800: Minibatch Loss: 0.001843 Error: 0.001838 Sparse: 0.049978\n",
      "| Loss :   0.001838 |\n",
      "| Sparse :   0.049978 |\n",
      "Step 1900: Minibatch Loss: 0.001882 Error: 0.001877 Sparse: 0.049680\n",
      "| Loss :   0.001877 |\n",
      "| Sparse :   0.049680 |\n",
      "Step 2000: Minibatch Loss: 0.001875 Error: 0.001870 Sparse: 0.049193\n",
      "| Loss :   0.001870 |\n",
      "| Sparse :   0.049193 |\n",
      "This run of SparceLinAutoEncoderRELU8 ran for 0:00:38 and logs are available locally at: C:\\Users\\cjhamilton4176\\.hyperdash\\logs\\sparcelinautoencoderrelu8\\sparcelinautoencoderrelu8_2018-11-07t12-19-25-236842.log\n",
      "Step 1: Minibatch Loss: 0.015432 Error: 0.015431 Sparse: 0.000393\n",
      "| Loss :   0.015431 |\n",
      "| Sparse :   0.000393 |\n",
      "Step 100: Minibatch Loss: 0.003501 Error: 0.003440 Sparse: 0.060348\n",
      "| Loss :   0.003440 |\n",
      "| Sparse :   0.060348 |\n",
      "Step 200: Minibatch Loss: 0.002332 Error: 0.002272 Sparse: 0.059637\n",
      "| Loss :   0.002272 |\n",
      "| Sparse :   0.059637 |\n",
      "Step 300: Minibatch Loss: 0.002126 Error: 0.002070 Sparse: 0.056431\n",
      "| Loss :   0.002070 |\n",
      "| Sparse :   0.056431 |\n",
      "Step 400: Minibatch Loss: 0.002024 Error: 0.001970 Sparse: 0.054153\n",
      "| Loss :   0.001970 |\n",
      "| Sparse :   0.054153 |\n",
      "Step 500: Minibatch Loss: 0.001980 Error: 0.001928 Sparse: 0.052120\n",
      "| Loss :   0.001928 |\n",
      "| Sparse :   0.052120 |\n",
      "Step 600: Minibatch Loss: 0.001949 Error: 0.001899 Sparse: 0.050219\n",
      "| Loss :   0.001899 |\n",
      "| Sparse :   0.050219 |\n",
      "Step 700: Minibatch Loss: 0.001966 Error: 0.001918 Sparse: 0.048307\n",
      "| Loss :   0.001918 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Sparse :   0.048307 |\n",
      "Step 800: Minibatch Loss: 0.001950 Error: 0.001904 Sparse: 0.046342\n",
      "| Loss :   0.001904 |\n",
      "| Sparse :   0.046342 |\n",
      "Step 900: Minibatch Loss: 0.001874 Error: 0.001830 Sparse: 0.044678\n",
      "| Loss :   0.001830 |\n",
      "| Sparse :   0.044678 |\n",
      "Step 1000: Minibatch Loss: 0.001926 Error: 0.001883 Sparse: 0.043337\n",
      "| Loss :   0.001883 |\n",
      "| Sparse :   0.043337 |\n",
      "Step 1100: Minibatch Loss: 0.001928 Error: 0.001886 Sparse: 0.041953\n",
      "| Loss :   0.001886 |\n",
      "| Sparse :   0.041953 |\n",
      "Step 1200: Minibatch Loss: 0.001941 Error: 0.001900 Sparse: 0.040468\n",
      "| Loss :   0.001900 |\n",
      "| Sparse :   0.040468 |\n",
      "Step 1300: Minibatch Loss: 0.001930 Error: 0.001890 Sparse: 0.039260\n",
      "| Loss :   0.001890 |\n",
      "| Sparse :   0.039260 |\n",
      "Step 1400: Minibatch Loss: 0.001893 Error: 0.001855 Sparse: 0.038067\n",
      "| Loss :   0.001855 |\n",
      "| Sparse :   0.038067 |\n",
      "Step 1500: Minibatch Loss: 0.001896 Error: 0.001859 Sparse: 0.036776\n",
      "| Loss :   0.001859 |\n",
      "| Sparse :   0.036776 |\n",
      "Step 1600: Minibatch Loss: 0.001893 Error: 0.001857 Sparse: 0.035941\n",
      "| Loss :   0.001857 |\n",
      "| Sparse :   0.035941 |\n",
      "Step 1700: Minibatch Loss: 0.001907 Error: 0.001872 Sparse: 0.034982\n",
      "| Loss :   0.001872 |\n",
      "| Sparse :   0.034982 |\n",
      "Step 1800: Minibatch Loss: 0.001902 Error: 0.001868 Sparse: 0.034089\n",
      "| Loss :   0.001868 |\n",
      "| Sparse :   0.034089 |\n",
      "Step 1900: Minibatch Loss: 0.001926 Error: 0.001893 Sparse: 0.033216\n",
      "| Loss :   0.001893 |\n",
      "| Sparse :   0.033216 |\n",
      "Step 2000: Minibatch Loss: 0.001911 Error: 0.001878 Sparse: 0.032596\n",
      "| Loss :   0.001878 |\n",
      "| Sparse :   0.032596 |\n",
      "This run of SparceLinAutoEncoderRELU8 ran for 0:00:37 and logs are available locally at: C:\\Users\\cjhamilton4176\\.hyperdash\\logs\\sparcelinautoencoderrelu8\\sparcelinautoencoderrelu8_2018-11-07t12-20-04-186530.log\n",
      "Step 1: Minibatch Loss: 0.015404 Error: 0.015400 Sparse: 0.000387\n",
      "| Loss :   0.015400 |\n",
      "| Sparse :   0.000387 |\n",
      "Step 100: Minibatch Loss: 0.003864 Error: 0.003374 Sparse: 0.048994\n",
      "| Loss :   0.003374 |\n",
      "| Sparse :   0.048994 |\n",
      "Step 200: Minibatch Loss: 0.002635 Error: 0.002206 Sparse: 0.042863\n",
      "| Loss :   0.002206 |\n",
      "| Sparse :   0.042863 |\n",
      "Step 300: Minibatch Loss: 0.002432 Error: 0.002064 Sparse: 0.036725\n",
      "| Loss :   0.002064 |\n",
      "| Sparse :   0.036725 |\n",
      "Step 400: Minibatch Loss: 0.002293 Error: 0.001968 Sparse: 0.032496\n",
      "| Loss :   0.001968 |\n",
      "| Sparse :   0.032496 |\n",
      "Step 500: Minibatch Loss: 0.002188 Error: 0.001896 Sparse: 0.029127\n",
      "| Loss :   0.001896 |\n",
      "| Sparse :   0.029127 |\n",
      "Step 600: Minibatch Loss: 0.002229 Error: 0.001963 Sparse: 0.026611\n",
      "| Loss :   0.001963 |\n",
      "| Sparse :   0.026611 |\n",
      "Step 700: Minibatch Loss: 0.002145 Error: 0.001900 Sparse: 0.024490\n",
      "| Loss :   0.001900 |\n",
      "| Sparse :   0.024490 |\n",
      "Step 800: Minibatch Loss: 0.002129 Error: 0.001899 Sparse: 0.023091\n",
      "| Loss :   0.001899 |\n",
      "| Sparse :   0.023091 |\n",
      "Step 900: Minibatch Loss: 0.002105 Error: 0.001888 Sparse: 0.021658\n",
      "| Loss :   0.001888 |\n",
      "| Sparse :   0.021658 |\n",
      "Step 1000: Minibatch Loss: 0.002087 Error: 0.001883 Sparse: 0.020406\n",
      "| Loss :   0.001883 |\n",
      "| Sparse :   0.020406 |\n",
      "Step 1100: Minibatch Loss: 0.002047 Error: 0.001853 Sparse: 0.019383\n",
      "| Loss :   0.001853 |\n",
      "| Sparse :   0.019383 |\n",
      "Step 1200: Minibatch Loss: 0.002043 Error: 0.001857 Sparse: 0.018607\n",
      "| Loss :   0.001857 |\n",
      "| Sparse :   0.018607 |\n",
      "Step 1300: Minibatch Loss: 0.002049 Error: 0.001870 Sparse: 0.017907\n",
      "| Loss :   0.001870 |\n",
      "| Sparse :   0.017907 |\n",
      "Step 1400: Minibatch Loss: 0.002056 Error: 0.001884 Sparse: 0.017190\n",
      "| Loss :   0.001884 |\n",
      "| Sparse :   0.017190 |\n",
      "Step 1500: Minibatch Loss: 0.002063 Error: 0.001898 Sparse: 0.016487\n",
      "| Loss :   0.001898 |\n",
      "| Sparse :   0.016487 |\n",
      "Step 1600: Minibatch Loss: 0.002011 Error: 0.001852 Sparse: 0.015923\n",
      "| Loss :   0.001852 |\n",
      "| Sparse :   0.015923 |\n",
      "Step 1700: Minibatch Loss: 0.002027 Error: 0.001873 Sparse: 0.015448\n",
      "| Loss :   0.001873 |\n",
      "| Sparse :   0.015448 |\n",
      "Step 1800: Minibatch Loss: 0.002033 Error: 0.001882 Sparse: 0.015026\n",
      "| Loss :   0.001882 |\n",
      "| Sparse :   0.015026 |\n",
      "Step 1900: Minibatch Loss: 0.002024 Error: 0.001879 Sparse: 0.014573\n",
      "| Loss :   0.001879 |\n",
      "| Sparse :   0.014573 |\n",
      "Step 2000: Minibatch Loss: 0.001984 Error: 0.001842 Sparse: 0.014188\n",
      "| Loss :   0.001842 |\n",
      "| Sparse :   0.014188 |\n",
      "This run of SparceLinAutoEncoderRELU8 ran for 0:00:39 and logs are available locally at: C:\\Users\\cjhamilton4176\\.hyperdash\\logs\\sparcelinautoencoderrelu8\\sparcelinautoencoderrelu8_2018-11-07t12-20-43-131678.log\n",
      "Step 1: Minibatch Loss: 0.015424 Error: 0.015384 Sparse: 0.000395\n",
      "| Loss :   0.015384 |\n",
      "| Sparse :   0.000395 |\n",
      "Step 100: Minibatch Loss: 0.005981 Error: 0.003870 Sparse: 0.021115\n",
      "| Loss :   0.003870 |\n",
      "| Sparse :   0.021115 |\n",
      "Step 200: Minibatch Loss: 0.004237 Error: 0.002508 Sparse: 0.017286\n",
      "| Loss :   0.002508 |\n",
      "| Sparse :   0.017286 |\n",
      "Step 300: Minibatch Loss: 0.003709 Error: 0.002270 Sparse: 0.014395\n",
      "| Loss :   0.002270 |\n",
      "| Sparse :   0.014395 |\n",
      "Step 400: Minibatch Loss: 0.003384 Error: 0.002131 Sparse: 0.012528\n",
      "| Loss :   0.002131 |\n",
      "| Sparse :   0.012528 |\n",
      "Step 500: Minibatch Loss: 0.003249 Error: 0.002118 Sparse: 0.011310\n",
      "| Loss :   0.002118 |\n",
      "| Sparse :   0.011310 |\n",
      "Step 600: Minibatch Loss: 0.003087 Error: 0.002058 Sparse: 0.010288\n",
      "| Loss :   0.002058 |\n",
      "| Sparse :   0.010288 |\n",
      "Step 700: Minibatch Loss: 0.002975 Error: 0.002022 Sparse: 0.009531\n",
      "| Loss :   0.002022 |\n",
      "| Sparse :   0.009531 |\n",
      "Step 800: Minibatch Loss: 0.002924 Error: 0.002034 Sparse: 0.008893\n",
      "| Loss :   0.002034 |\n",
      "| Sparse :   0.008893 |\n",
      "Step 900: Minibatch Loss: 0.002870 Error: 0.002037 Sparse: 0.008329\n",
      "| Loss :   0.002037 |\n",
      "| Sparse :   0.008329 |\n",
      "Step 1000: Minibatch Loss: 0.002782 Error: 0.001990 Sparse: 0.007920\n",
      "| Loss :   0.001990 |\n",
      "| Sparse :   0.007920 |\n",
      "Step 1100: Minibatch Loss: 0.002773 Error: 0.002020 Sparse: 0.007531\n",
      "| Loss :   0.002020 |\n",
      "| Sparse :   0.007531 |\n",
      "Step 1200: Minibatch Loss: 0.002718 Error: 0.001995 Sparse: 0.007230\n",
      "| Loss :   0.001995 |\n",
      "| Sparse :   0.007230 |\n",
      "Step 1300: Minibatch Loss: 0.002679 Error: 0.001984 Sparse: 0.006957\n",
      "| Loss :   0.001984 |\n",
      "| Sparse :   0.006957 |\n",
      "Step 1400: Minibatch Loss: 0.002685 Error: 0.002022 Sparse: 0.006631\n",
      "| Loss :   0.002022 |\n",
      "| Sparse :   0.006631 |\n",
      "Step 1500: Minibatch Loss: 0.002611 Error: 0.001974 Sparse: 0.006374\n",
      "| Loss :   0.001974 |\n",
      "| Sparse :   0.006374 |\n",
      "Step 1600: Minibatch Loss: 0.002599 Error: 0.001982 Sparse: 0.006168\n",
      "| Loss :   0.001982 |\n",
      "| Sparse :   0.006168 |\n",
      "Step 1700: Minibatch Loss: 0.002643 Error: 0.002050 Sparse: 0.005930\n",
      "| Loss :   0.002050 |\n",
      "| Sparse :   0.005930 |\n",
      "Step 1800: Minibatch Loss: 0.002577 Error: 0.001997 Sparse: 0.005807\n",
      "| Loss :   0.001997 |\n",
      "| Sparse :   0.005807 |\n",
      "Step 1900: Minibatch Loss: 0.002546 Error: 0.001980 Sparse: 0.005660\n",
      "| Loss :   0.001980 |\n",
      "| Sparse :   0.005660 |\n",
      "Step 2000: Minibatch Loss: 0.002527 Error: 0.001974 Sparse: 0.005528\n",
      "| Loss :   0.001974 |\n",
      "| Sparse :   0.005528 |\n",
      "This run of SparceLinAutoEncoderRELU8 ran for 0:00:39 and logs are available locally at: C:\\Users\\cjhamilton4176\\.hyperdash\\logs\\sparcelinautoencoderrelu8\\sparcelinautoencoderrelu8_2018-11-07t12-21-23-140835.log\n",
      "Step 1: Minibatch Loss: 0.015805 Error: 0.015419 Sparse: 0.000387\n",
      "| Loss :   0.015419 |\n",
      "| Sparse :   0.000387 |\n",
      "Step 100: Minibatch Loss: 0.015730 Error: 0.015289 Sparse: 0.000441\n",
      "| Loss :   0.015289 |\n",
      "| Sparse :   0.000441 |\n",
      "Step 200: Minibatch Loss: 0.013133 Error: 0.011440 Sparse: 0.001693\n",
      "| Loss :   0.011440 |\n",
      "| Sparse :   0.001693 |\n",
      "Step 300: Minibatch Loss: 0.011890 Error: 0.010179 Sparse: 0.001711\n",
      "| Loss :   0.010179 |\n",
      "| Sparse :   0.001711 |\n",
      "Step 400: Minibatch Loss: 0.010555 Error: 0.008498 Sparse: 0.002057\n",
      "| Loss :   0.008498 |\n",
      "| Sparse :   0.002057 |\n",
      "Step 500: Minibatch Loss: 0.009773 Error: 0.007679 Sparse: 0.002094\n",
      "| Loss :   0.007679 |\n",
      "| Sparse :   0.002094 |\n",
      "Step 600: Minibatch Loss: 0.008949 Error: 0.006655 Sparse: 0.002294\n",
      "| Loss :   0.006655 |\n",
      "| Sparse :   0.002294 |\n",
      "Step 700: Minibatch Loss: 0.008191 Error: 0.005941 Sparse: 0.002250\n",
      "| Loss :   0.005941 |\n",
      "| Sparse :   0.002250 |\n",
      "Step 800: Minibatch Loss: 0.007829 Error: 0.005552 Sparse: 0.002277\n",
      "| Loss :   0.005552 |\n",
      "| Sparse :   0.002277 |\n",
      "Step 900: Minibatch Loss: 0.007398 Error: 0.005221 Sparse: 0.002177\n",
      "| Loss :   0.005221 |\n",
      "| Sparse :   0.002177 |\n",
      "Step 1000: Minibatch Loss: 0.007014 Error: 0.004778 Sparse: 0.002236\n",
      "| Loss :   0.004778 |\n",
      "| Sparse :   0.002236 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1100: Minibatch Loss: 0.006623 Error: 0.004435 Sparse: 0.002188\n",
      "| Loss :   0.004435 |\n",
      "| Sparse :   0.002188 |\n",
      "Step 1200: Minibatch Loss: 0.006275 Error: 0.004053 Sparse: 0.002222\n",
      "| Loss :   0.004053 |\n",
      "| Sparse :   0.002222 |\n",
      "Step 1300: Minibatch Loss: 0.005897 Error: 0.003711 Sparse: 0.002186\n",
      "| Loss :   0.003711 |\n",
      "| Sparse :   0.002186 |\n",
      "Step 1400: Minibatch Loss: 0.005691 Error: 0.003526 Sparse: 0.002166\n",
      "| Loss :   0.003526 |\n",
      "| Sparse :   0.002166 |\n",
      "Step 1500: Minibatch Loss: 0.005534 Error: 0.003360 Sparse: 0.002175\n",
      "| Loss :   0.003360 |\n",
      "| Sparse :   0.002175 |\n",
      "Step 1600: Minibatch Loss: 0.005297 Error: 0.003156 Sparse: 0.002141\n",
      "| Loss :   0.003156 |\n",
      "| Sparse :   0.002141 |\n",
      "Step 1700: Minibatch Loss: 0.005095 Error: 0.002970 Sparse: 0.002125\n",
      "| Loss :   0.002970 |\n",
      "| Sparse :   0.002125 |\n",
      "Step 1800: Minibatch Loss: 0.004948 Error: 0.002809 Sparse: 0.002139\n",
      "| Loss :   0.002809 |\n",
      "| Sparse :   0.002139 |\n",
      "Step 1900: Minibatch Loss: 0.004779 Error: 0.002694 Sparse: 0.002085\n",
      "| Loss :   0.002694 |\n",
      "| Sparse :   0.002085 |\n",
      "Step 2000: Minibatch Loss: 0.004754 Error: 0.002684 Sparse: 0.002070\n",
      "| Loss :   0.002684 |\n",
      "| Sparse :   0.002070 |\n",
      "This run of SparceLinAutoEncoderRELU8 ran for 0:00:40 and logs are available locally at: C:\\Users\\cjhamilton4176\\.hyperdash\\logs\\sparcelinautoencoderrelu8\\sparcelinautoencoderrelu8_2018-11-07t12-22-03-369050.log\n",
      "Step 1: Minibatch Loss: 0.015428 Error: 0.015428 Sparse: 0.000399\n",
      "| Loss :   0.015428 |\n",
      "| Sparse :   0.000399 |\n",
      "Step 100: Minibatch Loss: 0.003314 Error: 0.003308 Sparse: 0.061773\n",
      "| Loss :   0.003308 |\n",
      "| Sparse :   0.061773 |\n",
      "Step 200: Minibatch Loss: 0.002238 Error: 0.002232 Sparse: 0.061253\n",
      "| Loss :   0.002232 |\n",
      "| Sparse :   0.061253 |\n",
      "Step 300: Minibatch Loss: 0.001977 Error: 0.001971 Sparse: 0.060434\n",
      "| Loss :   0.001971 |\n",
      "| Sparse :   0.060434 |\n"
     ]
    }
   ],
   "source": [
    "%%monitor_cell \"Metric Eval\"\n",
    "counter = 0\n",
    "tdata = data_loader('RawData/TrainingData8')\n",
    "with PdfPages('Results/ModelPlotsRELU8_2.pdf') as pdf:\n",
    "    for wscale in range(-3,-1,1):\n",
    "        wscale = 1*(10**wscale)\n",
    "        for learn_rate in range(-4,1,1):\n",
    "            learn_rate = 5*(10**learn_rate)\n",
    "            for nIterBat in range(1000,6000,1000):\n",
    "                for beta in range(-4,1,1):\n",
    "                    \n",
    "                    beta = 1*(10**beta)\n",
    "                    counter = counter + 1\n",
    "                    out = sparseLinAutoEncoder(wscale, learn_rate, nIterBat, beta, counter, tdata)\n",
    "                    fig = plt.figure(figsize=(15, 7))\n",
    "                    txt = \"Trained model : %d \\n _________________________\\n wscale : %f \\n learn_rate : %f \\n batch size : %d \\n beta : %f \\n loss : %f \\n msq : %f  \\n sparsity : %f \\n\" %(counter, wscale,learn_rate, nIterBat, beta, out[0], out[1], out[2])\n",
    "                    fig.suptitle(txt,x = 0.1, y= .65, fontsize=14, fontweight='bold', ha = 'left')\n",
    "                    \n",
    "                    fig.add_subplot(1,4,1).axis('off')\n",
    "\n",
    "                    fig.add_subplot(1, 4 , 2)\n",
    "                    plt.title('Original Images')\n",
    "                    plt.imshow(out[3], origin=\"upper\", cmap=\"gray\")\n",
    "                    fig.add_subplot(1,4,3)\n",
    "                    plt.title('Reconstructed Images')\n",
    "                    plt.imshow(out[4], origin=\"upper\", cmap=\"gray\")\n",
    "                    fig.add_subplot(1,4,4)\n",
    "                    plt.title('Wight Plots')\n",
    "                    plt.imshow(out[5], cmap=\"gray\")\n",
    "                    pdf.savefig(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "wscale          = .1;  # scale of the small random weights for model initialization\n",
    "beta            = .05;  # hyper-parameter which determines penality size\n",
    "learn_rate      = .05;\n",
    "nIterBat      = 1000;\n",
    "nBat            = 5000;\n",
    "num_steps       = 5;\n",
    "\n",
    "with PdfPages('ModelPlots16.pdf') as pdf:\n",
    "    \n",
    "\n",
    "    fig = plt.figure(figsize=(15, 7))\n",
    "    #fig.subplots_adjust(top=2.5)\n",
    "    txt = \"Trained model : %d \\n _________________________\\n wscale : %f \\n learn_rate : %f \\n batch size : %d \\n beta : %f \\n loss : %f \\n msq : %f  \\n sparsity : %f \\n\" %(1, wscale,learn_rate, nIterBat, beta, out[0], out[1], out[2])\n",
    "    fig.suptitle(txt,x = 0.1, y= .65, fontsize=14, fontweight='bold', ha = 'left')\n",
    "    #pdf.savefig(fig)\n",
    "    fig.add_subplot(1,4,1).axis('off')\n",
    "    \n",
    "    fig.add_subplot(1, 4 , 2)\n",
    "    plt.title('Original Images')\n",
    "    plt.imshow(out[3], origin=\"upper\", cmap=\"gray\")\n",
    "    fig.add_subplot(1,4,3)\n",
    "    plt.title('Reconstructed Images')\n",
    "    plt.imshow(out[4], origin=\"upper\", cmap=\"gray\")\n",
    "    fig.add_subplot(1,4,4)\n",
    "    plt.title('Wight Plots')\n",
    "    plt.imshow(out[5], cmap=\"gray\")\n",
    "    \n",
    "    \n",
    "    #fig.text(0.05,1.35,txt, transform=fig.transFigure, size=20)\n",
    "#     txt = \"Trained model : %d \\n wscale : %f \\n learn_rate : %f \\n batch size : %d \\n beta : %f \\n loss : %f \\n msq : %f  \\n sparsity : %f \\n\" %(1, wscale,learn_rate, nIterBat, beta, out[0], out[1], out[2])\n",
    "#     ax.figtext(0.05,1.1,txt, transform=fig.transFigure, size=20)\n",
    "\n",
    "    #plt.close()\n",
    "    pdf.savefig(fig)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
