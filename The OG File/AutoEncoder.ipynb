{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingData\\TrainSetPatch_1.mat\n",
      "TrainingData\\TrainSetPatch_10.mat\n",
      "TrainingData\\TrainSetPatch_11.mat\n",
      "TrainingData\\TrainSetPatch_12.mat\n",
      "TrainingData\\TrainSetPatch_13.mat\n",
      "TrainingData\\TrainSetPatch_14.mat\n",
      "TrainingData\\TrainSetPatch_15.mat\n",
      "TrainingData\\TrainSetPatch_16.mat\n",
      "TrainingData\\TrainSetPatch_17.mat\n",
      "TrainingData\\TrainSetPatch_18.mat\n",
      "TrainingData\\TrainSetPatch_19.mat\n",
      "TrainingData\\TrainSetPatch_2.mat\n",
      "TrainingData\\TrainSetPatch_20.mat\n",
      "TrainingData\\TrainSetPatch_3.mat\n",
      "TrainingData\\TrainSetPatch_4.mat\n",
      "TrainingData\\TrainSetPatch_5.mat\n",
      "TrainingData\\TrainSetPatch_6.mat\n",
      "TrainingData\\TrainSetPatch_7.mat\n",
      "TrainingData\\TrainSetPatch_8.mat\n",
      "TrainingData\\TrainSetPatch_9.mat\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Getting the current work directory (cwd)\n",
    "thisdir = os.getcwd()\n",
    "TrainingData= []\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk('TrainingData'):\n",
    "    for file in f:\n",
    "        if \".mat\" in file:\n",
    "            print(os.path.join(r, file))\n",
    "            TrainingData.append(os.path.join(r, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 80000)\n",
      "80000\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(TrainingData)):\n",
    "    td = loadmat(TrainingData[i])['X']\n",
    "    if (i==0):\n",
    "        tdata = td\n",
    "    else :\n",
    "        tdata = np.concatenate((tdata, td), axis=1)\n",
    "    \n",
    "print(tdata.shape)\n",
    "tdata = tdata.transpose()\n",
    "print(len(tdata.transpose()[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deciding how many nodes wach layer should have\n",
    "\n",
    "n_nodes_inpl = 64  #encoder\n",
    "n_nodes_hl1  = 32  #encoder\n",
    "\n",
    "n_nodes_hl2  = 32  #decoder\n",
    "n_nodes_outl = 64  #decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weights': <tf.Variable 'Variable:0' shape=(64, 32) dtype=float32_ref>, 'biases': <tf.Variable 'Variable_1:0' shape=(32,) dtype=float32_ref>}\n"
     ]
    }
   ],
   "source": [
    "# first hidden layer has 784*32 weights and 32 biases\n",
    "\n",
    "hidden_1_layer_vals = {\n",
    "'weights':tf.Variable(tf.random_normal([n_nodes_inpl,n_nodes_hl1])),\n",
    "'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))  }\n",
    "print(hidden_1_layer_vals)\n",
    "\n",
    "# second hidden layer has 32*32 weights and 32 biases\n",
    "\n",
    "hidden_2_layer_vals = {\n",
    "'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))  }\n",
    "\n",
    "# second hidden layer has 32*784 weights and 784 biases\n",
    "\n",
    "output_layer_vals = {\n",
    "'weights':tf.Variable(tf.random_normal([n_nodes_hl2,n_nodes_outl])),\n",
    "'biases':tf.Variable(tf.random_normal([n_nodes_outl])) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image with shape 784 goes in\n",
    "input_layer = tf.placeholder('float', [None, 64])\n",
    "\n",
    "# multiply output of input_layer wth a weight matrix and add biases\n",
    "\n",
    "layer_1 = tf.contrib.layers.fully_connected(tf.matmul(input_layer,hidden_1_layer_vals['weights']),\n",
    "                                            32,\n",
    "                                            activation_fn=tf.nn.relu)\n",
    "\n",
    "# multiply output of layer_1 wth a weight matrix and add biases\n",
    "\n",
    "layer_2 = tf.contrib.layers.fully_connected(\n",
    "       tf.add(tf.matmul(layer_1,hidden_2_layer_vals['weights']),\n",
    "       hidden_2_layer_vals['biases']),32, activation_fn=tf.nn.relu)\n",
    "\n",
    "# multiply output of layer_2 wth a weight matrix and add biases\n",
    "\n",
    "output_layer = tf.matmul(layer_1,output_layer_vals['weights']) \n",
    "\n",
    "# output_true shall have the original image for error calculations\n",
    "\n",
    "output_true = tf.placeholder('float', [None, 64])\n",
    "\n",
    "# define our cost function\n",
    "meansq =    tf.reduce_mean(tf.square(output_layer - output_true))\n",
    "\n",
    "# define our optimizer\n",
    "learn_rate = 0.1   # how fast the model should learn\n",
    "optimizer = tf.train.AdagradOptimizer(learn_rate).minimize(meansq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 1000 loss: 61.700785647146404\n",
      "Epoch 1 / 1000 loss: 24.69725851994008\n",
      "Epoch 2 / 1000 loss: 24.672554188407958\n",
      "Epoch 3 / 1000 loss: 24.66496326494962\n",
      "Epoch 4 / 1000 loss: 24.661571566015482\n",
      "Epoch 5 / 1000 loss: 24.659794171340764\n",
      "Epoch 6 / 1000 loss: 24.658714020624757\n",
      "Epoch 7 / 1000 loss: 24.6580301951617\n",
      "Epoch 8 / 1000 loss: 24.657547054812312\n",
      "Epoch 9 / 1000 loss: 24.657211874611676\n",
      "Epoch 10 / 1000 loss: 24.656986837275326\n",
      "Epoch 11 / 1000 loss: 24.656809746287763\n",
      "Epoch 12 / 1000 loss: 24.65669601224363\n",
      "Epoch 13 / 1000 loss: 24.656585623510182\n",
      "Epoch 14 / 1000 loss: 24.65650273859501\n",
      "Epoch 15 / 1000 loss: 24.65642774477601\n",
      "Epoch 16 / 1000 loss: 24.65637374855578\n",
      "Epoch 17 / 1000 loss: 24.656322410330176\n",
      "Epoch 18 / 1000 loss: 24.656277955509722\n",
      "Epoch 19 / 1000 loss: 24.656234229914844\n",
      "Epoch 20 / 1000 loss: 24.656203873455524\n",
      "Epoch 21 / 1000 loss: 24.656176093034446\n",
      "Epoch 22 / 1000 loss: 24.65615008957684\n",
      "Epoch 23 / 1000 loss: 24.656124295666814\n",
      "Epoch 24 / 1000 loss: 24.65610346570611\n",
      "Epoch 25 / 1000 loss: 24.656087575480342\n",
      "Epoch 26 / 1000 loss: 24.65607372764498\n",
      "Epoch 27 / 1000 loss: 24.656060567125678\n",
      "Epoch 28 / 1000 loss: 24.656040942296386\n",
      "Epoch 29 / 1000 loss: 24.65602534171194\n",
      "Epoch 30 / 1000 loss: 24.65601098537445\n",
      "Epoch 31 / 1000 loss: 24.655999395996332\n",
      "Epoch 32 / 1000 loss: 24.65598767902702\n",
      "Epoch 33 / 1000 loss: 24.65598073042929\n",
      "Epoch 34 / 1000 loss: 24.655966587364674\n",
      "Epoch 35 / 1000 loss: 24.65595682710409\n",
      "Epoch 36 / 1000 loss: 24.655946801416576\n",
      "Epoch 37 / 1000 loss: 24.655933280475438\n",
      "Epoch 38 / 1000 loss: 24.655918644741178\n",
      "Epoch 39 / 1000 loss: 24.655904496088624\n",
      "Epoch 40 / 1000 loss: 24.655895891599357\n",
      "Epoch 41 / 1000 loss: 24.655888711102307\n",
      "Epoch 42 / 1000 loss: 24.6558809382841\n",
      "Epoch 43 / 1000 loss: 24.655871076509356\n",
      "Epoch 44 / 1000 loss: 24.65585872810334\n",
      "Epoch 45 / 1000 loss: 24.655848243273795\n",
      "Epoch 46 / 1000 loss: 24.655839431099594\n",
      "Epoch 47 / 1000 loss: 24.655832447111607\n",
      "Epoch 48 / 1000 loss: 24.655822791159153\n",
      "Epoch 49 / 1000 loss: 24.655813936144114\n",
      "Epoch 50 / 1000 loss: 24.655806951224804\n",
      "Epoch 51 / 1000 loss: 24.655800168402493\n",
      "Epoch 52 / 1000 loss: 24.655793762765825\n",
      "Epoch 53 / 1000 loss: 24.655788217671216\n",
      "Epoch 54 / 1000 loss: 24.65577550791204\n",
      "Epoch 55 / 1000 loss: 24.655733360908926\n",
      "Epoch 56 / 1000 loss: 24.65570093691349\n",
      "Epoch 57 / 1000 loss: 24.655679190531373\n",
      "Epoch 58 / 1000 loss: 24.655663278885186\n",
      "Epoch 59 / 1000 loss: 24.655650915578008\n",
      "Epoch 60 / 1000 loss: 24.655641939491034\n",
      "Epoch 61 / 1000 loss: 24.65562936477363\n",
      "Epoch 62 / 1000 loss: 24.65562063921243\n",
      "Epoch 63 / 1000 loss: 24.65561440680176\n",
      "Epoch 64 / 1000 loss: 24.65560630429536\n",
      "Epoch 65 / 1000 loss: 24.655600254423916\n",
      "Epoch 66 / 1000 loss: 24.65559245739132\n",
      "Epoch 67 / 1000 loss: 24.65558315627277\n",
      "Epoch 68 / 1000 loss: 24.65557370148599\n",
      "Epoch 69 / 1000 loss: 24.655570642091334\n",
      "Epoch 70 / 1000 loss: 24.655560235492885\n",
      "Epoch 71 / 1000 loss: 24.65555541217327\n",
      "Epoch 72 / 1000 loss: 24.65554944705218\n",
      "Epoch 73 / 1000 loss: 24.655545161105692\n",
      "Epoch 74 / 1000 loss: 24.655542742460966\n",
      "Epoch 75 / 1000 loss: 24.655538024380803\n",
      "Epoch 76 / 1000 loss: 24.65553618501872\n",
      "Epoch 77 / 1000 loss: 24.655532219447196\n",
      "Epoch 78 / 1000 loss: 24.655530120246112\n",
      "Epoch 79 / 1000 loss: 24.655527405440807\n",
      "Epoch 80 / 1000 loss: 24.65552416536957\n",
      "Epoch 81 / 1000 loss: 24.655511897988617\n",
      "Epoch 82 / 1000 loss: 24.655489477328956\n",
      "Epoch 83 / 1000 loss: 24.65546545665711\n",
      "Epoch 84 / 1000 loss: 24.6554500721395\n",
      "Epoch 85 / 1000 loss: 24.655431383289397\n",
      "Epoch 86 / 1000 loss: 24.655413469299674\n",
      "Epoch 87 / 1000 loss: 24.655397434718907\n",
      "Epoch 88 / 1000 loss: 24.655372198671103\n",
      "Epoch 89 / 1000 loss: 24.655361683107913\n",
      "Epoch 90 / 1000 loss: 24.655340342782438\n",
      "Epoch 91 / 1000 loss: 24.65532031096518\n",
      "Epoch 92 / 1000 loss: 24.655307260341942\n",
      "Epoch 93 / 1000 loss: 24.65529582463205\n",
      "Epoch 94 / 1000 loss: 24.655280474573374\n",
      "Epoch 95 / 1000 loss: 24.65526925586164\n",
      "Epoch 96 / 1000 loss: 24.65525955054909\n",
      "Epoch 97 / 1000 loss: 24.655253489501774\n",
      "Epoch 98 / 1000 loss: 24.655248227529228\n",
      "Epoch 99 / 1000 loss: 24.655239891260862\n",
      "Epoch 100 / 1000 loss: 24.655234212055802\n",
      "Epoch 101 / 1000 loss: 24.655228103511035\n",
      "Epoch 102 / 1000 loss: 24.65522407181561\n",
      "Epoch 103 / 1000 loss: 24.655219236388803\n",
      "Epoch 104 / 1000 loss: 24.6552161462605\n",
      "Epoch 105 / 1000 loss: 24.655215264298022\n",
      "Epoch 106 / 1000 loss: 24.655191977508366\n",
      "Epoch 107 / 1000 loss: 24.655175203457475\n",
      "Epoch 108 / 1000 loss: 24.655163483694196\n",
      "Epoch 109 / 1000 loss: 24.65515071246773\n",
      "Epoch 110 / 1000 loss: 24.65514266770333\n",
      "Epoch 111 / 1000 loss: 24.655136334709823\n",
      "Epoch 112 / 1000 loss: 24.655122789554298\n",
      "Epoch 113 / 1000 loss: 24.655108647421002\n",
      "Epoch 114 / 1000 loss: 24.65509439818561\n",
      "Epoch 115 / 1000 loss: 24.655073926784098\n",
      "Epoch 116 / 1000 loss: 24.65506135672331\n",
      "Epoch 117 / 1000 loss: 24.655053222551942\n",
      "Epoch 118 / 1000 loss: 24.655046112835407\n",
      "Epoch 119 / 1000 loss: 24.655040742829442\n",
      "Epoch 120 / 1000 loss: 24.65503676328808\n",
      "Epoch 121 / 1000 loss: 24.655032402835786\n",
      "Epoch 122 / 1000 loss: 24.65502945613116\n",
      "Epoch 123 / 1000 loss: 24.655026112683117\n",
      "Epoch 124 / 1000 loss: 24.65502384956926\n",
      "Epoch 125 / 1000 loss: 24.655021142214537\n",
      "Epoch 126 / 1000 loss: 24.655019351281226\n",
      "Epoch 127 / 1000 loss: 24.655017122626305\n",
      "Epoch 128 / 1000 loss: 24.655015671625733\n",
      "Epoch 129 / 1000 loss: 24.655013780109584\n",
      "Epoch 130 / 1000 loss: 24.65501258149743\n",
      "Epoch 131 / 1000 loss: 24.655010897666216\n",
      "Epoch 132 / 1000 loss: 24.65500987600535\n",
      "Epoch 133 / 1000 loss: 24.65500837098807\n",
      "Epoch 134 / 1000 loss: 24.65500748436898\n",
      "Epoch 135 / 1000 loss: 24.655006133019924\n",
      "Epoch 136 / 1000 loss: 24.65500540100038\n",
      "Epoch 137 / 1000 loss: 24.65500416327268\n",
      "Epoch 138 / 1000 loss: 24.655003459192812\n",
      "Epoch 139 / 1000 loss: 24.655002363957465\n",
      "Epoch 140 / 1000 loss: 24.65500165335834\n",
      "Epoch 141 / 1000 loss: 24.655000512488186\n",
      "Epoch 142 / 1000 loss: 24.655001050792634\n",
      "Epoch 143 / 1000 loss: 24.654998218640685\n",
      "Epoch 144 / 1000 loss: 24.654996525496244\n",
      "Epoch 145 / 1000 loss: 24.654992657713592\n",
      "Epoch 146 / 1000 loss: 24.654987162910402\n",
      "Epoch 147 / 1000 loss: 24.654984012246132\n",
      "Epoch 148 / 1000 loss: 24.654982190579176\n",
      "Epoch 149 / 1000 loss: 24.65497797075659\n",
      "Epoch 150 / 1000 loss: 24.654966709204018\n",
      "Epoch 151 / 1000 loss: 24.65496176853776\n",
      "Epoch 152 / 1000 loss: 24.65495739877224\n",
      "Epoch 153 / 1000 loss: 24.654953675344586\n",
      "Epoch 154 / 1000 loss: 24.654963806271553\n",
      "Epoch 155 / 1000 loss: 24.654942760244012\n",
      "Epoch 156 / 1000 loss: 24.654920307919383\n",
      "Epoch 157 / 1000 loss: 24.654908536933362\n",
      "Epoch 158 / 1000 loss: 24.65490042604506\n",
      "Epoch 159 / 1000 loss: 24.65489507932216\n",
      "Epoch 160 / 1000 loss: 24.654890338890254\n",
      "Epoch 161 / 1000 loss: 24.654884410090744\n",
      "Epoch 162 / 1000 loss: 24.654877113178372\n",
      "Epoch 163 / 1000 loss: 24.65487033315003\n",
      "Epoch 164 / 1000 loss: 24.654859741218388\n",
      "Epoch 165 / 1000 loss: 24.6548460861668\n",
      "Epoch 166 / 1000 loss: 24.654837356880307\n",
      "Epoch 167 / 1000 loss: 24.654830653220415\n",
      "Epoch 168 / 1000 loss: 24.654826485551894\n",
      "Epoch 169 / 1000 loss: 24.654816472902894\n",
      "Epoch 170 / 1000 loss: 24.654809632338583\n",
      "Epoch 171 / 1000 loss: 24.65480433870107\n",
      "Epoch 172 / 1000 loss: 24.654800159856677\n",
      "Epoch 173 / 1000 loss: 24.654796783812344\n",
      "Epoch 174 / 1000 loss: 24.654793771915138\n",
      "Epoch 175 / 1000 loss: 24.65479119308293\n",
      "Epoch 176 / 1000 loss: 24.65478889644146\n",
      "Epoch 177 / 1000 loss: 24.65478692110628\n",
      "Epoch 178 / 1000 loss: 24.65478508733213\n",
      "Epoch 179 / 1000 loss: 24.654782166704535\n",
      "Epoch 180 / 1000 loss: 24.65477734990418\n",
      "Epoch 181 / 1000 loss: 24.654774256050587\n",
      "Epoch 182 / 1000 loss: 24.654771921224892\n",
      "Epoch 183 / 1000 loss: 24.65476997103542\n",
      "Epoch 184 / 1000 loss: 24.65476825553924\n",
      "Epoch 185 / 1000 loss: 24.654766680672765\n",
      "Epoch 186 / 1000 loss: 24.65476526506245\n",
      "Epoch 187 / 1000 loss: 24.654764053411782\n",
      "Epoch 188 / 1000 loss: 24.65476260986179\n",
      "Epoch 189 / 1000 loss: 24.65476146247238\n",
      "Epoch 190 / 1000 loss: 24.65476043242961\n",
      "Epoch 191 / 1000 loss: 24.654759161174297\n",
      "Epoch 192 / 1000 loss: 24.654758105054498\n",
      "Epoch 193 / 1000 loss: 24.65475721657276\n",
      "Epoch 194 / 1000 loss: 24.65475609805435\n",
      "Epoch 195 / 1000 loss: 24.654755285941064\n",
      "Epoch 196 / 1000 loss: 24.654755609109998\n",
      "Epoch 197 / 1000 loss: 24.654753838665783\n",
      "Epoch 198 / 1000 loss: 24.654752565547824\n",
      "Epoch 199 / 1000 loss: 24.65475173573941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200 / 1000 loss: 24.65475073736161\n",
      "Epoch 201 / 1000 loss: 24.654750087298453\n",
      "Epoch 202 / 1000 loss: 24.654749092645943\n",
      "Epoch 203 / 1000 loss: 24.65474841091782\n",
      "Epoch 204 / 1000 loss: 24.65474750287831\n",
      "Epoch 205 / 1000 loss: 24.65474688448012\n",
      "Epoch 206 / 1000 loss: 24.65474610310048\n",
      "Epoch 207 / 1000 loss: 24.654745498672128\n",
      "Epoch 208 / 1000 loss: 24.65474470704794\n",
      "Epoch 209 / 1000 loss: 24.654743854887784\n",
      "Epoch 210 / 1000 loss: 24.654743026010692\n",
      "Epoch 211 / 1000 loss: 24.6547425147146\n",
      "Epoch 212 / 1000 loss: 24.654741696082056\n",
      "Epoch 213 / 1000 loss: 24.654740885831416\n",
      "Epoch 214 / 1000 loss: 24.654740364290774\n",
      "Epoch 215 / 1000 loss: 24.654739636927843\n",
      "Epoch 216 / 1000 loss: 24.65473893005401\n",
      "Epoch 217 / 1000 loss: 24.65473836287856\n",
      "Epoch 218 / 1000 loss: 24.654737622477114\n",
      "Epoch 219 / 1000 loss: 24.6547369658947\n",
      "Epoch 220 / 1000 loss: 24.654736298136413\n",
      "Epoch 221 / 1000 loss: 24.654735763557255\n",
      "Epoch 222 / 1000 loss: 24.65473503433168\n",
      "Epoch 223 / 1000 loss: 24.65473440475762\n",
      "Epoch 224 / 1000 loss: 24.65473375376314\n",
      "Epoch 225 / 1000 loss: 24.65473310276866\n",
      "Epoch 226 / 1000 loss: 24.654732664115727\n",
      "Epoch 227 / 1000 loss: 24.65473193861544\n",
      "Epoch 228 / 1000 loss: 24.654731306247413\n",
      "Epoch 229 / 1000 loss: 24.654730721376836\n",
      "Epoch 230 / 1000 loss: 24.6547300927341\n",
      "Epoch 231 / 1000 loss: 24.654729610309005\n",
      "Epoch 232 / 1000 loss: 24.654728974215686\n",
      "Epoch 233 / 1000 loss: 24.6547283353284\n",
      "Epoch 234 / 1000 loss: 24.654727702960372\n",
      "Epoch 235 / 1000 loss: 24.65472721774131\n",
      "Epoch 236 / 1000 loss: 24.654726647771895\n",
      "Epoch 237 / 1000 loss: 24.654725945554674\n",
      "Epoch 238 / 1000 loss: 24.654725490137935\n",
      "Epoch 239 / 1000 loss: 24.654724715277553\n",
      "Epoch 240 / 1000 loss: 24.654724383726716\n",
      "Epoch 241 / 1000 loss: 24.65472391434014\n",
      "Epoch 242 / 1000 loss: 24.65472329314798\n",
      "Epoch 243 / 1000 loss: 24.654722687788308\n",
      "Epoch 244 / 1000 loss: 24.654722169041634\n",
      "Epoch 245 / 1000 loss: 24.654721662402153\n",
      "Epoch 246 / 1000 loss: 24.65472107473761\n",
      "Epoch 247 / 1000 loss: 24.654720776714385\n",
      "Epoch 248 / 1000 loss: 24.654720179736614\n",
      "Epoch 249 / 1000 loss: 24.65471945796162\n",
      "Epoch 250 / 1000 loss: 24.654718974605203\n",
      "Epoch 251 / 1000 loss: 24.654718331992626\n",
      "Epoch 252 / 1000 loss: 24.654717734083533\n",
      "Epoch 253 / 1000 loss: 24.654716128483415\n",
      "Epoch 254 / 1000 loss: 24.65471544303\n",
      "Epoch 255 / 1000 loss: 24.65471496619284\n",
      "Epoch 256 / 1000 loss: 24.6547144446522\n",
      "Epoch 257 / 1000 loss: 24.65471393428743\n",
      "Epoch 258 / 1000 loss: 24.654713433235884\n",
      "Epoch 259 / 1000 loss: 24.65471297968179\n",
      "Epoch 260 / 1000 loss: 24.654712598770857\n",
      "Epoch 261 / 1000 loss: 24.65471212659031\n",
      "Epoch 262 / 1000 loss: 24.65471147187054\n",
      "Epoch 263 / 1000 loss: 24.654710937291384\n",
      "Epoch 264 / 1000 loss: 24.65471031423658\n",
      "Epoch 265 / 1000 loss: 24.654711155220866\n",
      "Epoch 266 / 1000 loss: 24.654709924943745\n",
      "Epoch 267 / 1000 loss: 24.65470900479704\n",
      "Epoch 268 / 1000 loss: 24.654708269052207\n",
      "Epoch 269 / 1000 loss: 24.654707754962146\n",
      "Epoch 270 / 1000 loss: 24.65470721758902\n",
      "Epoch 271 / 1000 loss: 24.65470674354583\n",
      "Epoch 272 / 1000 loss: 24.654706188477576\n",
      "Epoch 273 / 1000 loss: 24.654705646447837\n",
      "Epoch 274 / 1000 loss: 24.654705106280744\n",
      "Epoch 275 / 1000 loss: 24.65470470301807\n",
      "Epoch 276 / 1000 loss: 24.654704119078815\n",
      "Epoch 277 / 1000 loss: 24.654703558422625\n",
      "Epoch 278 / 1000 loss: 24.654703103005886\n",
      "Epoch 279 / 1000 loss: 24.65470267739147\n",
      "Epoch 280 / 1000 loss: 24.654702434316278\n",
      "Epoch 281 / 1000 loss: 24.654699810780585\n",
      "Epoch 282 / 1000 loss: 24.654697850346565\n",
      "Epoch 283 / 1000 loss: 24.654696178622544\n",
      "Epoch 284 / 1000 loss: 24.65469474159181\n",
      "Epoch 285 / 1000 loss: 24.654693647287786\n",
      "Epoch 286 / 1000 loss: 24.654692728072405\n",
      "Epoch 287 / 1000 loss: 24.654692033305764\n",
      "Epoch 288 / 1000 loss: 24.654691180214286\n",
      "Epoch 289 / 1000 loss: 24.654690387658775\n",
      "Epoch 290 / 1000 loss: 24.654689671471715\n",
      "Epoch 291 / 1000 loss: 24.654689026996493\n",
      "Epoch 292 / 1000 loss: 24.654689547605813\n",
      "Epoch 293 / 1000 loss: 24.65468397643417\n",
      "Epoch 294 / 1000 loss: 24.654666698537767\n",
      "Epoch 295 / 1000 loss: 24.654653823934495\n",
      "Epoch 296 / 1000 loss: 24.654643688350916\n",
      "Epoch 297 / 1000 loss: 24.65463464334607\n",
      "Epoch 298 / 1000 loss: 24.654628125950694\n",
      "Epoch 299 / 1000 loss: 24.654614715836942\n",
      "Epoch 300 / 1000 loss: 24.654607605189085\n",
      "Epoch 301 / 1000 loss: 24.65460271574557\n",
      "Epoch 302 / 1000 loss: 24.654598883353174\n",
      "Epoch 303 / 1000 loss: 24.654595898464322\n",
      "Epoch 304 / 1000 loss: 24.654593500308692\n",
      "Epoch 305 / 1000 loss: 24.65459144115448\n",
      "Epoch 306 / 1000 loss: 24.654589627869427\n",
      "Epoch 307 / 1000 loss: 24.654587985947728\n",
      "Epoch 308 / 1000 loss: 24.65458650700748\n",
      "Epoch 309 / 1000 loss: 24.654585149139166\n",
      "Epoch 310 / 1000 loss: 24.654583803378046\n",
      "Epoch 311 / 1000 loss: 24.654582497663796\n",
      "Epoch 312 / 1000 loss: 24.65457881614566\n",
      "Epoch 313 / 1000 loss: 24.654576363041997\n",
      "Epoch 314 / 1000 loss: 24.65457445383072\n",
      "Epoch 315 / 1000 loss: 24.654572868719697\n",
      "Epoch 316 / 1000 loss: 24.6545714950189\n",
      "Epoch 317 / 1000 loss: 24.654570159502327\n",
      "Epoch 318 / 1000 loss: 24.6545689124614\n",
      "Epoch 319 / 1000 loss: 24.654567761346698\n",
      "Epoch 320 / 1000 loss: 24.654566457495093\n",
      "Epoch 321 / 1000 loss: 24.65456366725266\n",
      "Epoch 322 / 1000 loss: 24.65456171799451\n",
      "Epoch 323 / 1000 loss: 24.654560280032456\n",
      "Epoch 324 / 1000 loss: 24.654559020884335\n",
      "Epoch 325 / 1000 loss: 24.654557758010924\n",
      "Epoch 326 / 1000 loss: 24.65455665346235\n",
      "Epoch 327 / 1000 loss: 24.654555591754615\n",
      "Epoch 328 / 1000 loss: 24.65455464925617\n",
      "Epoch 329 / 1000 loss: 24.654553803615272\n",
      "Epoch 330 / 1000 loss: 24.65455290209502\n",
      "Epoch 331 / 1000 loss: 24.654551897197962\n",
      "Epoch 332 / 1000 loss: 24.65455108974129\n",
      "Epoch 333 / 1000 loss: 24.654550233855844\n",
      "Epoch 334 / 1000 loss: 24.65454944036901\n",
      "Epoch 335 / 1000 loss: 24.654548707418144\n",
      "Epoch 336 / 1000 loss: 24.654547920450568\n",
      "Epoch 337 / 1000 loss: 24.654547120444477\n",
      "Epoch 338 / 1000 loss: 24.654546345584095\n",
      "Epoch 339 / 1000 loss: 24.654545615427196\n",
      "Epoch 340 / 1000 loss: 24.654544979333878\n",
      "Epoch 341 / 1000 loss: 24.654544272460043\n",
      "Epoch 342 / 1000 loss: 24.654544971883297\n",
      "Epoch 343 / 1000 loss: 24.65454345755279\n",
      "Epoch 344 / 1000 loss: 24.65454233530909\n",
      "Epoch 345 / 1000 loss: 24.654541636817157\n",
      "Epoch 346 / 1000 loss: 24.654540929943323\n",
      "Epoch 347 / 1000 loss: 24.654540200717747\n",
      "Epoch 348 / 1000 loss: 24.6545395758003\n",
      "Epoch 349 / 1000 loss: 24.654538937844336\n",
      "Epoch 350 / 1000 loss: 24.65453833527863\n",
      "Epoch 351 / 1000 loss: 24.654537718743086\n",
      "Epoch 352 / 1000 loss: 24.65453706495464\n",
      "Epoch 353 / 1000 loss: 24.65453642141074\n",
      "Epoch 354 / 1000 loss: 24.65453585051\n",
      "Epoch 355 / 1000 loss: 24.654535315930843\n",
      "Epoch 356 / 1000 loss: 24.65453471802175\n",
      "Epoch 357 / 1000 loss: 24.654534056782722\n",
      "Epoch 358 / 1000 loss: 24.654533496126533\n",
      "Epoch 359 / 1000 loss: 24.65453291963786\n",
      "Epoch 360 / 1000 loss: 24.654532429762185\n",
      "Epoch 361 / 1000 loss: 24.654531847685575\n",
      "Epoch 362 / 1000 loss: 24.654531249776483\n",
      "Epoch 363 / 1000 loss: 24.65453068818897\n",
      "Epoch 364 / 1000 loss: 24.65453011635691\n",
      "Epoch 365 / 1000 loss: 24.654529634863138\n",
      "Epoch 366 / 1000 loss: 24.654529079794884\n",
      "Epoch 367 / 1000 loss: 24.65452852845192\n",
      "Epoch 368 / 1000 loss: 24.654528028331697\n",
      "Epoch 369 / 1000 loss: 24.654527455568314\n",
      "Epoch 370 / 1000 loss: 24.654526997357607\n",
      "Epoch 371 / 1000 loss: 24.654526442289352\n",
      "Epoch 372 / 1000 loss: 24.65452585183084\n",
      "Epoch 373 / 1000 loss: 24.654525328427553\n",
      "Epoch 374 / 1000 loss: 24.654524858109653\n",
      "Epoch 375 / 1000 loss: 24.654524357058108\n",
      "Epoch 376 / 1000 loss: 24.65452381130308\n",
      "Epoch 377 / 1000 loss: 24.65452326927334\n",
      "Epoch 378 / 1000 loss: 24.654522822238505\n",
      "Epoch 379 / 1000 loss: 24.654522399418056\n",
      "Epoch 380 / 1000 loss: 24.654521819204092\n",
      "Epoch 381 / 1000 loss: 24.654521256685257\n",
      "Epoch 382 / 1000 loss: 24.654520753771067\n",
      "Epoch 383 / 1000 loss: 24.65452042594552\n",
      "Epoch 384 / 1000 loss: 24.654519898816943\n",
      "Epoch 385 / 1000 loss: 24.65451932977885\n",
      "Epoch 386 / 1000 loss: 24.65451883058995\n",
      "Epoch 387 / 1000 loss: 24.654518405906856\n",
      "Epoch 388 / 1000 loss: 24.654517989605665\n",
      "Epoch 389 / 1000 loss: 24.654517485760152\n",
      "Epoch 390 / 1000 loss: 24.654516914859414\n",
      "Epoch 391 / 1000 loss: 24.65451650880277\n",
      "Epoch 392 / 1000 loss: 24.654516031965613\n",
      "Epoch 393 / 1000 loss: 24.65451547689736\n",
      "Epoch 394 / 1000 loss: 24.654515009373426\n",
      "Epoch 395 / 1000 loss: 24.654514618217945\n",
      "Epoch 396 / 1000 loss: 24.65451418608427\n",
      "Epoch 397 / 1000 loss: 24.654513608664274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 398 / 1000 loss: 24.65451316908002\n",
      "Epoch 399 / 1000 loss: 24.654512784443796\n",
      "Epoch 400 / 1000 loss: 24.654512311331928\n",
      "Epoch 401 / 1000 loss: 24.654511763714254\n",
      "Epoch 402 / 1000 loss: 24.65451129153371\n",
      "Epoch 403 / 1000 loss: 24.65451095160097\n",
      "Epoch 404 / 1000 loss: 24.654510468244553\n",
      "Epoch 405 / 1000 loss: 24.65450998209417\n",
      "Epoch 406 / 1000 loss: 24.654509593732655\n",
      "Epoch 407 / 1000 loss: 24.654509216547012\n",
      "Epoch 408 / 1000 loss: 24.65450866892934\n",
      "Epoch 409 / 1000 loss: 24.654508166015148\n",
      "Epoch 410 / 1000 loss: 24.65450779441744\n",
      "Epoch 411 / 1000 loss: 24.654507357627153\n",
      "Epoch 412 / 1000 loss: 24.65450695808977\n",
      "Epoch 413 / 1000 loss: 24.65450639370829\n",
      "Epoch 414 / 1000 loss: 24.654505958780646\n",
      "Epoch 415 / 1000 loss: 24.654505622573197\n",
      "Epoch 416 / 1000 loss: 24.65450516436249\n",
      "Epoch 417 / 1000 loss: 24.6545046325773\n",
      "Epoch 418 / 1000 loss: 24.654504249803722\n",
      "Epoch 419 / 1000 loss: 24.654503886587918\n",
      "Epoch 420 / 1000 loss: 24.654503386467695\n",
      "Epoch 421 / 1000 loss: 24.654502949677408\n",
      "Epoch 422 / 1000 loss: 24.65450256038457\n",
      "Epoch 423 / 1000 loss: 24.654502721503377\n",
      "Epoch 424 / 1000 loss: 24.654497551731765\n",
      "Epoch 425 / 1000 loss: 24.65449617151171\n",
      "Epoch 426 / 1000 loss: 24.654493438079953\n",
      "Epoch 427 / 1000 loss: 24.654492053203285\n",
      "Epoch 428 / 1000 loss: 24.654490889050066\n",
      "Epoch 429 / 1000 loss: 24.654489882290363\n",
      "Epoch 430 / 1000 loss: 24.654489020816982\n",
      "Epoch 431 / 1000 loss: 24.65448835399002\n",
      "Epoch 432 / 1000 loss: 24.6544875157997\n",
      "Epoch 433 / 1000 loss: 24.654486818239093\n",
      "Epoch 434 / 1000 loss: 24.654486161656678\n",
      "Epoch 435 / 1000 loss: 24.654485532082617\n",
      "Epoch 436 / 1000 loss: 24.65448487829417\n",
      "Epoch 437 / 1000 loss: 24.654484235681593\n",
      "Epoch 438 / 1000 loss: 24.654483544640243\n",
      "Epoch 439 / 1000 loss: 24.654483086429536\n",
      "Epoch 440 / 1000 loss: 24.654482427053154\n",
      "Epoch 441 / 1000 loss: 24.654481848701835\n",
      "Epoch 442 / 1000 loss: 24.654481288976967\n",
      "Epoch 443 / 1000 loss: 24.654480795376003\n",
      "Epoch 444 / 1000 loss: 24.654480220749974\n",
      "Epoch 445 / 1000 loss: 24.6544796731323\n",
      "Epoch 446 / 1000 loss: 24.654479186981916\n",
      "Epoch 447 / 1000 loss: 24.654478603973985\n",
      "Epoch 448 / 1000 loss: 24.6544779939577\n",
      "Epoch 449 / 1000 loss: 24.65447754226625\n",
      "Epoch 450 / 1000 loss: 24.65447710454464\n",
      "Epoch 451 / 1000 loss: 24.65447653271258\n",
      "Epoch 452 / 1000 loss: 24.654475981369615\n",
      "Epoch 453 / 1000 loss: 24.654475611634552\n",
      "Epoch 454 / 1000 loss: 24.654475163668394\n",
      "Epoch 455 / 1000 loss: 24.654474501498044\n",
      "Epoch 456 / 1000 loss: 24.654473996721208\n",
      "Epoch 457 / 1000 loss: 24.65447374433279\n",
      "Epoch 458 / 1000 loss: 24.654473138041794\n",
      "Epoch 459 / 1000 loss: 24.654472555033863\n",
      "Epoch 460 / 1000 loss: 24.654472204856575\n",
      "Epoch 461 / 1000 loss: 24.654471764340997\n",
      "Epoch 462 / 1000 loss: 24.65447108540684\n",
      "Epoch 463 / 1000 loss: 24.65447058621794\n",
      "Epoch 464 / 1000 loss: 24.65447035152465\n",
      "Epoch 465 / 1000 loss: 24.65446973219514\n",
      "Epoch 466 / 1000 loss: 24.654469187371433\n",
      "Epoch 467 / 1000 loss: 24.65446879155934\n",
      "Epoch 468 / 1000 loss: 24.654468334279954\n",
      "Epoch 469 / 1000 loss: 24.654467933811247\n",
      "Epoch 470 / 1000 loss: 24.654467408545315\n",
      "Epoch 471 / 1000 loss: 24.654467098414898\n",
      "Epoch 472 / 1000 loss: 24.654466683976352\n",
      "Epoch 473 / 1000 loss: 24.65446604602039\n",
      "Epoch 474 / 1000 loss: 24.654465687461197\n",
      "Epoch 475 / 1000 loss: 24.65446529444307\n",
      "Epoch 476 / 1000 loss: 24.654464690946043\n",
      "Epoch 477 / 1000 loss: 24.65446429513395\n",
      "Epoch 478 / 1000 loss: 24.654464066959918\n",
      "Epoch 479 / 1000 loss: 24.654463565908372\n",
      "Epoch 480 / 1000 loss: 24.65446303691715\n",
      "Epoch 481 / 1000 loss: 24.65446261037141\n",
      "Epoch 482 / 1000 loss: 24.65446225181222\n",
      "Epoch 483 / 1000 loss: 24.654461747966707\n",
      "Epoch 484 / 1000 loss: 24.654461284168065\n",
      "Epoch 485 / 1000 loss: 24.65446104016155\n",
      "Epoch 486 / 1000 loss: 24.654460636898875\n",
      "Epoch 487 / 1000 loss: 24.654460086487234\n",
      "Epoch 488 / 1000 loss: 24.654459786601365\n",
      "Epoch 489 / 1000 loss: 24.65445930045098\n",
      "Epoch 490 / 1000 loss: 24.6544588515535\n",
      "Epoch 491 / 1000 loss: 24.654458512552083\n",
      "Epoch 492 / 1000 loss: 24.654458101838827\n",
      "Epoch 493 / 1000 loss: 24.654457583092153\n",
      "Epoch 494 / 1000 loss: 24.654457226395607\n",
      "Epoch 495 / 1000 loss: 24.65445703174919\n",
      "Epoch 496 / 1000 loss: 24.654456446878612\n",
      "Epoch 497 / 1000 loss: 24.654456024058163\n",
      "Epoch 498 / 1000 loss: 24.65445562079549\n",
      "Epoch 499 / 1000 loss: 24.65445522312075\n",
      "Epoch 500 / 1000 loss: 24.65445472765714\n",
      "Epoch 501 / 1000 loss: 24.654454596340656\n",
      "Epoch 502 / 1000 loss: 24.654454092495143\n",
      "Epoch 503 / 1000 loss: 24.65445369388908\n",
      "Epoch 504 / 1000 loss: 24.65445325989276\n",
      "Epoch 505 / 1000 loss: 24.65445284638554\n",
      "Epoch 506 / 1000 loss: 24.65445247385651\n",
      "Epoch 507 / 1000 loss: 24.65445206873119\n",
      "Epoch 508 / 1000 loss: 24.65445172879845\n",
      "Epoch 509 / 1000 loss: 24.65445135626942\n",
      "Epoch 510 / 1000 loss: 24.654450872913003\n",
      "Epoch 511 / 1000 loss: 24.65445065498352\n",
      "Epoch 512 / 1000 loss: 24.65445026103407\n",
      "Epoch 513 / 1000 loss: 24.654449870809913\n",
      "Epoch 514 / 1000 loss: 24.654449448920786\n",
      "Epoch 515 / 1000 loss: 24.654449033550918\n",
      "Epoch 516 / 1000 loss: 24.654448667541146\n",
      "Epoch 517 / 1000 loss: 24.65444827079773\n",
      "Epoch 518 / 1000 loss: 24.654447911307216\n",
      "Epoch 519 / 1000 loss: 24.654447383247316\n",
      "Epoch 520 / 1000 loss: 24.65444707684219\n",
      "Epoch 521 / 1000 loss: 24.654446678236127\n",
      "Epoch 522 / 1000 loss: 24.654446402564645\n",
      "Epoch 523 / 1000 loss: 24.654445975087583\n",
      "Epoch 524 / 1000 loss: 24.654445650056005\n",
      "Epoch 525 / 1000 loss: 24.6544451052323\n",
      "Epoch 526 / 1000 loss: 24.654444851912558\n",
      "Epoch 527 / 1000 loss: 24.654444402083755\n",
      "Epoch 528 / 1000 loss: 24.654444011859596\n",
      "Epoch 529 / 1000 loss: 24.654443812556565\n",
      "Epoch 530 / 1000 loss: 24.654443343169987\n",
      "Epoch 531 / 1000 loss: 24.654442919418216\n",
      "Epoch 532 / 1000 loss: 24.65444262791425\n",
      "Epoch 533 / 1000 loss: 24.654442210681736\n",
      "Epoch 534 / 1000 loss: 24.654441769234836\n",
      "Epoch 535 / 1000 loss: 24.654441602528095\n",
      "Epoch 536 / 1000 loss: 24.654441133141518\n",
      "Epoch 537 / 1000 loss: 24.65444078296423\n",
      "Epoch 538 / 1000 loss: 24.65444037783891\n",
      "Epoch 539 / 1000 loss: 24.65444005280733\n",
      "Epoch 540 / 1000 loss: 24.654439677484334\n",
      "Epoch 541 / 1000 loss: 24.65443936921656\n",
      "Epoch 542 / 1000 loss: 24.65443904977292\n",
      "Epoch 543 / 1000 loss: 24.654438524506986\n",
      "Epoch 544 / 1000 loss: 24.654438260942698\n",
      "Epoch 545 / 1000 loss: 24.654437818564475\n",
      "Epoch 546 / 1000 loss: 24.65443741902709\n",
      "Epoch 547 / 1000 loss: 24.65443704277277\n",
      "Epoch 548 / 1000 loss: 24.65443667769432\n",
      "Epoch 549 / 1000 loss: 24.654436407610774\n",
      "Epoch 550 / 1000 loss: 24.654436031356454\n",
      "Epoch 551 / 1000 loss: 24.654435713775456\n",
      "Epoch 552 / 1000 loss: 24.65443520154804\n",
      "Epoch 553 / 1000 loss: 24.65443495195359\n",
      "Epoch 554 / 1000 loss: 24.654434517025948\n",
      "Epoch 555 / 1000 loss: 24.654434135183692\n",
      "Epoch 556 / 1000 loss: 24.65443405508995\n",
      "Epoch 557 / 1000 loss: 24.654434714466333\n",
      "Epoch 558 / 1000 loss: 24.654433019459248\n",
      "Epoch 559 / 1000 loss: 24.654432460665703\n",
      "Epoch 560 / 1000 loss: 24.65443194285035\n",
      "Epoch 561 / 1000 loss: 24.654431424103677\n",
      "Epoch 562 / 1000 loss: 24.654431133531034\n",
      "Epoch 563 / 1000 loss: 24.654430963099003\n",
      "Epoch 564 / 1000 loss: 24.654430399648845\n",
      "Epoch 565 / 1000 loss: 24.65442988369614\n",
      "Epoch 566 / 1000 loss: 24.654429364018142\n",
      "Epoch 567 / 1000 loss: 24.654428872279823\n",
      "Epoch 568 / 1000 loss: 24.6544285165146\n",
      "Epoch 569 / 1000 loss: 24.654428096488118\n",
      "Epoch 570 / 1000 loss: 24.654427610337734\n",
      "Epoch 571 / 1000 loss: 24.654427360743284\n",
      "Epoch 572 / 1000 loss: 24.654430121183395\n",
      "Epoch 573 / 1000 loss: 24.65442268922925\n",
      "Epoch 574 / 1000 loss: 24.65441713295877\n",
      "Epoch 575 / 1000 loss: 24.65441281348467\n",
      "Epoch 576 / 1000 loss: 24.65440962649882\n",
      "Epoch 577 / 1000 loss: 24.65440175589174\n",
      "Epoch 578 / 1000 loss: 24.654397354461253\n",
      "Epoch 579 / 1000 loss: 24.654394580982625\n",
      "Epoch 580 / 1000 loss: 24.65439273789525\n",
      "Epoch 581 / 1000 loss: 24.654389668256044\n",
      "Epoch 582 / 1000 loss: 24.654387827962637\n",
      "Epoch 583 / 1000 loss: 24.654386199079454\n",
      "Epoch 584 / 1000 loss: 24.65438517089933\n",
      "Epoch 585 / 1000 loss: 24.654384087771177\n",
      "Epoch 586 / 1000 loss: 24.654382897540927\n",
      "Epoch 587 / 1000 loss: 24.654382239095867\n",
      "Epoch 588 / 1000 loss: 24.654381371103227\n",
      "Epoch 589 / 1000 loss: 24.6543803345412\n",
      "Epoch 590 / 1000 loss: 24.65437983069569\n",
      "Epoch 591 / 1000 loss: 24.65437904652208\n",
      "Epoch 592 / 1000 loss: 24.654378153383732\n",
      "Epoch 593 / 1000 loss: 24.654377712868154\n",
      "Epoch 594 / 1000 loss: 24.654377045109868\n",
      "Epoch 595 / 1000 loss: 24.654376176185906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 596 / 1000 loss: 24.654375828802586\n",
      "Epoch 597 / 1000 loss: 24.654375219717622\n",
      "Epoch 598 / 1000 loss: 24.65437437221408\n",
      "Epoch 599 / 1000 loss: 24.65437411237508\n",
      "Epoch 600 / 1000 loss: 24.654373257420957\n",
      "Epoch 601 / 1000 loss: 24.654373046010733\n",
      "Epoch 602 / 1000 loss: 24.654372463934124\n",
      "Epoch 603 / 1000 loss: 24.65437174309045\n",
      "Epoch 604 / 1000 loss: 24.654371435754\n",
      "Epoch 605 / 1000 loss: 24.65437068976462\n",
      "Epoch 606 / 1000 loss: 24.654370460659266\n",
      "Epoch 607 / 1000 loss: 24.654369676485658\n",
      "Epoch 608 / 1000 loss: 24.65436947531998\n",
      "Epoch 609 / 1000 loss: 24.65436871536076\n",
      "Epoch 610 / 1000 loss: 24.654368506744504\n",
      "Epoch 611 / 1000 loss: 24.654367703944445\n",
      "Epoch 612 / 1000 loss: 24.65436657052487\n",
      "Epoch 613 / 1000 loss: 24.654365167953074\n",
      "Epoch 614 / 1000 loss: 24.654364702291787\n",
      "Epoch 615 / 1000 loss: 24.654363818466663\n",
      "Epoch 616 / 1000 loss: 24.654363453388214\n",
      "Epoch 617 / 1000 loss: 24.654362651519477\n",
      "Epoch 618 / 1000 loss: 24.65436233021319\n",
      "Epoch 619 / 1000 loss: 24.65436158888042\n",
      "Epoch 620 / 1000 loss: 24.654361334629357\n",
      "Epoch 621 / 1000 loss: 24.654360628686845\n",
      "Epoch 622 / 1000 loss: 24.654360353946686\n",
      "Epoch 623 / 1000 loss: 24.654359648935497\n",
      "Epoch 624 / 1000 loss: 24.65435940399766\n",
      "Epoch 625 / 1000 loss: 24.654358721338212\n",
      "Epoch 626 / 1000 loss: 24.654358490370214\n",
      "Epoch 627 / 1000 loss: 24.65435787383467\n",
      "Epoch 628 / 1000 loss: 24.654357574880123\n",
      "Epoch 629 / 1000 loss: 24.654356956481934\n",
      "Epoch 630 / 1000 loss: 24.65435673482716\n",
      "Epoch 631 / 1000 loss: 24.654356153681874\n",
      "Epoch 632 / 1000 loss: 24.6543556349352\n",
      "Epoch 633 / 1000 loss: 24.654355434700847\n",
      "Epoch 634 / 1000 loss: 24.6543548675254\n",
      "Epoch 635 / 1000 loss: 24.654354639351368\n",
      "Epoch 636 / 1000 loss: 24.654354092665017\n",
      "Epoch 637 / 1000 loss: 24.65435369964689\n",
      "Epoch 638 / 1000 loss: 24.654353439807892\n",
      "Epoch 639 / 1000 loss: 24.654352829791605\n",
      "Epoch 640 / 1000 loss: 24.6543524004519\n",
      "Epoch 641 / 1000 loss: 24.654352229088545\n",
      "Epoch 642 / 1000 loss: 24.654351646080613\n",
      "Epoch 643 / 1000 loss: 24.654351219534874\n",
      "Epoch 644 / 1000 loss: 24.65435098670423\n",
      "Epoch 645 / 1000 loss: 24.654350446537137\n",
      "Epoch 646 / 1000 loss: 24.6543499995023\n",
      "Epoch 647 / 1000 loss: 24.65434984769672\n",
      "Epoch 648 / 1000 loss: 24.65434930752963\n",
      "Epoch 649 / 1000 loss: 24.6543488483876\n",
      "Epoch 650 / 1000 loss: 24.654348637908697\n",
      "Epoch 651 / 1000 loss: 24.654348120093346\n",
      "Epoch 652 / 1000 loss: 24.654347702860832\n",
      "Epoch 653 / 1000 loss: 24.65434749610722\n",
      "Epoch 654 / 1000 loss: 24.654347031377256\n",
      "Epoch 655 / 1000 loss: 24.65434655919671\n",
      "Epoch 656 / 1000 loss: 24.654346372932196\n",
      "Epoch 657 / 1000 loss: 24.654345870949328\n",
      "Epoch 658 / 1000 loss: 24.65434540901333\n",
      "Epoch 659 / 1000 loss: 24.654345027171075\n",
      "Epoch 660 / 1000 loss: 24.654344838112593\n",
      "Epoch 661 / 1000 loss: 24.65434433054179\n",
      "Epoch 662 / 1000 loss: 24.654343905858696\n",
      "Epoch 663 / 1000 loss: 24.654343504458666\n",
      "Epoch 664 / 1000 loss: 24.65434328932315\n",
      "Epoch 665 / 1000 loss: 24.65434293076396\n",
      "Epoch 666 / 1000 loss: 24.654342448338866\n",
      "Epoch 667 / 1000 loss: 24.654342057183385\n",
      "Epoch 668 / 1000 loss: 24.654341800138354\n",
      "Epoch 669 / 1000 loss: 24.6543413316831\n",
      "Epoch 670 / 1000 loss: 24.65434093400836\n",
      "Epoch 671 / 1000 loss: 24.654340566135943\n",
      "Epoch 672 / 1000 loss: 24.654340314678848\n",
      "Epoch 673 / 1000 loss: 24.65433984901756\n",
      "Epoch 674 / 1000 loss: 24.654339464381337\n",
      "Epoch 675 / 1000 loss: 24.654339149594307\n",
      "Epoch 676 / 1000 loss: 24.65433870628476\n",
      "Epoch 677 / 1000 loss: 24.65433852095157\n",
      "Epoch 678 / 1000 loss: 24.654338038526475\n",
      "Epoch 679 / 1000 loss: 24.65433777309954\n",
      "Epoch 680 / 1000 loss: 24.6543373093009\n",
      "Epoch 681 / 1000 loss: 24.654336896725\n",
      "Epoch 682 / 1000 loss: 24.654336706735194\n",
      "Epoch 683 / 1000 loss: 24.654336359351873\n",
      "Epoch 684 / 1000 loss: 24.65433592069894\n",
      "Epoch 685 / 1000 loss: 24.65433546155691\n",
      "Epoch 686 / 1000 loss: 24.654335112310946\n",
      "Epoch 687 / 1000 loss: 24.654334771446884\n",
      "Epoch 688 / 1000 loss: 24.65433454979211\n",
      "Epoch 689 / 1000 loss: 24.654334132559597\n",
      "Epoch 690 / 1000 loss: 24.654333827085793\n",
      "Epoch 691 / 1000 loss: 24.654333387501538\n",
      "Epoch 692 / 1000 loss: 24.654333032667637\n",
      "Epoch 693 / 1000 loss: 24.65433264989406\n",
      "Epoch 694 / 1000 loss: 24.6543322885409\n",
      "Epoch 695 / 1000 loss: 24.654332112520933\n",
      "Epoch 696 / 1000 loss: 24.65433164872229\n",
      "Epoch 697 / 1000 loss: 24.65433141309768\n",
      "Epoch 698 / 1000 loss: 24.654330954886973\n",
      "Epoch 699 / 1000 loss: 24.654330567456782\n",
      "Epoch 700 / 1000 loss: 24.654330293647945\n",
      "Epoch 701 / 1000 loss: 24.654329918324947\n",
      "Epoch 702 / 1000 loss: 24.654329555109143\n",
      "Epoch 703 / 1000 loss: 24.65432926826179\n",
      "Epoch 704 / 1000 loss: 24.654329005628824\n",
      "Epoch 705 / 1000 loss: 24.65432854462415\n",
      "Epoch 706 / 1000 loss: 24.654328145086765\n",
      "Epoch 707 / 1000 loss: 24.654327801428735\n",
      "Epoch 708 / 1000 loss: 24.654315404593945\n",
      "Epoch 709 / 1000 loss: 24.654309977777302\n",
      "Epoch 710 / 1000 loss: 24.654306986369193\n",
      "Epoch 711 / 1000 loss: 24.654305182397366\n",
      "Epoch 712 / 1000 loss: 24.654303860850632\n",
      "Epoch 713 / 1000 loss: 24.654302835464478\n",
      "Epoch 714 / 1000 loss: 24.654301980510354\n",
      "Epoch 715 / 1000 loss: 24.654301232658327\n",
      "Epoch 716 / 1000 loss: 24.65430055372417\n",
      "Epoch 717 / 1000 loss: 24.65429975744337\n",
      "Epoch 718 / 1000 loss: 24.654299194924533\n",
      "Epoch 719 / 1000 loss: 24.654296742752194\n",
      "Epoch 720 / 1000 loss: 24.65429623518139\n",
      "Epoch 721 / 1000 loss: 24.65429376810789\n",
      "Epoch 722 / 1000 loss: 24.654290807433426\n",
      "Epoch 723 / 1000 loss: 24.654288698919117\n",
      "Epoch 724 / 1000 loss: 24.654287087731063\n",
      "Epoch 725 / 1000 loss: 24.654285754077137\n",
      "Epoch 726 / 1000 loss: 24.654285004362464\n",
      "Epoch 727 / 1000 loss: 24.65428197570145\n",
      "Epoch 728 / 1000 loss: 24.654280437156558\n",
      "Epoch 729 / 1000 loss: 24.654278964735568\n",
      "Epoch 730 / 1000 loss: 24.654277629218996\n",
      "Epoch 731 / 1000 loss: 24.654276540502906\n",
      "Epoch 732 / 1000 loss: 24.654275603592396\n",
      "Epoch 733 / 1000 loss: 24.65427466109395\n",
      "Epoch 734 / 1000 loss: 24.654273769818246\n",
      "Epoch 735 / 1000 loss: 24.654273012652993\n",
      "Epoch 736 / 1000 loss: 24.654272217303514\n",
      "Epoch 737 / 1000 loss: 24.654271124862134\n",
      "Epoch 738 / 1000 loss: 24.654270307160914\n",
      "Epoch 739 / 1000 loss: 24.65426948852837\n",
      "Epoch 740 / 1000 loss: 24.654268734157085\n",
      "Epoch 741 / 1000 loss: 24.654268083162606\n",
      "Epoch 742 / 1000 loss: 24.65426737163216\n",
      "Epoch 743 / 1000 loss: 24.65426669921726\n",
      "Epoch 744 / 1000 loss: 24.654266072437167\n",
      "Epoch 745 / 1000 loss: 24.65426543727517\n",
      "Epoch 746 / 1000 loss: 24.654264858923852\n",
      "Epoch 747 / 1000 loss: 24.654264274984598\n",
      "Epoch 748 / 1000 loss: 24.654263626784086\n",
      "Epoch 749 / 1000 loss: 24.654263022355735\n",
      "Epoch 750 / 1000 loss: 24.65426249615848\n",
      "Epoch 751 / 1000 loss: 24.654261933639646\n",
      "Epoch 752 / 1000 loss: 24.654261370189488\n",
      "Epoch 753 / 1000 loss: 24.654260802082717\n",
      "Epoch 754 / 1000 loss: 24.65426027495414\n",
      "Epoch 755 / 1000 loss: 24.65425972919911\n",
      "Epoch 756 / 1000 loss: 24.65425918251276\n",
      "Epoch 757 / 1000 loss: 24.654258667491376\n",
      "Epoch 758 / 1000 loss: 24.654258150607347\n",
      "Epoch 759 / 1000 loss: 24.654257701709867\n",
      "Epoch 760 / 1000 loss: 24.65425721835345\n",
      "Epoch 761 / 1000 loss: 24.654256633482873\n",
      "Epoch 762 / 1000 loss: 24.654256135225296\n",
      "Epoch 763 / 1000 loss: 24.65425562299788\n",
      "Epoch 764 / 1000 loss: 24.654255118221045\n",
      "Epoch 765 / 1000 loss: 24.654254685156047\n",
      "Epoch 766 / 1000 loss: 24.654254153370857\n",
      "Epoch 767 / 1000 loss: 24.65425364486873\n",
      "Epoch 768 / 1000 loss: 24.654253135435283\n",
      "Epoch 769 / 1000 loss: 24.654252842068672\n",
      "Epoch 770 / 1000 loss: 24.654252289794385\n",
      "Epoch 771 / 1000 loss: 24.654251877218485\n",
      "Epoch 772 / 1000 loss: 24.65425129327923\n",
      "Epoch 773 / 1000 loss: 24.654250802472234\n",
      "Epoch 774 / 1000 loss: 24.65425032004714\n",
      "Epoch 775 / 1000 loss: 24.654249837622046\n",
      "Epoch 776 / 1000 loss: 24.654249402694404\n",
      "Epoch 777 / 1000 loss: 24.654248962178826\n",
      "Epoch 778 / 1000 loss: 24.654248479753733\n",
      "Epoch 779 / 1000 loss: 24.654247959144413\n",
      "Epoch 780 / 1000 loss: 24.654247472062707\n",
      "Epoch 781 / 1000 loss: 24.65424709022045\n",
      "Epoch 782 / 1000 loss: 24.654246542602777\n",
      "Epoch 783 / 1000 loss: 24.654246093705297\n",
      "Epoch 784 / 1000 loss: 24.654245642013848\n",
      "Epoch 785 / 1000 loss: 24.654245256446302\n",
      "Epoch 786 / 1000 loss: 24.654244764707983\n",
      "Epoch 787 / 1000 loss: 24.654244267381728\n",
      "Epoch 788 / 1000 loss: 24.654243791475892\n",
      "Epoch 789 / 1000 loss: 24.654243426397443\n",
      "Epoch 790 / 1000 loss: 24.65424293745309\n",
      "Epoch 791 / 1000 loss: 24.65424250718206\n",
      "Epoch 792 / 1000 loss: 24.654241993092\n",
      "Epoch 793 / 1000 loss: 24.654241639189422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 794 / 1000 loss: 24.65424111764878\n",
      "Epoch 795 / 1000 loss: 24.65424075908959\n",
      "Epoch 796 / 1000 loss: 24.654240272939205\n",
      "Epoch 797 / 1000 loss: 24.654239911586046\n",
      "Epoch 798 / 1000 loss: 24.65423938073218\n",
      "Epoch 799 / 1000 loss: 24.65423903800547\n",
      "Epoch 800 / 1000 loss: 24.65423865802586\n",
      "Epoch 801 / 1000 loss: 24.654238206334412\n",
      "Epoch 802 / 1000 loss: 24.654237613081932\n",
      "Epoch 803 / 1000 loss: 24.654237190261483\n",
      "Epoch 804 / 1000 loss: 24.65423672273755\n",
      "Epoch 805 / 1000 loss: 24.654236365109682\n",
      "Epoch 806 / 1000 loss: 24.654235860332847\n",
      "Epoch 807 / 1000 loss: 24.654235370457172\n",
      "Epoch 808 / 1000 loss: 24.654234948568046\n",
      "Epoch 809 / 1000 loss: 24.654234568588436\n",
      "Epoch 810 / 1000 loss: 24.654234209097922\n",
      "Epoch 811 / 1000 loss: 24.654233663342893\n",
      "Epoch 812 / 1000 loss: 24.65423321723938\n",
      "Epoch 813 / 1000 loss: 24.654232871718705\n",
      "Epoch 814 / 1000 loss: 24.65423240698874\n",
      "Epoch 815 / 1000 loss: 24.65423193667084\n",
      "Epoch 816 / 1000 loss: 24.654231623746455\n",
      "Epoch 817 / 1000 loss: 24.654231016524136\n",
      "Epoch 818 / 1000 loss: 24.65422919858247\n",
      "Epoch 819 / 1000 loss: 24.65422860905528\n",
      "Epoch 820 / 1000 loss: 24.654227810911834\n",
      "Epoch 821 / 1000 loss: 24.654227200895548\n",
      "Epoch 822 / 1000 loss: 24.654226646758616\n",
      "Epoch 823 / 1000 loss: 24.654226254671812\n",
      "Epoch 824 / 1000 loss: 24.65422568935901\n",
      "Epoch 825 / 1000 loss: 24.65422515757382\n",
      "Epoch 826 / 1000 loss: 24.654224682599306\n",
      "Epoch 827 / 1000 loss: 24.654224164783955\n",
      "Epoch 828 / 1000 loss: 24.654223890975118\n",
      "Epoch 829 / 1000 loss: 24.654223307035863\n",
      "Epoch 830 / 1000 loss: 24.654222847893834\n",
      "Epoch 831 / 1000 loss: 24.654222345910966\n",
      "Epoch 832 / 1000 loss: 24.654221982695162\n",
      "Epoch 833 / 1000 loss: 24.654221639037132\n",
      "Epoch 834 / 1000 loss: 24.65422080643475\n",
      "Epoch 835 / 1000 loss: 24.654220183379948\n",
      "Epoch 836 / 1000 loss: 24.65421966649592\n",
      "Epoch 837 / 1000 loss: 24.65421910583973\n",
      "Epoch 838 / 1000 loss: 24.654218667186797\n",
      "Epoch 839 / 1000 loss: 24.65421813633293\n",
      "Epoch 840 / 1000 loss: 24.65421767346561\n",
      "Epoch 841 / 1000 loss: 24.654217188246548\n",
      "Epoch 842 / 1000 loss: 24.654216712340713\n",
      "Epoch 843 / 1000 loss: 24.654216420836747\n",
      "Epoch 844 / 1000 loss: 24.654215900227427\n",
      "Epoch 845 / 1000 loss: 24.654215377755463\n",
      "Epoch 846 / 1000 loss: 24.654214929789305\n",
      "Epoch 847 / 1000 loss: 24.654214491136372\n",
      "Epoch 848 / 1000 loss: 24.65421413630247\n",
      "Epoch 849 / 1000 loss: 24.654214480891824\n",
      "Epoch 850 / 1000 loss: 24.654212643392384\n",
      "Epoch 851 / 1000 loss: 24.654202702455223\n",
      "Epoch 852 / 1000 loss: 24.654199068434536\n",
      "Epoch 853 / 1000 loss: 24.65419604908675\n",
      "Epoch 854 / 1000 loss: 24.654193592257798\n",
      "Epoch 855 / 1000 loss: 24.654191905632615\n",
      "Epoch 856 / 1000 loss: 24.65419060923159\n",
      "Epoch 857 / 1000 loss: 24.654184200800955\n",
      "Epoch 858 / 1000 loss: 24.654179750941694\n",
      "Epoch 859 / 1000 loss: 24.654177312739193\n",
      "Epoch 860 / 1000 loss: 24.654175328090787\n",
      "Epoch 861 / 1000 loss: 24.654173471033573\n",
      "Epoch 862 / 1000 loss: 24.654172253794968\n",
      "Epoch 863 / 1000 loss: 24.654170751571655\n",
      "Epoch 864 / 1000 loss: 24.654170844703913\n",
      "Epoch 865 / 1000 loss: 24.65416831150651\n",
      "Epoch 866 / 1000 loss: 24.654169416986406\n",
      "Epoch 867 / 1000 loss: 24.654167300090194\n",
      "Epoch 868 / 1000 loss: 24.654162612743676\n",
      "Epoch 869 / 1000 loss: 24.654161530546844\n",
      "Epoch 870 / 1000 loss: 24.654159483499825\n",
      "Epoch 871 / 1000 loss: 24.654157118871808\n",
      "Epoch 872 / 1000 loss: 24.654157075099647\n",
      "Epoch 873 / 1000 loss: 24.654155931435525\n",
      "Epoch 874 / 1000 loss: 24.654153916053474\n",
      "Epoch 875 / 1000 loss: 24.654154241085052\n",
      "Epoch 876 / 1000 loss: 24.65415344759822\n",
      "Epoch 877 / 1000 loss: 24.654151929542422\n",
      "Epoch 878 / 1000 loss: 24.65414986014366\n",
      "Epoch 879 / 1000 loss: 24.654149963520467\n",
      "Epoch 880 / 1000 loss: 24.65415028948337\n",
      "Epoch 881 / 1000 loss: 24.654148744419217\n",
      "Epoch 882 / 1000 loss: 24.654146598652005\n",
      "Epoch 883 / 1000 loss: 24.654147090390325\n",
      "Epoch 884 / 1000 loss: 24.654146280139685\n",
      "Epoch 885 / 1000 loss: 24.654145664535463\n",
      "Epoch 886 / 1000 loss: 24.65414398163557\n",
      "Epoch 887 / 1000 loss: 24.654144511558115\n",
      "Epoch 888 / 1000 loss: 24.654143933206797\n",
      "Epoch 889 / 1000 loss: 24.65414326544851\n",
      "Epoch 890 / 1000 loss: 24.654141675680876\n",
      "Epoch 891 / 1000 loss: 24.654142260551453\n",
      "Epoch 892 / 1000 loss: 24.654141625389457\n",
      "Epoch 893 / 1000 loss: 24.654141186736524\n",
      "Epoch 894 / 1000 loss: 24.65413963329047\n",
      "Epoch 895 / 1000 loss: 24.654140600003302\n",
      "Epoch 896 / 1000 loss: 24.65413407422602\n",
      "Epoch 897 / 1000 loss: 24.654128483496606\n",
      "Epoch 898 / 1000 loss: 24.654121636413038\n",
      "Epoch 899 / 1000 loss: 24.65411753114313\n",
      "Epoch 900 / 1000 loss: 24.654113653115928\n",
      "Epoch 901 / 1000 loss: 24.65411222819239\n",
      "Epoch 902 / 1000 loss: 24.654110023751855\n",
      "Epoch 903 / 1000 loss: 24.65410863235593\n",
      "Epoch 904 / 1000 loss: 24.65410595946014\n",
      "Epoch 905 / 1000 loss: 24.65410407446325\n",
      "Epoch 906 / 1000 loss: 24.654102597385645\n",
      "Epoch 907 / 1000 loss: 24.654102182015777\n",
      "Epoch 908 / 1000 loss: 24.65410059504211\n",
      "Epoch 909 / 1000 loss: 24.65409937221557\n",
      "Epoch 910 / 1000 loss: 24.654099070467055\n",
      "Epoch 911 / 1000 loss: 24.654097852297127\n",
      "Epoch 912 / 1000 loss: 24.654096907004714\n",
      "Epoch 913 / 1000 loss: 24.654096025042236\n",
      "Epoch 914 / 1000 loss: 24.654096027836204\n",
      "Epoch 915 / 1000 loss: 24.654094733297825\n",
      "Epoch 916 / 1000 loss: 24.654093620367348\n",
      "Epoch 917 / 1000 loss: 24.654092834331095\n",
      "Epoch 918 / 1000 loss: 24.65409227926284\n",
      "Epoch 919 / 1000 loss: 24.65409254282713\n",
      "Epoch 920 / 1000 loss: 24.654091145843267\n",
      "Epoch 921 / 1000 loss: 24.654090432450175\n",
      "Epoch 922 / 1000 loss: 24.654089827090502\n",
      "Epoch 923 / 1000 loss: 24.65408427082002\n",
      "Epoch 924 / 1000 loss: 24.654041826725006\n",
      "Epoch 925 / 1000 loss: 24.653998997062445\n",
      "Epoch 926 / 1000 loss: 24.653979714028537\n",
      "Epoch 927 / 1000 loss: 24.653968919999897\n",
      "Epoch 928 / 1000 loss: 24.653961245901883\n",
      "Epoch 929 / 1000 loss: 24.653955856338143\n",
      "Epoch 930 / 1000 loss: 24.65394388884306\n",
      "Epoch 931 / 1000 loss: 24.65393586922437\n",
      "Epoch 932 / 1000 loss: 24.65392240229994\n",
      "Epoch 933 / 1000 loss: 24.653928499668837\n",
      "Epoch 934 / 1000 loss: 24.653934882953763\n",
      "Epoch 935 / 1000 loss: 24.653920518234372\n",
      "Epoch 936 / 1000 loss: 24.6539140753448\n",
      "Epoch 937 / 1000 loss: 24.653905354440212\n",
      "Epoch 938 / 1000 loss: 24.653899886645377\n",
      "Epoch 939 / 1000 loss: 24.653879494406283\n",
      "Epoch 940 / 1000 loss: 24.653767962940037\n",
      "Epoch 941 / 1000 loss: 24.653654027730227\n",
      "Epoch 942 / 1000 loss: 24.653566014021635\n",
      "Epoch 943 / 1000 loss: 24.65331137739122\n",
      "Epoch 944 / 1000 loss: 24.653185673989356\n",
      "Epoch 945 / 1000 loss: 24.653130288235843\n",
      "Epoch 946 / 1000 loss: 24.653056295588613\n",
      "Epoch 947 / 1000 loss: 24.653091987594962\n",
      "Epoch 948 / 1000 loss: 24.652910602279007\n",
      "Epoch 949 / 1000 loss: 24.652849346399307\n",
      "Epoch 950 / 1000 loss: 24.652718637138605\n",
      "Epoch 951 / 1000 loss: 24.65272993966937\n",
      "Epoch 952 / 1000 loss: 24.652666501700878\n",
      "Epoch 953 / 1000 loss: 24.652605117298663\n",
      "Epoch 954 / 1000 loss: 24.65253426413983\n",
      "Epoch 955 / 1000 loss: 24.652542116120458\n",
      "Epoch 956 / 1000 loss: 24.652401165105402\n",
      "Epoch 957 / 1000 loss: 24.652339993044734\n",
      "Epoch 958 / 1000 loss: 24.65228094626218\n",
      "Epoch 959 / 1000 loss: 24.65222934167832\n",
      "Epoch 960 / 1000 loss: 24.652143439278007\n",
      "Epoch 961 / 1000 loss: 24.652058228850365\n",
      "Epoch 962 / 1000 loss: 24.651820730417967\n",
      "Epoch 963 / 1000 loss: 24.651742505840957\n",
      "Epoch 964 / 1000 loss: 24.651644461788237\n",
      "Epoch 965 / 1000 loss: 24.651535462588072\n",
      "Epoch 966 / 1000 loss: 24.651438288390636\n",
      "Epoch 967 / 1000 loss: 24.651392293162644\n",
      "Epoch 968 / 1000 loss: 24.651325930841267\n",
      "Epoch 969 / 1000 loss: 24.65129680838436\n",
      "Epoch 970 / 1000 loss: 24.651226237416267\n",
      "Epoch 971 / 1000 loss: 24.651197161525488\n",
      "Epoch 972 / 1000 loss: 24.65117614530027\n",
      "Epoch 973 / 1000 loss: 24.651131492108107\n",
      "Epoch 974 / 1000 loss: 24.65109920874238\n",
      "Epoch 975 / 1000 loss: 24.65108564309776\n",
      "Epoch 976 / 1000 loss: 24.651033005677164\n",
      "Epoch 977 / 1000 loss: 24.651002581231296\n",
      "Epoch 978 / 1000 loss: 24.65097915288061\n",
      "Epoch 979 / 1000 loss: 24.65097494237125\n",
      "Epoch 980 / 1000 loss: 24.65094716195017\n",
      "Epoch 981 / 1000 loss: 24.651007179170847\n",
      "Epoch 982 / 1000 loss: 24.650636478327215\n",
      "Epoch 983 / 1000 loss: 24.6502077402547\n",
      "Epoch 984 / 1000 loss: 24.65002710558474\n",
      "Epoch 985 / 1000 loss: 24.649850913323462\n",
      "Epoch 986 / 1000 loss: 24.649650624021888\n",
      "Epoch 987 / 1000 loss: 24.64948690496385\n",
      "Epoch 988 / 1000 loss: 24.649370760656893\n",
      "Epoch 989 / 1000 loss: 24.64842527359724\n",
      "Epoch 990 / 1000 loss: 24.647655703127384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 991 / 1000 loss: 24.647433352656662\n",
      "Epoch 992 / 1000 loss: 24.64684474375099\n",
      "Epoch 993 / 1000 loss: 24.646322699263692\n",
      "Epoch 994 / 1000 loss: 24.645853233523667\n",
      "Epoch 995 / 1000 loss: 24.645717927254736\n",
      "Epoch 996 / 1000 loss: 24.64536587893963\n",
      "Epoch 997 / 1000 loss: 24.644785035401583\n",
      "Epoch 998 / 1000 loss: 24.644891389645636\n",
      "Epoch 999 / 1000 loss: 24.644346735440195\n"
     ]
    }
   ],
   "source": [
    "# initialising stuff and starting the session\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# defining batch size, number of epochs and learning rate\n",
    "\n",
    "batch_size = 50  # how many images to use together for training\n",
    "hm_epochs =1000    # how many times to go through the entire dataset\n",
    "tot_images =  len(tdata.transpose()[0])# total number of images\n",
    "\n",
    "# running the model for a 1000 epochs taking 100 images in batches\n",
    "# total improvement is printed out after each epoch\n",
    "\n",
    "for epoch in range(hm_epochs):\n",
    "\n",
    "    epoch_loss = 0    # initializing error as 0\n",
    "\n",
    "    for i in range(int(tot_images/batch_size)):\n",
    "\n",
    "        epoch_x = tdata[ i*batch_size : (i+1)*batch_size ]\n",
    "\n",
    "        _, c = sess.run([optimizer, meansq],\\\n",
    "               feed_dict={input_layer: epoch_x, \\\n",
    "               output_true: epoch_x})\n",
    "\n",
    "        epoch_loss += c\n",
    "\n",
    "    print('Epoch', epoch, '/', hm_epochs, 'loss:',epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,)\n",
      "This is the weights of the hidden layer:\n",
      "[[ 0.10239866  0.4470244   0.3706759  ... -0.46308723 -0.14580439\n",
      "  -0.03778163]\n",
      " [-0.11594263  0.21886504 -0.41518387 ... -0.05295731 -0.5831738\n",
      "   0.393596  ]\n",
      " [ 0.9509254   0.11576141  0.7386869  ... -1.1432886   0.97309065\n",
      "   0.6085143 ]\n",
      " ...\n",
      " [-0.4978385  -0.25941956 -1.2866685  ...  0.39847556 -0.98149574\n",
      "   0.49990568]\n",
      " [ 0.09814221  0.20810133  1.032067   ... -0.8694392   0.44546327\n",
      "   0.92902774]\n",
      " [-1.2396507  -1.0283618   0.00473646 ...  0.21956551  0.63410616\n",
      "   0.21212213]]\n",
      "<tf.Variable 'Variable_4:0' shape=(32, 64) dtype=float32_ref>\n",
      "This is the original image:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the reconsructed image after running through the auto encoder:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pick any image\n",
    "\n",
    "any_image = tdata[10]\n",
    "print(any_image.shape)\n",
    "# run it though the autoencoder\n",
    "\n",
    "output_any_image = sess.run(output_layer,\\\n",
    "                   feed_dict={input_layer:[any_image]})\n",
    "\n",
    "# run it though just the encoder\n",
    "\n",
    "encoded_any_image = sess.run(layer_1,\\\n",
    "                   feed_dict={input_layer:[any_image]})\n",
    "\n",
    "print(\"This is the weights of the hidden layer:\")\n",
    "print(sess.run(output_layer_vals['weights']))\n",
    "print(output_layer_vals['weights'])\n",
    "# print the original image\n",
    "print(\"This is the original image:\")\n",
    "plt.imshow(np.reshape(any_image,(8,8)),  cmap='Greys')\n",
    "plt.show()\n",
    "print(\"This is the reconsructed image after running through the auto encoder:\")\n",
    "# print the encoding\n",
    "plt.imshow(np.reshape(output_any_image,(8,8)),  cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
