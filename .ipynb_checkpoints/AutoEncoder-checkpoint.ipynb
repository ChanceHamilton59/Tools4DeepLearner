{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingData\\TrainSetPatch_1.mat\n",
      "TrainingData\\TrainSetPatch_10.mat\n",
      "TrainingData\\TrainSetPatch_11.mat\n",
      "TrainingData\\TrainSetPatch_12.mat\n",
      "TrainingData\\TrainSetPatch_13.mat\n",
      "TrainingData\\TrainSetPatch_14.mat\n",
      "TrainingData\\TrainSetPatch_15.mat\n",
      "TrainingData\\TrainSetPatch_16.mat\n",
      "TrainingData\\TrainSetPatch_17.mat\n",
      "TrainingData\\TrainSetPatch_18.mat\n",
      "TrainingData\\TrainSetPatch_19.mat\n",
      "TrainingData\\TrainSetPatch_2.mat\n",
      "TrainingData\\TrainSetPatch_20.mat\n",
      "TrainingData\\TrainSetPatch_3.mat\n",
      "TrainingData\\TrainSetPatch_4.mat\n",
      "TrainingData\\TrainSetPatch_5.mat\n",
      "TrainingData\\TrainSetPatch_6.mat\n",
      "TrainingData\\TrainSetPatch_7.mat\n",
      "TrainingData\\TrainSetPatch_8.mat\n",
      "TrainingData\\TrainSetPatch_9.mat\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Getting the current work directory (cwd)\n",
    "thisdir = os.getcwd()\n",
    "TrainingData= []\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk('TrainingData'):\n",
    "    for file in f:\n",
    "        if \".mat\" in file:\n",
    "            print(os.path.join(r, file))\n",
    "            TrainingData.append(os.path.join(r, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 80000)\n",
      "80000\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(TrainingData)):\n",
    "    td = loadmat(TrainingData[i])['X']\n",
    "    if (i==0):\n",
    "        tdata = td\n",
    "    else :\n",
    "        tdata = np.concatenate((tdata, td), axis=1)\n",
    "    \n",
    "print(tdata.shape)\n",
    "tdata = tdata.transpose()\n",
    "print(len(tdata.transpose()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deciding how many nodes wach layer should have\n",
    "\n",
    "n_nodes_inpl = 64  #encoder\n",
    "n_nodes_hl1  = 32  #encoder\n",
    "\n",
    "n_nodes_hl2  = 32  #decoder\n",
    "n_nodes_outl = 64  #decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weights': <tf.Variable 'Variable:0' shape=(64, 32) dtype=float32_ref>, 'biases': <tf.Variable 'Variable_1:0' shape=(32,) dtype=float32_ref>}\n"
     ]
    }
   ],
   "source": [
    "# first hidden layer has 784*32 weights and 32 biases\n",
    "\n",
    "hidden_1_layer_vals = {\n",
    "'weights':tf.Variable(tf.random_normal([n_nodes_inpl,n_nodes_hl1])),\n",
    "'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))  }\n",
    "print(hidden_1_layer_vals)\n",
    "\n",
    "# second hidden layer has 32*32 weights and 32 biases\n",
    "\n",
    "hidden_2_layer_vals = {\n",
    "'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))  }\n",
    "\n",
    "# second hidden layer has 32*784 weights and 784 biases\n",
    "\n",
    "output_layer_vals = {\n",
    "'weights':tf.Variable(tf.random_normal([n_nodes_hl2,n_nodes_outl])),\n",
    "'biases':tf.Variable(tf.random_normal([n_nodes_outl])) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image with shape 784 goes in\n",
    "input_layer = tf.placeholder('float', [None, 64])\n",
    "\n",
    "# multiply output of input_layer wth a weight matrix and add biases\n",
    "\n",
    "layer_1 = tf.contrib.layers.fully_connected(tf.matmul(input_layer,hidden_1_layer_vals['weights']),\n",
    "                                            32,\n",
    "                                            activation_fn=tf.nn.relu)\n",
    "\n",
    "# multiply output of layer_1 wth a weight matrix and add biases\n",
    "\n",
    "layer_2 = tf.contrib.layers.fully_connected(\n",
    "       tf.add(tf.matmul(layer_1,hidden_2_layer_vals['weights']),\n",
    "       hidden_2_layer_vals['biases']),32, activation_fn=tf.nn.relu)\n",
    "\n",
    "# multiply output of layer_2 wth a weight matrix and add biases\n",
    "\n",
    "output_layer = tf.matmul(layer_1,output_layer_vals['weights']) \n",
    "\n",
    "# output_true shall have the original image for error calculations\n",
    "\n",
    "output_true = tf.placeholder('float', [None, 64])\n",
    "\n",
    "# define our cost function\n",
    "meansq =    tf.reduce_mean(tf.square(output_layer - output_true))\n",
    "\n",
    "# define our optimizer\n",
    "learn_rate = 0.1   # how fast the model should learn\n",
    "optimizer = tf.train.AdagradOptimizer(learn_rate).minimize(meansq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 1000 loss: 61.700785647146404\n",
      "Epoch 1 / 1000 loss: 24.69725851994008\n",
      "Epoch 2 / 1000 loss: 24.672554188407958\n",
      "Epoch 3 / 1000 loss: 24.66496326494962\n",
      "Epoch 4 / 1000 loss: 24.661571566015482\n",
      "Epoch 5 / 1000 loss: 24.659794171340764\n",
      "Epoch 6 / 1000 loss: 24.658714020624757\n",
      "Epoch 7 / 1000 loss: 24.6580301951617\n",
      "Epoch 8 / 1000 loss: 24.657547054812312\n",
      "Epoch 9 / 1000 loss: 24.657211874611676\n",
      "Epoch 10 / 1000 loss: 24.656986837275326\n",
      "Epoch 11 / 1000 loss: 24.656809746287763\n",
      "Epoch 12 / 1000 loss: 24.65669601224363\n",
      "Epoch 13 / 1000 loss: 24.656585623510182\n",
      "Epoch 14 / 1000 loss: 24.65650273859501\n",
      "Epoch 15 / 1000 loss: 24.65642774477601\n",
      "Epoch 16 / 1000 loss: 24.65637374855578\n",
      "Epoch 17 / 1000 loss: 24.656322410330176\n",
      "Epoch 18 / 1000 loss: 24.656277955509722\n",
      "Epoch 19 / 1000 loss: 24.656234229914844\n",
      "Epoch 20 / 1000 loss: 24.656203873455524\n",
      "Epoch 21 / 1000 loss: 24.656176093034446\n",
      "Epoch 22 / 1000 loss: 24.65615008957684\n",
      "Epoch 23 / 1000 loss: 24.656124295666814\n",
      "Epoch 24 / 1000 loss: 24.65610346570611\n",
      "Epoch 25 / 1000 loss: 24.656087575480342\n",
      "Epoch 26 / 1000 loss: 24.65607372764498\n",
      "Epoch 27 / 1000 loss: 24.656060567125678\n",
      "Epoch 28 / 1000 loss: 24.656040942296386\n",
      "Epoch 29 / 1000 loss: 24.65602534171194\n",
      "Epoch 30 / 1000 loss: 24.65601098537445\n",
      "Epoch 31 / 1000 loss: 24.655999395996332\n"
     ]
    }
   ],
   "source": [
    "# initialising stuff and starting the session\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# defining batch size, number of epochs and learning rate\n",
    "\n",
    "batch_size = 50  # how many images to use together for training\n",
    "hm_epochs =1000    # how many times to go through the entire dataset\n",
    "tot_images =  len(tdata.transpose()[0])# total number of images\n",
    "\n",
    "# running the model for a 1000 epochs taking 100 images in batches\n",
    "# total improvement is printed out after each epoch\n",
    "\n",
    "for epoch in range(hm_epochs):\n",
    "\n",
    "    epoch_loss = 0    # initializing error as 0\n",
    "\n",
    "    for i in range(int(tot_images/batch_size)):\n",
    "\n",
    "        epoch_x = tdata[ i*batch_size : (i+1)*batch_size ]\n",
    "\n",
    "        _, c = sess.run([optimizer, meansq],\\\n",
    "               feed_dict={input_layer: epoch_x, \\\n",
    "               output_true: epoch_x})\n",
    "\n",
    "        epoch_loss += c\n",
    "\n",
    "    print('Epoch', epoch, '/', hm_epochs, 'loss:',epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick any image\n",
    "\n",
    "any_image = all_images[999]\n",
    "\n",
    "# run it though the autoencoder\n",
    "\n",
    "output_any_image = sess.run(output_layer,\\\n",
    "                   feed_dict={input_layer:[any_image]})\n",
    "\n",
    "# run it though just the encoder\n",
    "\n",
    "encoded_any_image = sess.run(layer_1,\\\n",
    "                   feed_dict={input_layer:[any_image]})\n",
    "\n",
    "# print the original image\n",
    "\n",
    "plt.imshow(any_image(28,28),  cmap='Greys')\n",
    "plt.show()\n",
    "\n",
    "# print the encoding\n",
    "\n",
    "print(encoded_any_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
