{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingData\\TrainSetPatch_1.mat\n",
      "TrainingData\\TrainSetPatch_10.mat\n",
      "TrainingData\\TrainSetPatch_11.mat\n",
      "TrainingData\\TrainSetPatch_12.mat\n",
      "TrainingData\\TrainSetPatch_13.mat\n",
      "TrainingData\\TrainSetPatch_14.mat\n",
      "TrainingData\\TrainSetPatch_15.mat\n",
      "TrainingData\\TrainSetPatch_16.mat\n",
      "TrainingData\\TrainSetPatch_17.mat\n",
      "TrainingData\\TrainSetPatch_18.mat\n",
      "TrainingData\\TrainSetPatch_19.mat\n",
      "TrainingData\\TrainSetPatch_2.mat\n",
      "TrainingData\\TrainSetPatch_20.mat\n",
      "TrainingData\\TrainSetPatch_3.mat\n",
      "TrainingData\\TrainSetPatch_4.mat\n",
      "TrainingData\\TrainSetPatch_5.mat\n",
      "TrainingData\\TrainSetPatch_6.mat\n",
      "TrainingData\\TrainSetPatch_7.mat\n",
      "TrainingData\\TrainSetPatch_8.mat\n",
      "TrainingData\\TrainSetPatch_9.mat\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Getting the current work directory (cwd)\n",
    "thisdir = os.getcwd()\n",
    "TrainingData= []\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk('TrainingData'):\n",
    "    for file in f:\n",
    "        if \".mat\" in file:\n",
    "            print(os.path.join(r, file))\n",
    "            TrainingData.append(os.path.join(r, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 80000)\n",
      "80000\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(TrainingData)):\n",
    "    td = loadmat(TrainingData[i])['X']\n",
    "    if (i==0):\n",
    "        tdata = td\n",
    "    else :\n",
    "        tdata = np.concatenate((tdata, td), axis=1)\n",
    "    \n",
    "print(tdata.shape)\n",
    "tdata = tdata.transpose()\n",
    "print(len(tdata.transpose()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deciding how many nodes wach layer should have\n",
    "\n",
    "n_nodes_inpl = 64  #encoder\n",
    "n_nodes_hl1  = 32  #encoder\n",
    "\n",
    "n_nodes_hl2  = 32  #decoder\n",
    "n_nodes_outl = 64  #decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weights': <tf.Variable 'Variable_12:0' shape=(64, 32) dtype=float32_ref>, 'biases': <tf.Variable 'Variable_13:0' shape=(32,) dtype=float32_ref>}\n"
     ]
    }
   ],
   "source": [
    "# first hidden layer has 784*32 weights and 32 biases\n",
    "\n",
    "hidden_1_layer_vals = {\n",
    "'weights':tf.Variable(tf.random_normal([n_nodes_inpl,n_nodes_hl1])),\n",
    "'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))  }\n",
    "print(hidden_1_layer_vals)\n",
    "\n",
    "# second hidden layer has 32*32 weights and 32 biases\n",
    "\n",
    "hidden_2_layer_vals = {\n",
    "'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))  }\n",
    "\n",
    "# second hidden layer has 32*784 weights and 784 biases\n",
    "\n",
    "output_layer_vals = {\n",
    "'weights':tf.Variable(tf.random_normal([n_nodes_hl2,n_nodes_outl])),\n",
    "'biases':tf.Variable(tf.random_normal([n_nodes_outl])) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# image with shape 784 goes in\n",
    "input_layer = tf.placeholder('float', [None, 64])\n",
    "\n",
    "# multiply output of input_layer wth a weight matrix and add biases\n",
    "\n",
    "layer_1 = tf.contrib.layers.fully_connected(tf.matmul(input_layer,hidden_1_layer_vals['weights']),\n",
    "                                            32,\n",
    "                                            activation_fn=tf.nn.relu)\n",
    "\n",
    "# multiply output of layer_1 wth a weight matrix and add biases\n",
    "\n",
    "layer_2 = tf.contrib.layers.fully_connected(\n",
    "       tf.add(tf.matmul(layer_1,hidden_2_layer_vals['weights']),\n",
    "       hidden_2_layer_vals['biases']),32, activation_fn=tf.nn.relu)\n",
    "\n",
    "# multiply output of layer_2 wth a weight matrix and add biases\n",
    "\n",
    "output_layer = tf.matmul(layer_1,output_layer_vals['weights']) \n",
    "\n",
    "# output_true shall have the original image for error calculations\n",
    "\n",
    "output_true = tf.placeholder('float', [None, 64])\n",
    "\n",
    "# define our cost function\n",
    "meansq =    tf.reduce_mean(tf.square(output_layer - output_true))\n",
    "\n",
    "# define our optimizer\n",
    "learn_rate = 0.1   # how fast the model should learn\n",
    "optimizer = tf.train.AdagradOptimizer(learn_rate).minimize(meansq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 1000 loss: 369.0336474389769\n",
      "Epoch 1 / 1000 loss: 246.6332360333763\n",
      "Epoch 2 / 1000 loss: 246.59716539876536\n",
      "Epoch 3 / 1000 loss: 246.58973107067868\n",
      "Epoch 4 / 1000 loss: 246.5873072934337\n",
      "Epoch 5 / 1000 loss: 246.5867451936938\n",
      "Epoch 6 / 1000 loss: 246.5865256083198\n",
      "Epoch 7 / 1000 loss: 246.58641373412684\n",
      "Epoch 8 / 1000 loss: 246.5863606161438\n",
      "Epoch 9 / 1000 loss: 246.58633316727355\n",
      "Epoch 10 / 1000 loss: 246.58631753874943\n",
      "Epoch 11 / 1000 loss: 246.58630432141945\n",
      "Epoch 12 / 1000 loss: 246.5863012331538\n",
      "Epoch 13 / 1000 loss: 246.5862911255099\n",
      "Epoch 14 / 1000 loss: 246.58628601161763\n",
      "Epoch 15 / 1000 loss: 246.58628255175427\n",
      "Epoch 16 / 1000 loss: 246.58628186164424\n",
      "Epoch 17 / 1000 loss: 246.58627906208858\n",
      "Epoch 18 / 1000 loss: 246.58627708861604\n",
      "Epoch 19 / 1000 loss: 246.58627562923357\n",
      "Epoch 20 / 1000 loss: 246.58627456473187\n",
      "Epoch 21 / 1000 loss: 246.5862734154798\n",
      "Epoch 22 / 1000 loss: 246.58627250464633\n",
      "Epoch 23 / 1000 loss: 246.5862716450356\n",
      "Epoch 24 / 1000 loss: 246.586270860862\n",
      "Epoch 25 / 1000 loss: 246.58627015398815\n",
      "Epoch 26 / 1000 loss: 246.5862694424577\n",
      "Epoch 27 / 1000 loss: 246.5862687570043\n",
      "Epoch 28 / 1000 loss: 246.58626809017733\n",
      "Epoch 29 / 1000 loss: 246.58626744011417\n",
      "Epoch 30 / 1000 loss: 246.58626679191366\n",
      "Epoch 31 / 1000 loss: 246.58626615302637\n",
      "Epoch 32 / 1000 loss: 246.58626551786438\n",
      "Epoch 33 / 1000 loss: 246.58626489015296\n",
      "Epoch 34 / 1000 loss: 246.58626425964758\n",
      "Epoch 35 / 1000 loss: 246.58626363100484\n",
      "Epoch 36 / 1000 loss: 246.58626300701872\n",
      "Epoch 37 / 1000 loss: 246.58626238116995\n",
      "Epoch 38 / 1000 loss: 246.58626173762605\n",
      "Epoch 39 / 1000 loss: 246.58626111177728\n",
      "Epoch 40 / 1000 loss: 246.5862605064176\n",
      "Epoch 41 / 1000 loss: 246.5862598712556\n",
      "Epoch 42 / 1000 loss: 246.5862592398189\n",
      "Epoch 43 / 1000 loss: 246.58625861862674\n",
      "Epoch 44 / 1000 loss: 246.58625798625872\n",
      "Epoch 45 / 1000 loss: 246.58625735947862\n",
      "Epoch 46 / 1000 loss: 246.58625674480572\n",
      "Epoch 47 / 1000 loss: 246.58625612733886\n",
      "Epoch 48 / 1000 loss: 246.58625550428405\n",
      "Epoch 49 / 1000 loss: 246.58625486446545\n",
      "Epoch 50 / 1000 loss: 246.58625423023477\n",
      "Epoch 51 / 1000 loss: 246.58625361369923\n",
      "Epoch 52 / 1000 loss: 246.58625298785046\n",
      "Epoch 53 / 1000 loss: 246.5862523666583\n",
      "Epoch 54 / 1000 loss: 246.58625173801556\n",
      "Epoch 55 / 1000 loss: 246.58625110657886\n",
      "Epoch 56 / 1000 loss: 246.58625048445538\n",
      "Epoch 57 / 1000 loss: 246.5862498502247\n",
      "Epoch 58 / 1000 loss: 246.5862492225133\n",
      "Epoch 59 / 1000 loss: 246.5862485873513\n",
      "Epoch 60 / 1000 loss: 246.5862479689531\n",
      "Epoch 61 / 1000 loss: 246.58624736173078\n",
      "Epoch 62 / 1000 loss: 246.58624671818689\n",
      "Epoch 63 / 1000 loss: 246.58624610723928\n",
      "Epoch 64 / 1000 loss: 246.5862454609014\n",
      "Epoch 65 / 1000 loss: 246.5862448583357\n",
      "Epoch 66 / 1000 loss: 246.58624423248693\n",
      "Epoch 67 / 1000 loss: 246.58624358987436\n",
      "Epoch 68 / 1000 loss: 246.58624296681955\n",
      "Epoch 69 / 1000 loss: 246.5862423456274\n",
      "Epoch 70 / 1000 loss: 246.58624172070995\n",
      "Epoch 71 / 1000 loss: 246.58624109299853\n",
      "Epoch 72 / 1000 loss: 246.58624047366902\n",
      "Epoch 73 / 1000 loss: 246.5862398338504\n",
      "Epoch 74 / 1000 loss: 246.58623920893297\n",
      "Epoch 75 / 1000 loss: 246.58623857563362\n",
      "Epoch 76 / 1000 loss: 246.58623795444146\n",
      "Epoch 77 / 1000 loss: 246.58623732579872\n",
      "Epoch 78 / 1000 loss: 246.58623669249937\n",
      "Epoch 79 / 1000 loss: 246.58623607316986\n",
      "Epoch 80 / 1000 loss: 246.58623545384035\n",
      "Epoch 81 / 1000 loss: 246.58623481588438\n",
      "Epoch 82 / 1000 loss: 246.58623418072239\n",
      "Epoch 83 / 1000 loss: 246.58623357163742\n",
      "Epoch 84 / 1000 loss: 246.58623292623088\n",
      "Epoch 85 / 1000 loss: 246.58623230317608\n",
      "Epoch 86 / 1000 loss: 246.58623166615143\n",
      "Epoch 87 / 1000 loss: 246.58623103937134\n",
      "Epoch 88 / 1000 loss: 246.5862304070033\n",
      "Epoch 89 / 1000 loss: 246.58622980536893\n",
      "Epoch 90 / 1000 loss: 246.58622918231413\n",
      "Epoch 91 / 1000 loss: 246.58622853597626\n",
      "Epoch 92 / 1000 loss: 246.58622790826485\n",
      "Epoch 93 / 1000 loss: 246.58622728055343\n",
      "Epoch 94 / 1000 loss: 246.58622666122392\n",
      "Epoch 95 / 1000 loss: 246.58622602606192\n",
      "Epoch 96 / 1000 loss: 246.58622539276257\n",
      "Epoch 97 / 1000 loss: 246.5862247752957\n",
      "Epoch 98 / 1000 loss: 246.58622414292768\n",
      "Epoch 99 / 1000 loss: 246.58622351428494\n",
      "Epoch 100 / 1000 loss: 246.58622290519997\n",
      "Epoch 101 / 1000 loss: 246.58622225513682\n",
      "Epoch 102 / 1000 loss: 246.58622161252424\n",
      "Epoch 103 / 1000 loss: 246.58622099785134\n",
      "Epoch 104 / 1000 loss: 246.586220364552\n",
      "Epoch 105 / 1000 loss: 246.58621974335983\n",
      "Epoch 106 / 1000 loss: 246.58621912123635\n",
      "Epoch 107 / 1000 loss: 246.58621848886833\n",
      "Epoch 108 / 1000 loss: 246.58621785463765\n",
      "Epoch 109 / 1000 loss: 246.58621723903343\n",
      "Epoch 110 / 1000 loss: 246.58621660014614\n",
      "Epoch 111 / 1000 loss: 246.58621597429737\n",
      "Epoch 112 / 1000 loss: 246.5862153354101\n",
      "Epoch 113 / 1000 loss: 246.58621471421793\n",
      "Epoch 114 / 1000 loss: 246.5862140939571\n",
      "Epoch 115 / 1000 loss: 246.58621346531436\n",
      "Epoch 116 / 1000 loss: 246.5862128236331\n",
      "Epoch 117 / 1000 loss: 246.58621218940243\n",
      "Epoch 118 / 1000 loss: 246.5862115523778\n",
      "Epoch 119 / 1000 loss: 246.58621093211696\n",
      "Epoch 120 / 1000 loss: 246.58621030161157\n",
      "Epoch 121 / 1000 loss: 246.58620968973264\n",
      "Epoch 122 / 1000 loss: 246.586209052708\n",
      "Epoch 123 / 1000 loss: 246.58620841940865\n",
      "Epoch 124 / 1000 loss: 246.58620778890327\n",
      "Epoch 125 / 1000 loss: 246.58620715374127\n",
      "Epoch 126 / 1000 loss: 246.5862065232359\n",
      "Epoch 127 / 1000 loss: 246.58620589552447\n",
      "Epoch 128 / 1000 loss: 246.586205273401\n",
      "Epoch 129 / 1000 loss: 246.5862046428956\n",
      "Epoch 130 / 1000 loss: 246.5862040198408\n",
      "Epoch 131 / 1000 loss: 246.58620338188484\n",
      "Epoch 132 / 1000 loss: 246.58620275044814\n",
      "Epoch 133 / 1000 loss: 246.58620212739334\n",
      "Epoch 134 / 1000 loss: 246.5862014782615\n",
      "Epoch 135 / 1000 loss: 246.58620087476447\n",
      "Epoch 136 / 1000 loss: 246.58620022935793\n",
      "Epoch 137 / 1000 loss: 246.58619959605858\n",
      "Epoch 138 / 1000 loss: 246.5861989776604\n",
      "Epoch 139 / 1000 loss: 246.58619834529236\n",
      "Epoch 140 / 1000 loss: 246.58619769709185\n",
      "Epoch 141 / 1000 loss: 246.58619708800688\n",
      "Epoch 142 / 1000 loss: 246.58619644818828\n",
      "Epoch 143 / 1000 loss: 246.58619581488892\n",
      "Epoch 144 / 1000 loss: 246.58619518904015\n",
      "Epoch 145 / 1000 loss: 246.58619455387816\n",
      "Epoch 146 / 1000 loss: 246.5861939289607\n",
      "Epoch 147 / 1000 loss: 246.586193297524\n",
      "Epoch 148 / 1000 loss: 246.58619267167524\n",
      "Epoch 149 / 1000 loss: 246.5861920393072\n",
      "Epoch 150 / 1000 loss: 246.5861914032139\n",
      "Epoch 151 / 1000 loss: 246.58619077177718\n",
      "Epoch 152 / 1000 loss: 246.5861901366152\n",
      "Epoch 153 / 1000 loss: 246.5861895098351\n",
      "Epoch 154 / 1000 loss: 246.5861888793297\n",
      "Epoch 155 / 1000 loss: 246.58618824975565\n",
      "Epoch 156 / 1000 loss: 246.58618762297556\n",
      "Epoch 157 / 1000 loss: 246.58618699619547\n",
      "Epoch 158 / 1000 loss: 246.58618636569008\n",
      "Epoch 159 / 1000 loss: 246.5861857230775\n",
      "Epoch 160 / 1000 loss: 246.58618508605286\n",
      "Epoch 161 / 1000 loss: 246.58618444995955\n",
      "Epoch 162 / 1000 loss: 246.58618381759152\n",
      "Epoch 163 / 1000 loss: 246.58618319639936\n",
      "Epoch 164 / 1000 loss: 246.58618256775662\n",
      "Epoch 165 / 1000 loss: 246.5861819186248\n",
      "Epoch 166 / 1000 loss: 246.58618129277602\n",
      "Epoch 167 / 1000 loss: 246.5861806771718\n",
      "Epoch 168 / 1000 loss: 246.58618004480377\n",
      "Epoch 169 / 1000 loss: 246.58617940777913\n",
      "Epoch 170 / 1000 loss: 246.5861787707545\n",
      "Epoch 171 / 1000 loss: 246.58617812534794\n",
      "Epoch 172 / 1000 loss: 246.58617750694975\n",
      "Epoch 173 / 1000 loss: 246.5861768773757\n",
      "Epoch 174 / 1000 loss: 246.58617623941973\n",
      "Epoch 175 / 1000 loss: 246.5861756117083\n",
      "Epoch 176 / 1000 loss: 246.58617499051616\n",
      "Epoch 177 / 1000 loss: 246.58617435162887\n",
      "Epoch 178 / 1000 loss: 246.58617371553555\n",
      "Epoch 179 / 1000 loss: 246.5861730906181\n",
      "Epoch 180 / 1000 loss: 246.5861724591814\n",
      "Epoch 181 / 1000 loss: 246.5861718156375\n",
      "Epoch 182 / 1000 loss: 246.5861711972393\n",
      "Epoch 183 / 1000 loss: 246.58617053972557\n",
      "Epoch 184 / 1000 loss: 246.5861699101515\n",
      "Epoch 185 / 1000 loss: 246.5861692870967\n",
      "Epoch 186 / 1000 loss: 246.58616865752265\n",
      "Epoch 187 / 1000 loss: 246.58616803353652\n",
      "Epoch 188 / 1000 loss: 246.5861673974432\n",
      "Epoch 189 / 1000 loss: 246.58616675669327\n",
      "Epoch 190 / 1000 loss: 246.58616611221805\n",
      "Epoch 191 / 1000 loss: 246.58616548450664\n",
      "Epoch 192 / 1000 loss: 246.58616487821564\n",
      "Epoch 193 / 1000 loss: 246.58616421883926\n",
      "Epoch 194 / 1000 loss: 246.58616359205917\n",
      "Epoch 195 / 1000 loss: 246.5861629624851\n",
      "Epoch 196 / 1000 loss: 246.58616231521592\n",
      "Epoch 197 / 1000 loss: 246.58616170985624\n",
      "Epoch 198 / 1000 loss: 246.58616106538102\n",
      "Epoch 199 / 1000 loss: 246.58616041438654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200 / 1000 loss: 246.58615979971364\n",
      "Epoch 201 / 1000 loss: 246.58615917013958\n",
      "Epoch 202 / 1000 loss: 246.58615852845833\n",
      "Epoch 203 / 1000 loss: 246.5861578718759\n",
      "Epoch 204 / 1000 loss: 246.58615725906566\n",
      "Epoch 205 / 1000 loss: 246.5861566257663\n",
      "Epoch 206 / 1000 loss: 246.58615600457415\n",
      "Epoch 207 / 1000 loss: 246.5861553750001\n",
      "Epoch 208 / 1000 loss: 246.58615474076942\n",
      "Epoch 209 / 1000 loss: 246.58615410560742\n",
      "Epoch 210 / 1000 loss: 246.5861534611322\n",
      "Epoch 211 / 1000 loss: 246.58615283248946\n",
      "Epoch 212 / 1000 loss: 246.5861521861516\n",
      "Epoch 213 / 1000 loss: 246.58615155378357\n",
      "Epoch 214 / 1000 loss: 246.5861509195529\n",
      "Epoch 215 / 1000 loss: 246.5861502881162\n",
      "Epoch 216 / 1000 loss: 246.58614986529574\n",
      "Epoch 217 / 1000 loss: 246.58613966824487\n",
      "Epoch 218 / 1000 loss: 246.5861386959441\n",
      "Epoch 219 / 1000 loss: 246.58613795181736\n",
      "Epoch 220 / 1000 loss: 246.586137260776\n",
      "Epoch 221 / 1000 loss: 246.58613659674302\n",
      "Epoch 222 / 1000 loss: 246.5861359569244\n",
      "Epoch 223 / 1000 loss: 246.5861352928914\n",
      "Epoch 224 / 1000 loss: 246.5861346484162\n",
      "Epoch 225 / 1000 loss: 246.58613401232287\n",
      "Epoch 226 / 1000 loss: 246.586133365985\n",
      "Epoch 227 / 1000 loss: 246.58613272896037\n",
      "Epoch 228 / 1000 loss: 246.5861320826225\n",
      "Epoch 229 / 1000 loss: 246.58613146515563\n",
      "Epoch 230 / 1000 loss: 246.5861308076419\n",
      "Epoch 231 / 1000 loss: 246.5861301762052\n",
      "Epoch 232 / 1000 loss: 246.58612953172997\n",
      "Epoch 233 / 1000 loss: 246.586128893774\n",
      "Epoch 234 / 1000 loss: 246.5861282707192\n",
      "Epoch 235 / 1000 loss: 246.58612762158737\n",
      "Epoch 236 / 1000 loss: 246.5861270041205\n",
      "Epoch 237 / 1000 loss: 246.58612634288147\n",
      "Epoch 238 / 1000 loss: 246.58612571982667\n",
      "Epoch 239 / 1000 loss: 246.58612506603822\n",
      "Epoch 240 / 1000 loss: 246.58612441504374\n",
      "Epoch 241 / 1000 loss: 246.5861237780191\n",
      "Epoch 242 / 1000 loss: 246.5861231586896\n",
      "Epoch 243 / 1000 loss: 246.58612249186262\n",
      "Epoch 244 / 1000 loss: 246.5861218594946\n",
      "Epoch 245 / 1000 loss: 246.5861212289892\n",
      "Epoch 246 / 1000 loss: 246.5861206050031\n",
      "Epoch 247 / 1000 loss: 246.5861199623905\n",
      "Epoch 248 / 1000 loss: 246.58611937100068\n",
      "Epoch 249 / 1000 loss: 246.58611871814355\n",
      "Epoch 250 / 1000 loss: 246.5861180596985\n",
      "Epoch 251 / 1000 loss: 246.58611740684137\n",
      "Epoch 252 / 1000 loss: 246.58611676143482\n",
      "Epoch 253 / 1000 loss: 246.586116141174\n",
      "Epoch 254 / 1000 loss: 246.58611549204215\n",
      "Epoch 255 / 1000 loss: 246.5861148252152\n",
      "Epoch 256 / 1000 loss: 246.58611420402303\n",
      "Epoch 257 / 1000 loss: 246.58611357351765\n",
      "Epoch 258 / 1000 loss: 246.58611293369904\n",
      "Epoch 259 / 1000 loss: 246.58611229853705\n",
      "Epoch 260 / 1000 loss: 246.5861116736196\n",
      "Epoch 261 / 1000 loss: 246.58611101424322\n",
      "Epoch 262 / 1000 loss: 246.58611037628725\n",
      "Epoch 263 / 1000 loss: 246.58610972529277\n",
      "Epoch 264 / 1000 loss: 246.586109099444\n",
      "Epoch 265 / 1000 loss: 246.58610845869407\n",
      "Epoch 266 / 1000 loss: 246.58610781794414\n",
      "Epoch 267 / 1000 loss: 246.58610716788098\n",
      "Epoch 268 / 1000 loss: 246.58610653178766\n",
      "Epoch 269 / 1000 loss: 246.5861059022136\n",
      "Epoch 270 / 1000 loss: 246.58610524935648\n",
      "Epoch 271 / 1000 loss: 246.5861046030186\n",
      "Epoch 272 / 1000 loss: 246.58610396413133\n",
      "Epoch 273 / 1000 loss: 246.58610334759578\n",
      "Epoch 274 / 1000 loss: 246.5861027003266\n",
      "Epoch 275 / 1000 loss: 246.58610206050798\n",
      "Epoch 276 / 1000 loss: 246.58610142068937\n",
      "Epoch 277 / 1000 loss: 246.58610077155754\n",
      "Epoch 278 / 1000 loss: 246.5861001382582\n",
      "Epoch 279 / 1000 loss: 246.58609949192032\n",
      "Epoch 280 / 1000 loss: 246.58609884837642\n",
      "Epoch 281 / 1000 loss: 246.58609821135178\n",
      "Epoch 282 / 1000 loss: 246.58609758270904\n",
      "Epoch 283 / 1000 loss: 246.58609692705795\n",
      "Epoch 284 / 1000 loss: 246.586096289102\n",
      "Epoch 285 / 1000 loss: 246.5860956585966\n",
      "Epoch 286 / 1000 loss: 246.58609500946477\n",
      "Epoch 287 / 1000 loss: 246.586094383616\n",
      "Epoch 288 / 1000 loss: 246.5860937363468\n",
      "Epoch 289 / 1000 loss: 246.586093084421\n",
      "Epoch 290 / 1000 loss: 246.58609245950356\n",
      "Epoch 291 / 1000 loss: 246.5860918094404\n",
      "Epoch 292 / 1000 loss: 246.58609116775915\n",
      "Epoch 293 / 1000 loss: 246.58609054563567\n",
      "Epoch 294 / 1000 loss: 246.5860898909159\n",
      "Epoch 295 / 1000 loss: 246.58608925202861\n",
      "Epoch 296 / 1000 loss: 246.5860886038281\n",
      "Epoch 297 / 1000 loss: 246.5860879640095\n",
      "Epoch 298 / 1000 loss: 246.58608731394634\n",
      "Epoch 299 / 1000 loss: 246.5860866853036\n",
      "Epoch 300 / 1000 loss: 246.58608602033928\n",
      "Epoch 301 / 1000 loss: 246.58608538797125\n",
      "Epoch 302 / 1000 loss: 246.58608473697677\n",
      "Epoch 303 / 1000 loss: 246.58608411299065\n",
      "Epoch 304 / 1000 loss: 246.58608346572146\n",
      "Epoch 305 / 1000 loss: 246.58608282124624\n",
      "Epoch 306 / 1000 loss: 246.58608219074085\n",
      "Epoch 307 / 1000 loss: 246.58608153415844\n",
      "Epoch 308 / 1000 loss: 246.58608090365306\n",
      "Epoch 309 / 1000 loss: 246.58608023496345\n",
      "Epoch 310 / 1000 loss: 246.58607962774113\n",
      "Epoch 311 / 1000 loss: 246.58607897302136\n",
      "Epoch 312 / 1000 loss: 246.5860783266835\n",
      "Epoch 313 / 1000 loss: 246.586077675689\n",
      "Epoch 314 / 1000 loss: 246.5860770479776\n",
      "Epoch 315 / 1000 loss: 246.58607639884576\n",
      "Epoch 316 / 1000 loss: 246.5860757608898\n",
      "Epoch 317 / 1000 loss: 246.5860751136206\n",
      "Epoch 318 / 1000 loss: 246.58607446355745\n",
      "Epoch 319 / 1000 loss: 246.58607383770868\n",
      "Epoch 320 / 1000 loss: 246.58607320068404\n",
      "Epoch 321 / 1000 loss: 246.5860725552775\n",
      "Epoch 322 / 1000 loss: 246.58607190893963\n",
      "Epoch 323 / 1000 loss: 246.58607126632705\n",
      "Epoch 324 / 1000 loss: 246.58607061905786\n",
      "Epoch 325 / 1000 loss: 246.58606995968148\n",
      "Epoch 326 / 1000 loss: 246.58606931893155\n",
      "Epoch 327 / 1000 loss: 246.58606866979972\n",
      "Epoch 328 / 1000 loss: 246.5860680420883\n",
      "Epoch 329 / 1000 loss: 246.58606739575043\n",
      "Epoch 330 / 1000 loss: 246.58606676897034\n",
      "Epoch 331 / 1000 loss: 246.58606612170115\n",
      "Epoch 332 / 1000 loss: 246.58606547722593\n",
      "Epoch 333 / 1000 loss: 246.58606484392658\n",
      "Epoch 334 / 1000 loss: 246.58606419479474\n",
      "Epoch 335 / 1000 loss: 246.58606355590746\n",
      "Epoch 336 / 1000 loss: 246.586062902119\n",
      "Epoch 337 / 1000 loss: 246.58606224833056\n",
      "Epoch 338 / 1000 loss: 246.58606160851195\n",
      "Epoch 339 / 1000 loss: 246.58606096962467\n",
      "Epoch 340 / 1000 loss: 246.5860603195615\n",
      "Epoch 341 / 1000 loss: 246.5860596750863\n",
      "Epoch 342 / 1000 loss: 246.58605903340504\n",
      "Epoch 343 / 1000 loss: 246.58605839451775\n",
      "Epoch 344 / 1000 loss: 246.5860577528365\n",
      "Epoch 345 / 1000 loss: 246.58605710836127\n",
      "Epoch 346 / 1000 loss: 246.58605646761134\n",
      "Epoch 347 / 1000 loss: 246.58605581102893\n",
      "Epoch 348 / 1000 loss: 246.58605516841635\n",
      "Epoch 349 / 1000 loss: 246.58605452952906\n",
      "Epoch 350 / 1000 loss: 246.58605388505384\n"
     ]
    }
   ],
   "source": [
    "# initialising stuff and starting the session\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# defining batch size, number of epochs and learning rate\n",
    "\n",
    "batch_size = 5  # how many images to use together for training\n",
    "hm_epochs =1000    # how many times to go through the entire dataset\n",
    "tot_images =  len(tdata.transpose()[0])# total number of images\n",
    "\n",
    "# running the model for a 1000 epochs taking 100 images in batches\n",
    "# total improvement is printed out after each epoch\n",
    "\n",
    "for epoch in range(hm_epochs):\n",
    "\n",
    "    epoch_loss = 0    # initializing error as 0\n",
    "\n",
    "    for i in range(int(tot_images/batch_size)):\n",
    "\n",
    "        epoch_x = tdata[ i*batch_size : (i+1)*batch_size ]\n",
    "\n",
    "        _, c = sess.run([optimizer, meansq],\\\n",
    "               feed_dict={input_layer: epoch_x, \\\n",
    "               output_true: epoch_x})\n",
    "\n",
    "        epoch_loss += c\n",
    "\n",
    "    print('Epoch', epoch, '/', hm_epochs, 'loss:',epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pick any image\n",
    "\n",
    "any_image = all_images[999]\n",
    "\n",
    "# run it though the autoencoder\n",
    "\n",
    "output_any_image = sess.run(output_layer,\\\n",
    "                   feed_dict={input_layer:[any_image]})\n",
    "\n",
    "# run it though just the encoder\n",
    "\n",
    "encoded_any_image = sess.run(layer_1,\\\n",
    "                   feed_dict={input_layer:[any_image]})\n",
    "\n",
    "# print the original image\n",
    "\n",
    "plt.imshow(any_image(28,28),  cmap='Greys')\n",
    "plt.show()\n",
    "\n",
    "# print the encoding\n",
    "\n",
    "print(encoded_any_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
