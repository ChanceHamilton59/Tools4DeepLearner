{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingData\\TrainSetPatch_1.mat\n",
      "TrainingData\\TrainSetPatch_10.mat\n",
      "TrainingData\\TrainSetPatch_11.mat\n",
      "TrainingData\\TrainSetPatch_12.mat\n",
      "TrainingData\\TrainSetPatch_13.mat\n",
      "TrainingData\\TrainSetPatch_14.mat\n",
      "TrainingData\\TrainSetPatch_15.mat\n",
      "TrainingData\\TrainSetPatch_16.mat\n",
      "TrainingData\\TrainSetPatch_17.mat\n",
      "TrainingData\\TrainSetPatch_18.mat\n",
      "TrainingData\\TrainSetPatch_19.mat\n",
      "TrainingData\\TrainSetPatch_2.mat\n",
      "TrainingData\\TrainSetPatch_20.mat\n",
      "TrainingData\\TrainSetPatch_3.mat\n",
      "TrainingData\\TrainSetPatch_4.mat\n",
      "TrainingData\\TrainSetPatch_5.mat\n",
      "TrainingData\\TrainSetPatch_6.mat\n",
      "TrainingData\\TrainSetPatch_7.mat\n",
      "TrainingData\\TrainSetPatch_8.mat\n",
      "TrainingData\\TrainSetPatch_9.mat\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Getting the current work directory (cwd)\n",
    "thisdir = os.getcwd()\n",
    "TrainingData= []\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk('TrainingData'):\n",
    "    for file in f:\n",
    "        if \".mat\" in file:\n",
    "            print(os.path.join(r, file))\n",
    "            TrainingData.append(os.path.join(r, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 80000)\n",
      "80000\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(TrainingData)):\n",
    "    td = loadmat(TrainingData[i])['X']\n",
    "    if (i==0):\n",
    "        tdata = td\n",
    "    else :\n",
    "        tdata = np.concatenate((tdata, td), axis=1)\n",
    "    \n",
    "print(tdata.shape)\n",
    "tdata = tdata.transpose()\n",
    "print(len(tdata.transpose()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deciding how many nodes wach layer should have\n",
    "\n",
    "n_nodes_inpl = 64  #encoder\n",
    "n_nodes_hl1  = 32  #encoder\n",
    "\n",
    "n_nodes_hl2  = 32  #decoder\n",
    "n_nodes_outl = 64  #decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weights': <tf.Variable 'Variable:0' shape=(64, 32) dtype=float32_ref>, 'biases': <tf.Variable 'Variable_1:0' shape=(32,) dtype=float32_ref>}\n"
     ]
    }
   ],
   "source": [
    "# first hidden layer has 64*32 weights and 32 biases\n",
    "\n",
    "hidden_1_layer_vals = {\n",
    "'weights':tf.Variable(tf.random_normal([n_nodes_inpl,n_nodes_hl1])),\n",
    "'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))  }\n",
    "print(hidden_1_layer_vals)\n",
    "\n",
    "# second hidden layer has 32*32 weights and 32 biases\n",
    "\n",
    "hidden_2_layer_vals = {\n",
    "'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))  }\n",
    "\n",
    "# second hidden layer has 32*784 weights and 784 biases\n",
    "\n",
    "output_layer_vals = {\n",
    "'weights':tf.Variable(tf.random_normal([n_nodes_hl2,n_nodes_outl])),\n",
    "'biases':tf.Variable(tf.random_normal([n_nodes_outl])) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# image with shape 784 goes in\n",
    "input_layer = tf.placeholder('float', [None, 64])\n",
    "\n",
    "# multiply output of input_layer wth a weight matrix and add biases\n",
    "\n",
    "layer_1 = tf.contrib.layers.fully_connected(tf.matmul(input_layer,hidden_1_layer_vals['weights']),\n",
    "                                            32,\n",
    "                                            activation_fn=tf.nn.relu)\n",
    "\n",
    "# multiply output of layer_1 wth a weight matrix and add biases\n",
    "\n",
    "layer_2 = tf.contrib.layers.fully_connected(\n",
    "       tf.add(tf.matmul(layer_1,hidden_2_layer_vals['weights']),\n",
    "       hidden_2_layer_vals['biases']),32, activation_fn=tf.nn.relu)\n",
    "\n",
    "# multiply output of layer_2 wth a weight matrix and add biases\n",
    "\n",
    "output_layer = tf.matmul(layer_1,output_layer_vals['weights']) \n",
    "\n",
    "# output_true shall have the original image for error calculations\n",
    "\n",
    "output_true = tf.placeholder('float', [None, 64])\n",
    "\n",
    "# define our cost function\n",
    "meansq =    tf.reduce_mean(tf.square(output_layer - output_true))\n",
    "\n",
    "# define our optimizer\n",
    "learn_rate = 0.1   # how fast the model should learn\n",
    "optimizer = tf.train.AdagradOptimizer(learn_rate).minimize(meansq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 1000 loss: 33.0531020332\n",
      "Epoch 1 / 1000 loss: 2.57701327559\n",
      "Epoch 2 / 1000 loss: 2.50986067392\n",
      "Epoch 3 / 1000 loss: 2.49045878835\n",
      "Epoch 4 / 1000 loss: 2.48183400277\n",
      "Epoch 5 / 1000 loss: 2.47716243006\n",
      "Epoch 6 / 1000 loss: 2.4743047189\n",
      "Epoch 7 / 1000 loss: 2.47243312839\n",
      "Epoch 8 / 1000 loss: 2.47113559954\n",
      "Epoch 9 / 1000 loss: 2.47019128967\n",
      "Epoch 10 / 1000 loss: 2.469486624\n",
      "Epoch 11 / 1000 loss: 2.46894389577\n",
      "Epoch 12 / 1000 loss: 2.46851201542\n",
      "Epoch 13 / 1000 loss: 2.46816143394\n",
      "Epoch 14 / 1000 loss: 2.46787037794\n",
      "Epoch 15 / 1000 loss: 2.46762892883\n",
      "Epoch 16 / 1000 loss: 2.46742583625\n",
      "Epoch 17 / 1000 loss: 2.46725263074\n",
      "Epoch 18 / 1000 loss: 2.46710237488\n",
      "Epoch 19 / 1000 loss: 2.46697201673\n",
      "Epoch 20 / 1000 loss: 2.46685803868\n",
      "Epoch 21 / 1000 loss: 2.46675757412\n",
      "Epoch 22 / 1000 loss: 2.46666874085\n",
      "Epoch 23 / 1000 loss: 2.46658795793\n",
      "Epoch 24 / 1000 loss: 2.46651588567\n",
      "Epoch 25 / 1000 loss: 2.46645113546\n",
      "Epoch 26 / 1000 loss: 2.46639174968\n",
      "Epoch 27 / 1000 loss: 2.46633727755\n",
      "Epoch 28 / 1000 loss: 2.46628723666\n",
      "Epoch 29 / 1000 loss: 2.46624104772\n",
      "Epoch 30 / 1000 loss: 2.46619834192\n",
      "Epoch 31 / 1000 loss: 2.46615856886\n",
      "Epoch 32 / 1000 loss: 2.46612174157\n",
      "Epoch 33 / 1000 loss: 2.46608642023\n",
      "Epoch 34 / 1000 loss: 2.46605103184\n",
      "Epoch 35 / 1000 loss: 2.46601863764\n",
      "Epoch 36 / 1000 loss: 2.4659880111\n",
      "Epoch 37 / 1000 loss: 2.4659603117\n",
      "Epoch 38 / 1000 loss: 2.46593424957\n",
      "Epoch 39 / 1000 loss: 2.46590940841\n",
      "Epoch 40 / 1000 loss: 2.4658844946\n",
      "Epoch 41 / 1000 loss: 2.46585852932\n",
      "Epoch 42 / 1000 loss: 2.46583509352\n",
      "Epoch 43 / 1000 loss: 2.46581337228\n",
      "Epoch 44 / 1000 loss: 2.46579281427\n",
      "Epoch 45 / 1000 loss: 2.46577277407\n",
      "Epoch 46 / 1000 loss: 2.46575348452\n",
      "Epoch 47 / 1000 loss: 2.46573556401\n",
      "Epoch 48 / 1000 loss: 2.46571883839\n",
      "Epoch 49 / 1000 loss: 2.46570316982\n",
      "Epoch 50 / 1000 loss: 2.46568829007\n",
      "Epoch 51 / 1000 loss: 2.46567367483\n",
      "Epoch 52 / 1000 loss: 2.46566022094\n",
      "Epoch 53 / 1000 loss: 2.46564729884\n",
      "Epoch 54 / 1000 loss: 2.46563465428\n",
      "Epoch 55 / 1000 loss: 2.46562235989\n",
      "Epoch 56 / 1000 loss: 2.46560949832\n",
      "Epoch 57 / 1000 loss: 2.46559695713\n",
      "Epoch 58 / 1000 loss: 2.46558512934\n",
      "Epoch 59 / 1000 loss: 2.46557409782\n",
      "Epoch 60 / 1000 loss: 2.46556404233\n",
      "Epoch 61 / 1000 loss: 2.46555461362\n",
      "Epoch 62 / 1000 loss: 2.4655453926\n",
      "Epoch 63 / 1000 loss: 2.46553628705\n",
      "Epoch 64 / 1000 loss: 2.46552760992\n",
      "Epoch 65 / 1000 loss: 2.46551913116\n",
      "Epoch 66 / 1000 loss: 2.46551036835\n",
      "Epoch 67 / 1000 loss: 2.46550165024\n",
      "Epoch 68 / 1000 loss: 2.46549384855\n",
      "Epoch 69 / 1000 loss: 2.46548645571\n",
      "Epoch 70 / 1000 loss: 2.46547885332\n",
      "Epoch 71 / 1000 loss: 2.46547123417\n",
      "Epoch 72 / 1000 loss: 2.46546407416\n",
      "Epoch 73 / 1000 loss: 2.46545704268\n",
      "Epoch 74 / 1000 loss: 2.4654497439\n",
      "Epoch 75 / 1000 loss: 2.46544306166\n",
      "Epoch 76 / 1000 loss: 2.46543676872\n",
      "Epoch 77 / 1000 loss: 2.46543070674\n",
      "Epoch 78 / 1000 loss: 2.4654246578\n",
      "Epoch 79 / 1000 loss: 2.46541856043\n",
      "Epoch 80 / 1000 loss: 2.46541282535\n",
      "Epoch 81 / 1000 loss: 2.46540745161\n",
      "Epoch 82 / 1000 loss: 2.4654021496\n",
      "Epoch 83 / 1000 loss: 2.46539718285\n",
      "Epoch 84 / 1000 loss: 2.46539196279\n",
      "Epoch 85 / 1000 loss: 2.46538662259\n",
      "Epoch 86 / 1000 loss: 2.4653810896\n",
      "Epoch 87 / 1000 loss: 2.46537584811\n",
      "Epoch 88 / 1000 loss: 2.46536981221\n",
      "Epoch 89 / 1000 loss: 2.46536336653\n",
      "Epoch 90 / 1000 loss: 2.46535761841\n",
      "Epoch 91 / 1000 loss: 2.4653522782\n",
      "Epoch 92 / 1000 loss: 2.46534643136\n",
      "Epoch 93 / 1000 loss: 2.46534111071\n",
      "Epoch 94 / 1000 loss: 2.4653360704\n",
      "Epoch 95 / 1000 loss: 2.46533147525\n",
      "Epoch 96 / 1000 loss: 2.46532671712\n",
      "Epoch 97 / 1000 loss: 2.46532221138\n",
      "Epoch 98 / 1000 loss: 2.4653176209\n",
      "Epoch 99 / 1000 loss: 2.46531310678\n",
      "Epoch 100 / 1000 loss: 2.46530799102\n",
      "Epoch 101 / 1000 loss: 2.46530314442\n",
      "Epoch 102 / 1000 loss: 2.46529774088\n",
      "Epoch 103 / 1000 loss: 2.46529272851\n",
      "Epoch 104 / 1000 loss: 2.46528784093\n",
      "Epoch 105 / 1000 loss: 2.46528354567\n",
      "Epoch 106 / 1000 loss: 2.46527957171\n",
      "Epoch 107 / 1000 loss: 2.46527457703\n",
      "Epoch 108 / 1000 loss: 2.46527003963\n",
      "Epoch 109 / 1000 loss: 2.46526591759\n",
      "Epoch 110 / 1000 loss: 2.46526176576\n",
      "Epoch 111 / 1000 loss: 2.46525792219\n",
      "Epoch 112 / 1000 loss: 2.46525425557\n",
      "Epoch 113 / 1000 loss: 2.46525087673\n",
      "Epoch 114 / 1000 loss: 2.46524770185\n",
      "Epoch 115 / 1000 loss: 2.46524450555\n",
      "Epoch 116 / 1000 loss: 2.46524128318\n",
      "Epoch 117 / 1000 loss: 2.46523837931\n",
      "Epoch 118 / 1000 loss: 2.46523571387\n",
      "Epoch 119 / 1000 loss: 2.46523254085\n",
      "Epoch 120 / 1000 loss: 2.46522929613\n",
      "Epoch 121 / 1000 loss: 2.46522622742\n",
      "Epoch 122 / 1000 loss: 2.46522338409\n",
      "Epoch 123 / 1000 loss: 2.46522068605\n",
      "Epoch 124 / 1000 loss: 2.46521787066\n",
      "Epoch 125 / 1000 loss: 2.46521471161\n",
      "Epoch 126 / 1000 loss: 2.46521101054\n",
      "Epoch 127 / 1000 loss: 2.46520688944\n",
      "Epoch 128 / 1000 loss: 2.46520325076\n",
      "Epoch 129 / 1000 loss: 2.46519962139\n",
      "Epoch 130 / 1000 loss: 2.46519599948\n",
      "Epoch 131 / 1000 loss: 2.46519262809\n",
      "Epoch 132 / 1000 loss: 2.46518917941\n",
      "Epoch 133 / 1000 loss: 2.46518578287\n",
      "Epoch 134 / 1000 loss: 2.46518269833\n",
      "Epoch 135 / 1000 loss: 2.46517991833\n",
      "Epoch 136 / 1000 loss: 2.46517735254\n",
      "Epoch 137 / 1000 loss: 2.46517383121\n",
      "Epoch 138 / 1000 loss: 2.465170715\n",
      "Epoch 139 / 1000 loss: 2.46516646259\n",
      "Epoch 140 / 1000 loss: 2.46516257059\n",
      "Epoch 141 / 1000 loss: 2.4651586758\n",
      "Epoch 142 / 1000 loss: 2.46515504643\n",
      "Epoch 143 / 1000 loss: 2.46515153814\n",
      "Epoch 144 / 1000 loss: 2.46514825057\n",
      "Epoch 145 / 1000 loss: 2.46514492854\n",
      "Epoch 146 / 1000 loss: 2.46514184121\n",
      "Epoch 147 / 1000 loss: 2.46513891313\n",
      "Epoch 148 / 1000 loss: 2.4651358407\n",
      "Epoch 149 / 1000 loss: 2.46513319947\n",
      "Epoch 150 / 1000 loss: 2.46512984671\n",
      "Epoch 151 / 1000 loss: 2.4651273163\n",
      "Epoch 152 / 1000 loss: 2.46512336843\n",
      "Epoch 153 / 1000 loss: 2.46511929948\n",
      "Epoch 154 / 1000 loss: 2.46511571575\n",
      "Epoch 155 / 1000 loss: 2.46511288919\n",
      "Epoch 156 / 1000 loss: 2.46510922909\n",
      "Epoch 157 / 1000 loss: 2.46510517411\n",
      "Epoch 158 / 1000 loss: 2.46510190517\n",
      "Epoch 159 / 1000 loss: 2.465098029\n",
      "Epoch 160 / 1000 loss: 2.46509425435\n",
      "Epoch 161 / 1000 loss: 2.46509078611\n",
      "Epoch 162 / 1000 loss: 2.46508767921\n",
      "Epoch 163 / 1000 loss: 2.46508474182\n",
      "Epoch 164 / 1000 loss: 2.46508196741\n",
      "Epoch 165 / 1000 loss: 2.46507931687\n",
      "Epoch 166 / 1000 loss: 2.46507683024\n",
      "Epoch 167 / 1000 loss: 2.46507430449\n",
      "Epoch 168 / 1000 loss: 2.46507186256\n",
      "Epoch 169 / 1000 loss: 2.4650696991\n",
      "Epoch 170 / 1000 loss: 2.46506750677\n",
      "Epoch 171 / 1000 loss: 2.4650656404\n",
      "Epoch 172 / 1000 loss: 2.46506376565\n",
      "Epoch 173 / 1000 loss: 2.46506195795\n",
      "Epoch 174 / 1000 loss: 2.46506030113\n",
      "Epoch 175 / 1000 loss: 2.46505879052\n",
      "Epoch 176 / 1000 loss: 2.46505719796\n",
      "Epoch 177 / 1000 loss: 2.46505557559\n",
      "Epoch 178 / 1000 loss: 2.46505414136\n",
      "Epoch 179 / 1000 loss: 2.46505266149\n",
      "Epoch 180 / 1000 loss: 2.4650511248\n",
      "Epoch 181 / 1000 loss: 2.46504963282\n",
      "Epoch 182 / 1000 loss: 2.46504824609\n",
      "Epoch 183 / 1000 loss: 2.46504686959\n",
      "Epoch 184 / 1000 loss: 2.46504554525\n",
      "Epoch 185 / 1000 loss: 2.46504425909\n",
      "Epoch 186 / 1000 loss: 2.46504295431\n",
      "Epoch 187 / 1000 loss: 2.46504171006\n",
      "Epoch 188 / 1000 loss: 2.4650403494\n",
      "Epoch 189 / 1000 loss: 2.46503901388\n",
      "Epoch 190 / 1000 loss: 2.4650376793\n",
      "Epoch 191 / 1000 loss: 2.46503607463\n",
      "Epoch 192 / 1000 loss: 2.46503453795\n",
      "Epoch 193 / 1000 loss: 2.46503288858\n",
      "Epoch 194 / 1000 loss: 2.46503141336\n",
      "Epoch 195 / 1000 loss: 2.4650298031\n",
      "Epoch 196 / 1000 loss: 2.46502840985\n",
      "Epoch 197 / 1000 loss: 2.46502702124\n",
      "Epoch 198 / 1000 loss: 2.46502581984\n",
      "Epoch 199 / 1000 loss: 2.46502465941\n",
      "Epoch 200 / 1000 loss: 2.46502330806\n",
      "Epoch 201 / 1000 loss: 2.46502189245\n",
      "Epoch 202 / 1000 loss: 2.46501962934\n",
      "Epoch 203 / 1000 loss: 2.46501657926\n",
      "Epoch 204 / 1000 loss: 2.46501382627\n",
      "Epoch 205 / 1000 loss: 2.46501083393\n",
      "Epoch 206 / 1000 loss: 2.46500797849\n",
      "Epoch 207 / 1000 loss: 2.46500578336\n",
      "Epoch 208 / 1000 loss: 2.46500332281\n",
      "Epoch 209 / 1000 loss: 2.46500107553\n",
      "Epoch 210 / 1000 loss: 2.46499897726\n",
      "Epoch 211 / 1000 loss: 2.46499681193\n",
      "Epoch 212 / 1000 loss: 2.46499483287\n",
      "Epoch 213 / 1000 loss: 2.4649931537\n",
      "Epoch 214 / 1000 loss: 2.46499172412\n",
      "Epoch 215 / 1000 loss: 2.46499001887\n",
      "Epoch 216 / 1000 loss: 2.46498833224\n",
      "Epoch 217 / 1000 loss: 2.46498660557\n",
      "Epoch 218 / 1000 loss: 2.46498479228\n",
      "Epoch 219 / 1000 loss: 2.46498261485\n",
      "Epoch 220 / 1000 loss: 2.46498015337\n",
      "Epoch 221 / 1000 loss: 2.46497745253\n",
      "Epoch 222 / 1000 loss: 2.46497520059\n",
      "Epoch 223 / 1000 loss: 2.4649726944\n",
      "Epoch 224 / 1000 loss: 2.46497075353\n",
      "Epoch 225 / 1000 loss: 2.46496850532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 226 / 1000 loss: 2.4649666287\n",
      "Epoch 227 / 1000 loss: 2.46496490762\n",
      "Epoch 228 / 1000 loss: 2.4649632154\n",
      "Epoch 229 / 1000 loss: 2.46496138908\n",
      "Epoch 230 / 1000 loss: 2.46495909151\n",
      "Epoch 231 / 1000 loss: 2.46495669521\n",
      "Epoch 232 / 1000 loss: 2.46495477855\n",
      "Epoch 233 / 1000 loss: 2.46495307889\n",
      "Epoch 234 / 1000 loss: 2.46495132893\n",
      "Epoch 235 / 1000 loss: 2.46494949795\n",
      "Epoch 236 / 1000 loss: 2.46494793147\n",
      "Epoch 237 / 1000 loss: 2.46494297776\n",
      "Epoch 238 / 1000 loss: 2.46493818983\n",
      "Epoch 239 / 1000 loss: 2.46493380424\n",
      "Epoch 240 / 1000 loss: 2.46493003238\n",
      "Epoch 241 / 1000 loss: 2.46492651105\n",
      "Epoch 242 / 1000 loss: 2.46492325049\n",
      "Epoch 243 / 1000 loss: 2.46492030099\n",
      "Epoch 244 / 1000 loss: 2.4649172537\n",
      "Epoch 245 / 1000 loss: 2.46491424181\n",
      "Epoch 246 / 1000 loss: 2.46491159406\n",
      "Epoch 247 / 1000 loss: 2.46490847226\n",
      "Epoch 248 / 1000 loss: 2.46490527596\n",
      "Epoch 249 / 1000 loss: 2.46490192413\n",
      "Epoch 250 / 1000 loss: 2.46489828266\n",
      "Epoch 251 / 1000 loss: 2.46489493269\n",
      "Epoch 252 / 1000 loss: 2.46489179414\n",
      "Epoch 253 / 1000 loss: 2.46488887817\n",
      "Epoch 254 / 1000 loss: 2.46488633193\n",
      "Epoch 255 / 1000 loss: 2.46488394588\n",
      "Epoch 256 / 1000 loss: 2.46488156263\n",
      "Epoch 257 / 1000 loss: 2.46487847343\n",
      "Epoch 258 / 1000 loss: 2.46487528551\n",
      "Epoch 259 / 1000 loss: 2.46487151459\n",
      "Epoch 260 / 1000 loss: 2.46486825775\n",
      "Epoch 261 / 1000 loss: 2.46486551408\n",
      "Epoch 262 / 1000 loss: 2.46486244816\n",
      "Epoch 263 / 1000 loss: 2.46485735942\n",
      "Epoch 264 / 1000 loss: 2.46485177521\n",
      "Epoch 265 / 1000 loss: 2.46484644804\n",
      "Epoch 266 / 1000 loss: 2.4648407856\n",
      "Epoch 267 / 1000 loss: 2.46483603027\n",
      "Epoch 268 / 1000 loss: 2.46483085956\n",
      "Epoch 269 / 1000 loss: 2.46482617501\n",
      "Epoch 270 / 1000 loss: 2.46482185554\n",
      "Epoch 271 / 1000 loss: 2.46481715515\n",
      "Epoch 272 / 1000 loss: 2.46481320728\n",
      "Epoch 273 / 1000 loss: 2.4648097381\n",
      "Epoch 274 / 1000 loss: 2.46480627451\n",
      "Epoch 275 / 1000 loss: 2.46480296925\n",
      "Epoch 276 / 1000 loss: 2.46480017435\n",
      "Epoch 277 / 1000 loss: 2.46479672007\n",
      "Epoch 278 / 1000 loss: 2.46479292307\n",
      "Epoch 279 / 1000 loss: 2.46478984039\n",
      "Epoch 280 / 1000 loss: 2.46478676982\n",
      "Epoch 281 / 1000 loss: 2.46478397865\n",
      "Epoch 282 / 1000 loss: 2.46478097327\n",
      "Epoch 283 / 1000 loss: 2.4647784438\n",
      "Epoch 284 / 1000 loss: 2.464775878\n",
      "Epoch 285 / 1000 loss: 2.46477301326\n",
      "Epoch 286 / 1000 loss: 2.46476969309\n",
      "Epoch 287 / 1000 loss: 2.4647669103\n",
      "Epoch 288 / 1000 loss: 2.46476392448\n",
      "Epoch 289 / 1000 loss: 2.46476040501\n",
      "Epoch 290 / 1000 loss: 2.46475582663\n",
      "Epoch 291 / 1000 loss: 2.46475137211\n",
      "Epoch 292 / 1000 loss: 2.46474670898\n",
      "Epoch 293 / 1000 loss: 2.46474177949\n",
      "Epoch 294 / 1000 loss: 2.46473789308\n",
      "Epoch 295 / 1000 loss: 2.46473266184\n",
      "Epoch 296 / 1000 loss: 2.46472665295\n",
      "Epoch 297 / 1000 loss: 2.4647210706\n",
      "Epoch 298 / 1000 loss: 2.46471677907\n",
      "Epoch 299 / 1000 loss: 2.46471288614\n",
      "Epoch 300 / 1000 loss: 2.46470915992\n",
      "Epoch 301 / 1000 loss: 2.4647050593\n",
      "Epoch 302 / 1000 loss: 2.46470191889\n",
      "Epoch 303 / 1000 loss: 2.46469852608\n",
      "Epoch 304 / 1000 loss: 2.4646954108\n",
      "Epoch 305 / 1000 loss: 2.46469223686\n",
      "Epoch 306 / 1000 loss: 2.4646889288\n",
      "Epoch 307 / 1000 loss: 2.46468580235\n",
      "Epoch 308 / 1000 loss: 2.4646826759\n",
      "Epoch 309 / 1000 loss: 2.46467997041\n",
      "Epoch 310 / 1000 loss: 2.46467737481\n",
      "Epoch 311 / 1000 loss: 2.46467459109\n",
      "Epoch 312 / 1000 loss: 2.46467186976\n",
      "Epoch 313 / 1000 loss: 2.46466914471\n",
      "Epoch 314 / 1000 loss: 2.4646666823\n",
      "Epoch 315 / 1000 loss: 2.46466439962\n",
      "Epoch 316 / 1000 loss: 2.46466188133\n",
      "Epoch 317 / 1000 loss: 2.46465955209\n",
      "Epoch 318 / 1000 loss: 2.46465702541\n",
      "Epoch 319 / 1000 loss: 2.464654807\n",
      "Epoch 320 / 1000 loss: 2.4646526156\n",
      "Epoch 321 / 1000 loss: 2.46465012338\n",
      "Epoch 322 / 1000 loss: 2.46464772429\n",
      "Epoch 323 / 1000 loss: 2.46464538388\n",
      "Epoch 324 / 1000 loss: 2.46464320179\n",
      "Epoch 325 / 1000 loss: 2.46464120038\n",
      "Epoch 326 / 1000 loss: 2.46463930327\n",
      "Epoch 327 / 1000 loss: 2.4646373149\n",
      "Epoch 328 / 1000 loss: 2.46463532187\n",
      "Epoch 329 / 1000 loss: 2.46463359706\n",
      "Epoch 330 / 1000 loss: 2.46463167574\n",
      "Epoch 331 / 1000 loss: 2.46462920681\n",
      "Epoch 332 / 1000 loss: 2.46462683845\n",
      "Epoch 333 / 1000 loss: 2.46462457441\n",
      "Epoch 334 / 1000 loss: 2.46462258231\n",
      "Epoch 335 / 1000 loss: 2.46462038346\n",
      "Epoch 336 / 1000 loss: 2.46461821534\n",
      "Epoch 337 / 1000 loss: 2.46461644862\n",
      "Epoch 338 / 1000 loss: 2.46461435687\n",
      "Epoch 339 / 1000 loss: 2.46461206395\n",
      "Epoch 340 / 1000 loss: 2.4646095261\n",
      "Epoch 341 / 1000 loss: 2.46460777428\n",
      "Epoch 342 / 1000 loss: 2.46460501198\n",
      "Epoch 343 / 1000 loss: 2.4646029193\n",
      "Epoch 344 / 1000 loss: 2.46460096072\n",
      "Epoch 345 / 1000 loss: 2.4645992117\n",
      "Epoch 346 / 1000 loss: 2.4645973444\n",
      "Epoch 347 / 1000 loss: 2.46459537651\n",
      "Epoch 348 / 1000 loss: 2.46459351853\n",
      "Epoch 349 / 1000 loss: 2.46459185611\n",
      "Epoch 350 / 1000 loss: 2.46459045168\n",
      "Epoch 351 / 1000 loss: 2.46458869521\n",
      "Epoch 352 / 1000 loss: 2.46458713431\n",
      "Epoch 353 / 1000 loss: 2.46458550822\n",
      "Epoch 354 / 1000 loss: 2.46458397433\n",
      "Epoch 355 / 1000 loss: 2.46458227746\n",
      "Epoch 356 / 1000 loss: 2.46458058618\n",
      "Epoch 357 / 1000 loss: 2.46457864437\n",
      "Epoch 358 / 1000 loss: 2.46457504109\n",
      "Epoch 359 / 1000 loss: 2.46457115095\n",
      "Epoch 360 / 1000 loss: 2.46456795\n",
      "Epoch 361 / 1000 loss: 2.46456543915\n",
      "Epoch 362 / 1000 loss: 2.46456323471\n",
      "Epoch 363 / 1000 loss: 2.46456123982\n",
      "Epoch 364 / 1000 loss: 2.46455892362\n",
      "Epoch 365 / 1000 loss: 2.46455659717\n",
      "Epoch 366 / 1000 loss: 2.46455423068\n",
      "Epoch 367 / 1000 loss: 2.46455045324\n",
      "Epoch 368 / 1000 loss: 2.46454639267\n",
      "Epoch 369 / 1000 loss: 2.46454217006\n",
      "Epoch 370 / 1000 loss: 2.46453829482\n",
      "Epoch 371 / 1000 loss: 2.46453451086\n",
      "Epoch 372 / 1000 loss: 2.46453029942\n",
      "Epoch 373 / 1000 loss: 2.46452666912\n",
      "Epoch 374 / 1000 loss: 2.46452343743\n",
      "Epoch 375 / 1000 loss: 2.46452021506\n",
      "Epoch 376 / 1000 loss: 2.46451745089\n",
      "Epoch 377 / 1000 loss: 2.46451463271\n",
      "Epoch 378 / 1000 loss: 2.46451032721\n",
      "Epoch 379 / 1000 loss: 2.46450541634\n",
      "Epoch 380 / 1000 loss: 2.46450029779\n",
      "Epoch 381 / 1000 loss: 2.46449567191\n",
      "Epoch 382 / 1000 loss: 2.46449048445\n",
      "Epoch 383 / 1000 loss: 2.46448629163\n",
      "Epoch 384 / 1000 loss: 2.4644826958\n",
      "Epoch 385 / 1000 loss: 2.46447930671\n",
      "Epoch 386 / 1000 loss: 2.46447640285\n",
      "Epoch 387 / 1000 loss: 2.46447329503\n",
      "Epoch 388 / 1000 loss: 2.46447070315\n",
      "Epoch 389 / 1000 loss: 2.46446732059\n",
      "Epoch 390 / 1000 loss: 2.46446448471\n",
      "Epoch 391 / 1000 loss: 2.46446117852\n",
      "Epoch 392 / 1000 loss: 2.46445820015\n",
      "Epoch 393 / 1000 loss: 2.46445538756\n",
      "Epoch 394 / 1000 loss: 2.46445348859\n",
      "Epoch 395 / 1000 loss: 2.46444975398\n",
      "Epoch 396 / 1000 loss: 2.46444675326\n",
      "Epoch 397 / 1000 loss: 2.46444331761\n",
      "Epoch 398 / 1000 loss: 2.46443896927\n",
      "Epoch 399 / 1000 loss: 2.46443373058\n",
      "Epoch 400 / 1000 loss: 2.46442846768\n",
      "Epoch 401 / 1000 loss: 2.4644227419\n",
      "Epoch 402 / 1000 loss: 2.46441797633\n",
      "Epoch 403 / 1000 loss: 2.46441347525\n",
      "Epoch 404 / 1000 loss: 2.46440960653\n",
      "Epoch 405 / 1000 loss: 2.46440500394\n",
      "Epoch 406 / 1000 loss: 2.46439920273\n",
      "Epoch 407 / 1000 loss: 2.46439258568\n",
      "Epoch 408 / 1000 loss: 2.46438678913\n",
      "Epoch 409 / 1000 loss: 2.46438135393\n",
      "Epoch 410 / 1000 loss: 2.46437436342\n",
      "Epoch 411 / 1000 loss: 2.46436809935\n",
      "Epoch 412 / 1000 loss: 2.46436368488\n",
      "Epoch 413 / 1000 loss: 2.46435577609\n",
      "Epoch 414 / 1000 loss: 2.46434849408\n",
      "Epoch 415 / 1000 loss: 2.46434251219\n",
      "Epoch 416 / 1000 loss: 2.46433663089\n",
      "Epoch 417 / 1000 loss: 2.46433095448\n",
      "Epoch 418 / 1000 loss: 2.46432588063\n",
      "Epoch 419 / 1000 loss: 2.46431912761\n",
      "Epoch 420 / 1000 loss: 2.46431301069\n",
      "Epoch 421 / 1000 loss: 2.46430756524\n",
      "Epoch 422 / 1000 loss: 2.46430165228\n",
      "Epoch 423 / 1000 loss: 2.46429406386\n",
      "Epoch 424 / 1000 loss: 2.46428632829\n",
      "Epoch 425 / 1000 loss: 2.46427820809\n",
      "Epoch 426 / 1000 loss: 2.46427084971\n",
      "Epoch 427 / 1000 loss: 2.46426307876\n",
      "Epoch 428 / 1000 loss: 2.46425499767\n",
      "Epoch 429 / 1000 loss: 2.46424772125\n",
      "Epoch 430 / 1000 loss: 2.46424142178\n",
      "Epoch 431 / 1000 loss: 2.46423502639\n",
      "Epoch 432 / 1000 loss: 2.46422932949\n",
      "Epoch 433 / 1000 loss: 2.46422383934\n",
      "Epoch 434 / 1000 loss: 2.46421817224\n",
      "Epoch 435 / 1000 loss: 2.4642122807\n",
      "Epoch 436 / 1000 loss: 2.46420686413\n",
      "Epoch 437 / 1000 loss: 2.46420085058\n",
      "Epoch 438 / 1000 loss: 2.46419466194\n",
      "Epoch 439 / 1000 loss: 2.46418793779\n",
      "Epoch 440 / 1000 loss: 2.46418159176\n",
      "Epoch 441 / 1000 loss: 2.46417580172\n",
      "Epoch 442 / 1000 loss: 2.4641700387\n",
      "Epoch 443 / 1000 loss: 2.46416483354\n",
      "Epoch 444 / 1000 loss: 2.46415931918\n",
      "Epoch 445 / 1000 loss: 2.4641531771\n",
      "Epoch 446 / 1000 loss: 2.46414793096\n",
      "Epoch 447 / 1000 loss: 2.46414295025\n",
      "Epoch 448 / 1000 loss: 2.46413708944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 449 / 1000 loss: 2.46413157601\n",
      "Epoch 450 / 1000 loss: 2.46412640158\n",
      "Epoch 451 / 1000 loss: 2.46412011608\n",
      "Epoch 452 / 1000 loss: 2.46411274932\n",
      "Epoch 453 / 1000 loss: 2.46410491969\n",
      "Epoch 454 / 1000 loss: 2.46409721673\n",
      "Epoch 455 / 1000 loss: 2.46408877615\n",
      "Epoch 456 / 1000 loss: 2.46408044174\n",
      "Epoch 457 / 1000 loss: 2.46407152805\n",
      "Epoch 458 / 1000 loss: 2.46406290494\n",
      "Epoch 459 / 1000 loss: 2.46405452397\n",
      "Epoch 460 / 1000 loss: 2.46404569782\n",
      "Epoch 461 / 1000 loss: 2.46403800137\n",
      "Epoch 462 / 1000 loss: 2.46403164137\n",
      "Epoch 463 / 1000 loss: 2.4640255142\n",
      "Epoch 464 / 1000 loss: 2.46402012277\n",
      "Epoch 465 / 1000 loss: 2.46401486825\n",
      "Epoch 466 / 1000 loss: 2.4640114354\n",
      "Epoch 467 / 1000 loss: 2.46400449425\n",
      "Epoch 468 / 1000 loss: 2.46399805788\n",
      "Epoch 469 / 1000 loss: 2.46399211977\n",
      "Epoch 470 / 1000 loss: 2.46398640797\n",
      "Epoch 471 / 1000 loss: 2.46398090571\n",
      "Epoch 472 / 1000 loss: 2.46397451311\n",
      "Epoch 473 / 1000 loss: 2.46396859456\n",
      "Epoch 474 / 1000 loss: 2.46396162454\n",
      "Epoch 475 / 1000 loss: 2.4639546508\n",
      "Epoch 476 / 1000 loss: 2.46394851524\n",
      "Epoch 477 / 1000 loss: 2.46394025814\n",
      "Epoch 478 / 1000 loss: 2.46393075772\n",
      "Epoch 479 / 1000 loss: 2.46392213553\n",
      "Epoch 480 / 1000 loss: 2.46391390078\n",
      "Epoch 481 / 1000 loss: 2.46390586719\n",
      "Epoch 482 / 1000 loss: 2.46389908437\n",
      "Epoch 483 / 1000 loss: 2.4638921367\n",
      "Epoch 484 / 1000 loss: 2.46388417762\n",
      "Epoch 485 / 1000 loss: 2.46387690492\n",
      "Epoch 486 / 1000 loss: 2.46387106087\n",
      "Epoch 487 / 1000 loss: 2.46386509109\n",
      "Epoch 488 / 1000 loss: 2.463855993\n",
      "Epoch 489 / 1000 loss: 2.4638467608\n",
      "Epoch 490 / 1000 loss: 2.46383766364\n",
      "Epoch 491 / 1000 loss: 2.46382690873\n",
      "Epoch 492 / 1000 loss: 2.46381759644\n",
      "Epoch 493 / 1000 loss: 2.46380802151\n",
      "Epoch 494 / 1000 loss: 2.46379915811\n",
      "Epoch 495 / 1000 loss: 2.46378759574\n",
      "Epoch 496 / 1000 loss: 2.46377645992\n",
      "Epoch 497 / 1000 loss: 2.46376761608\n",
      "Epoch 498 / 1000 loss: 2.46375706606\n",
      "Epoch 499 / 1000 loss: 2.46374821942\n",
      "Epoch 500 / 1000 loss: 2.4637392601\n",
      "Epoch 501 / 1000 loss: 2.46372994501\n",
      "Epoch 502 / 1000 loss: 2.46372031793\n",
      "Epoch 503 / 1000 loss: 2.46370949224\n",
      "Epoch 504 / 1000 loss: 2.46369825304\n",
      "Epoch 505 / 1000 loss: 2.46368754189\n",
      "Epoch 506 / 1000 loss: 2.46367712971\n",
      "Epoch 507 / 1000 loss: 2.46366672404\n",
      "Epoch 508 / 1000 loss: 2.46365682781\n",
      "Epoch 509 / 1000 loss: 2.4636489572\n",
      "Epoch 510 / 1000 loss: 2.46363961045\n",
      "Epoch 511 / 1000 loss: 2.46363043226\n",
      "Epoch 512 / 1000 loss: 2.46362226363\n",
      "Epoch 513 / 1000 loss: 2.46361517906\n",
      "Epoch 514 / 1000 loss: 2.46360855736\n",
      "Epoch 515 / 1000 loss: 2.46359987278\n",
      "Epoch 516 / 1000 loss: 2.46359194443\n",
      "Epoch 517 / 1000 loss: 2.46358180698\n",
      "Epoch 518 / 1000 loss: 2.46356677264\n",
      "Epoch 519 / 1000 loss: 2.46355500724\n",
      "Epoch 520 / 1000 loss: 2.46354374848\n",
      "Epoch 521 / 1000 loss: 2.46353462897\n",
      "Epoch 522 / 1000 loss: 2.46352678444\n",
      "Epoch 523 / 1000 loss: 2.4635188533\n",
      "Epoch 524 / 1000 loss: 2.4635113785\n",
      "Epoch 525 / 1000 loss: 2.46350328811\n",
      "Epoch 526 / 1000 loss: 2.46349374391\n",
      "Epoch 527 / 1000 loss: 2.46348493546\n",
      "Epoch 528 / 1000 loss: 2.46347678453\n",
      "Epoch 529 / 1000 loss: 2.46346934605\n",
      "Epoch 530 / 1000 loss: 2.4634605553\n",
      "Epoch 531 / 1000 loss: 2.46345000993\n",
      "Epoch 532 / 1000 loss: 2.46344217565\n",
      "Epoch 533 / 1000 loss: 2.46343226079\n",
      "Epoch 534 / 1000 loss: 2.46342120413\n",
      "Epoch 535 / 1000 loss: 2.46340927575\n",
      "Epoch 536 / 1000 loss: 2.46339560114\n",
      "Epoch 537 / 1000 loss: 2.46338372864\n",
      "Epoch 538 / 1000 loss: 2.46337074786\n",
      "Epoch 539 / 1000 loss: 2.46335616335\n",
      "Epoch 540 / 1000 loss: 2.46334326267\n",
      "Epoch 541 / 1000 loss: 2.46332460362\n",
      "Epoch 542 / 1000 loss: 2.46331024077\n",
      "Epoch 543 / 1000 loss: 2.46329338197\n",
      "Epoch 544 / 1000 loss: 2.46328031458\n",
      "Epoch 545 / 1000 loss: 2.46326345205\n",
      "Epoch 546 / 1000 loss: 2.46324788406\n",
      "Epoch 547 / 1000 loss: 2.46323487721\n",
      "Epoch 548 / 1000 loss: 2.46321902703\n",
      "Epoch 549 / 1000 loss: 2.46320416499\n",
      "Epoch 550 / 1000 loss: 2.46318685543\n",
      "Epoch 551 / 1000 loss: 2.46317225974\n",
      "Epoch 552 / 1000 loss: 2.4631534284\n",
      "Epoch 553 / 1000 loss: 2.46313723456\n",
      "Epoch 554 / 1000 loss: 2.46312001161\n",
      "Epoch 555 / 1000 loss: 2.4631066788\n",
      "Epoch 556 / 1000 loss: 2.46308991499\n",
      "Epoch 557 / 1000 loss: 2.46307483409\n",
      "Epoch 558 / 1000 loss: 2.4630573662\n",
      "Epoch 559 / 1000 loss: 2.46304127295\n",
      "Epoch 560 / 1000 loss: 2.46302564442\n",
      "Epoch 561 / 1000 loss: 2.46301396284\n",
      "Epoch 562 / 1000 loss: 2.46299838275\n",
      "Epoch 563 / 1000 loss: 2.46298497915\n",
      "Epoch 564 / 1000 loss: 2.46297188103\n",
      "Epoch 565 / 1000 loss: 2.46295845509\n",
      "Epoch 566 / 1000 loss: 2.46294383891\n",
      "Epoch 567 / 1000 loss: 2.46292684134\n",
      "Epoch 568 / 1000 loss: 2.46291428618\n",
      "Epoch 569 / 1000 loss: 2.46290131938\n",
      "Epoch 570 / 1000 loss: 2.46288690343\n",
      "Epoch 571 / 1000 loss: 2.46287235059\n",
      "Epoch 572 / 1000 loss: 2.46285820566\n",
      "Epoch 573 / 1000 loss: 2.46284257341\n",
      "Epoch 574 / 1000 loss: 2.46283028554\n",
      "Epoch 575 / 1000 loss: 2.46281615086\n",
      "Epoch 576 / 1000 loss: 2.46280383226\n",
      "Epoch 577 / 1000 loss: 2.46279254463\n",
      "Epoch 578 / 1000 loss: 2.46278086305\n",
      "Epoch 579 / 1000 loss: 2.46276878566\n",
      "Epoch 580 / 1000 loss: 2.46275651734\n",
      "Epoch 581 / 1000 loss: 2.46274206042\n",
      "Epoch 582 / 1000 loss: 2.46272694413\n",
      "Epoch 583 / 1000 loss: 2.46271346509\n",
      "Epoch 584 / 1000 loss: 2.46270009968\n",
      "Epoch 585 / 1000 loss: 2.46268756781\n",
      "Epoch 586 / 1000 loss: 2.46267665271\n",
      "Epoch 587 / 1000 loss: 2.46266246028\n",
      "Epoch 588 / 1000 loss: 2.46265186835\n",
      "Epoch 589 / 1000 loss: 2.46264235396\n",
      "Epoch 590 / 1000 loss: 2.46262849495\n",
      "Epoch 591 / 1000 loss: 2.46261626016\n",
      "Epoch 592 / 1000 loss: 2.46260550432\n",
      "Epoch 593 / 1000 loss: 2.46259215195\n",
      "Epoch 594 / 1000 loss: 2.46258274838\n",
      "Epoch 595 / 1000 loss: 2.46257135924\n",
      "Epoch 596 / 1000 loss: 2.46255881339\n",
      "Epoch 597 / 1000 loss: 2.46254717465\n",
      "Epoch 598 / 1000 loss: 2.46253677923\n",
      "Epoch 599 / 1000 loss: 2.4625248732\n",
      "Epoch 600 / 1000 loss: 2.46251437441\n",
      "Epoch 601 / 1000 loss: 2.46250396222\n",
      "Epoch 602 / 1000 loss: 2.46249409858\n",
      "Epoch 603 / 1000 loss: 2.46248416509\n",
      "Epoch 604 / 1000 loss: 2.46247214079\n",
      "Epoch 605 / 1000 loss: 2.46246075816\n",
      "Epoch 606 / 1000 loss: 2.46245157812\n",
      "Epoch 607 / 1000 loss: 2.46243916824\n",
      "Epoch 608 / 1000 loss: 2.46242821217\n",
      "Epoch 609 / 1000 loss: 2.46241873596\n",
      "Epoch 610 / 1000 loss: 2.46240659617\n",
      "Epoch 611 / 1000 loss: 2.46239584219\n",
      "Epoch 612 / 1000 loss: 2.46238398552\n",
      "Epoch 613 / 1000 loss: 2.46237321757\n",
      "Epoch 614 / 1000 loss: 2.46236238908\n",
      "Epoch 615 / 1000 loss: 2.46235253103\n",
      "Epoch 616 / 1000 loss: 2.46234244481\n",
      "Epoch 617 / 1000 loss: 2.46232867241\n",
      "Epoch 618 / 1000 loss: 2.46231437474\n",
      "Epoch 619 / 1000 loss: 2.4622996496\n",
      "Epoch 620 / 1000 loss: 2.46228767373\n",
      "Epoch 621 / 1000 loss: 2.46227578074\n",
      "Epoch 622 / 1000 loss: 2.46226419881\n",
      "Epoch 623 / 1000 loss: 2.46225296427\n",
      "Epoch 624 / 1000 loss: 2.46224021353\n",
      "Epoch 625 / 1000 loss: 2.46222649515\n",
      "Epoch 626 / 1000 loss: 2.4622128997\n",
      "Epoch 627 / 1000 loss: 2.46220074315\n",
      "Epoch 628 / 1000 loss: 2.46218958776\n",
      "Epoch 629 / 1000 loss: 2.46217678115\n",
      "Epoch 630 / 1000 loss: 2.46216473542\n",
      "Epoch 631 / 1000 loss: 2.46215250995\n",
      "Epoch 632 / 1000 loss: 2.46214187425\n",
      "Epoch 633 / 1000 loss: 2.46212888137\n",
      "Epoch 634 / 1000 loss: 2.46211583074\n",
      "Epoch 635 / 1000 loss: 2.4621012304\n",
      "Epoch 636 / 1000 loss: 2.4620875651\n",
      "Epoch 637 / 1000 loss: 2.46207546163\n",
      "Epoch 638 / 1000 loss: 2.46206227504\n",
      "Epoch 639 / 1000 loss: 2.46204802953\n",
      "Epoch 640 / 1000 loss: 2.46203476377\n",
      "Epoch 641 / 1000 loss: 2.46201878134\n",
      "Epoch 642 / 1000 loss: 2.46200433839\n",
      "Epoch 643 / 1000 loss: 2.46198968776\n",
      "Epoch 644 / 1000 loss: 2.46197544411\n",
      "Epoch 645 / 1000 loss: 2.4619617248\n",
      "Epoch 646 / 1000 loss: 2.46194534842\n",
      "Epoch 647 / 1000 loss: 2.46192966867\n",
      "Epoch 648 / 1000 loss: 2.46191374399\n",
      "Epoch 649 / 1000 loss: 2.46189774666\n",
      "Epoch 650 / 1000 loss: 2.46188198682\n",
      "Epoch 651 / 1000 loss: 2.46186667494\n",
      "Epoch 652 / 1000 loss: 2.46184963174\n",
      "Epoch 653 / 1000 loss: 2.46183242463\n",
      "Epoch 654 / 1000 loss: 2.46181818377\n",
      "Epoch 655 / 1000 loss: 2.46179990377\n",
      "Epoch 656 / 1000 loss: 2.46177608334\n",
      "Epoch 657 / 1000 loss: 2.46175537817\n",
      "Epoch 658 / 1000 loss: 2.46173571981\n",
      "Epoch 659 / 1000 loss: 2.46171567589\n",
      "Epoch 660 / 1000 loss: 2.46169730835\n",
      "Epoch 661 / 1000 loss: 2.46167925932\n",
      "Epoch 662 / 1000 loss: 2.46166228969\n",
      "Epoch 663 / 1000 loss: 2.46164288092\n",
      "Epoch 664 / 1000 loss: 2.46162428055\n",
      "Epoch 665 / 1000 loss: 2.46160655096\n",
      "Epoch 666 / 1000 loss: 2.46158859413\n",
      "Epoch 667 / 1000 loss: 2.46156939771\n",
      "Epoch 668 / 1000 loss: 2.46154781431\n",
      "Epoch 669 / 1000 loss: 2.46152633615\n",
      "Epoch 670 / 1000 loss: 2.46150519606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 671 / 1000 loss: 2.46147823799\n",
      "Epoch 672 / 1000 loss: 2.46145304944\n",
      "Epoch 673 / 1000 loss: 2.46142713074\n",
      "Epoch 674 / 1000 loss: 2.46140113659\n",
      "Epoch 675 / 1000 loss: 2.46137732733\n",
      "Epoch 676 / 1000 loss: 2.46135128755\n",
      "Epoch 677 / 1000 loss: 2.46132291295\n",
      "Epoch 678 / 1000 loss: 2.46129598655\n",
      "Epoch 679 / 1000 loss: 2.46126933116\n",
      "Epoch 680 / 1000 loss: 2.46124115121\n",
      "Epoch 681 / 1000 loss: 2.46121247858\n",
      "Epoch 682 / 1000 loss: 2.46118256915\n",
      "Epoch 683 / 1000 loss: 2.4611491356\n",
      "Epoch 684 / 1000 loss: 2.46111218259\n",
      "Epoch 685 / 1000 loss: 2.46107783169\n",
      "Epoch 686 / 1000 loss: 2.4610411264\n",
      "Epoch 687 / 1000 loss: 2.46100168675\n",
      "Epoch 688 / 1000 loss: 2.46096902434\n",
      "Epoch 689 / 1000 loss: 2.46092945058\n",
      "Epoch 690 / 1000 loss: 2.46088873316\n",
      "Epoch 691 / 1000 loss: 2.46084885485\n",
      "Epoch 692 / 1000 loss: 2.46080751065\n",
      "Epoch 693 / 1000 loss: 2.46076192707\n",
      "Epoch 694 / 1000 loss: 2.46071268898\n",
      "Epoch 695 / 1000 loss: 2.460659693\n",
      "Epoch 696 / 1000 loss: 2.46060473658\n",
      "Epoch 697 / 1000 loss: 2.46054627839\n",
      "Epoch 698 / 1000 loss: 2.46048340574\n",
      "Epoch 699 / 1000 loss: 2.46042488515\n",
      "Epoch 700 / 1000 loss: 2.46035624761\n",
      "Epoch 701 / 1000 loss: 2.46028453112\n",
      "Epoch 702 / 1000 loss: 2.46021218691\n",
      "Epoch 703 / 1000 loss: 2.46013979986\n",
      "Epoch 704 / 1000 loss: 2.46006496996\n",
      "Epoch 705 / 1000 loss: 2.45999100525\n",
      "Epoch 706 / 1000 loss: 2.45991432667\n",
      "Epoch 707 / 1000 loss: 2.45983378589\n",
      "Epoch 708 / 1000 loss: 2.45975491591\n",
      "Epoch 709 / 1000 loss: 2.45967379119\n",
      "Epoch 710 / 1000 loss: 2.45958710276\n",
      "Epoch 711 / 1000 loss: 2.45949590299\n",
      "Epoch 712 / 1000 loss: 2.45939796139\n",
      "Epoch 713 / 1000 loss: 2.45929851104\n",
      "Epoch 714 / 1000 loss: 2.45918908529\n",
      "Epoch 715 / 1000 loss: 2.45907566138\n",
      "Epoch 716 / 1000 loss: 2.45896696299\n",
      "Epoch 717 / 1000 loss: 2.45884620119\n",
      "Epoch 718 / 1000 loss: 2.45872448571\n",
      "Epoch 719 / 1000 loss: 2.45859780442\n",
      "Epoch 720 / 1000 loss: 2.458463463\n",
      "Epoch 721 / 1000 loss: 2.45832783449\n",
      "Epoch 722 / 1000 loss: 2.45817303378\n",
      "Epoch 723 / 1000 loss: 2.45800699107\n",
      "Epoch 724 / 1000 loss: 2.45782783441\n",
      "Epoch 725 / 1000 loss: 2.45762984175\n",
      "Epoch 726 / 1000 loss: 2.45743207075\n",
      "Epoch 727 / 1000 loss: 2.45720767416\n",
      "Epoch 728 / 1000 loss: 2.45697599929\n",
      "Epoch 729 / 1000 loss: 2.45671728626\n",
      "Epoch 730 / 1000 loss: 2.45645159297\n",
      "Epoch 731 / 1000 loss: 2.45615357812\n",
      "Epoch 732 / 1000 loss: 2.45582264289\n",
      "Epoch 733 / 1000 loss: 2.45546304807\n",
      "Epoch 734 / 1000 loss: 2.45505573694\n",
      "Epoch 735 / 1000 loss: 2.45461245812\n",
      "Epoch 736 / 1000 loss: 2.4541135896\n",
      "Epoch 737 / 1000 loss: 2.45353088994\n",
      "Epoch 738 / 1000 loss: 2.45286017098\n",
      "Epoch 739 / 1000 loss: 2.45206859987\n",
      "Epoch 740 / 1000 loss: 2.45108786691\n",
      "Epoch 741 / 1000 loss: 2.44983509183\n",
      "Epoch 742 / 1000 loss: 2.44820663705\n",
      "Epoch 743 / 1000 loss: 2.44596737344\n",
      "Epoch 744 / 1000 loss: 2.44330472592\n",
      "Epoch 745 / 1000 loss: 2.44179124013\n",
      "Epoch 746 / 1000 loss: 2.44134619646\n",
      "Epoch 747 / 1000 loss: 2.44093538169\n",
      "Epoch 748 / 1000 loss: 2.44046261907\n",
      "Epoch 749 / 1000 loss: 2.43989969511\n",
      "Epoch 750 / 1000 loss: 2.43922123127\n",
      "Epoch 751 / 1000 loss: 2.43841023184\n",
      "Epoch 752 / 1000 loss: 2.43740811106\n",
      "Epoch 753 / 1000 loss: 2.43618560676\n",
      "Epoch 754 / 1000 loss: 2.43476817198\n",
      "Epoch 755 / 1000 loss: 2.43331775907\n",
      "Epoch 756 / 1000 loss: 2.43233470339\n",
      "Epoch 757 / 1000 loss: 2.43198822998\n",
      "Epoch 758 / 1000 loss: 2.43189611472\n",
      "Epoch 759 / 1000 loss: 2.43184268568\n",
      "Epoch 760 / 1000 loss: 2.43179299496\n",
      "Epoch 761 / 1000 loss: 2.4317435734\n",
      "Epoch 762 / 1000 loss: 2.43169407267\n",
      "Epoch 763 / 1000 loss: 2.43164515682\n",
      "Epoch 764 / 1000 loss: 2.43159613479\n",
      "Epoch 765 / 1000 loss: 2.43154639285\n",
      "Epoch 766 / 1000 loss: 2.43149659224\n",
      "Epoch 767 / 1000 loss: 2.4314466631\n",
      "Epoch 768 / 1000 loss: 2.43139752001\n",
      "Epoch 769 / 1000 loss: 2.4313481478\n",
      "Epoch 770 / 1000 loss: 2.43129840028\n",
      "Epoch 771 / 1000 loss: 2.43124809675\n",
      "Epoch 772 / 1000 loss: 2.43119798601\n",
      "Epoch 773 / 1000 loss: 2.43114731181\n",
      "Epoch 774 / 1000 loss: 2.43109579477\n",
      "Epoch 775 / 1000 loss: 2.43104349915\n",
      "Epoch 776 / 1000 loss: 2.43099179585\n",
      "Epoch 777 / 1000 loss: 2.43094054516\n",
      "Epoch 778 / 1000 loss: 2.43088960461\n",
      "Epoch 779 / 1000 loss: 2.43083829992\n",
      "Epoch 780 / 1000 loss: 2.43078683689\n",
      "Epoch 781 / 1000 loss: 2.43073498644\n",
      "Epoch 782 / 1000 loss: 2.43068316858\n",
      "Epoch 783 / 1000 loss: 2.43063147645\n",
      "Epoch 784 / 1000 loss: 2.43057958968\n",
      "Epoch 785 / 1000 loss: 2.43052757904\n",
      "Epoch 786 / 1000 loss: 2.43047538213\n",
      "Epoch 787 / 1000 loss: 2.43042320386\n",
      "Epoch 788 / 1000 loss: 2.4303706307\n",
      "Epoch 789 / 1000 loss: 2.4303186005\n",
      "Epoch 790 / 1000 loss: 2.43026591744\n",
      "Epoch 791 / 1000 loss: 2.43021383602\n",
      "Epoch 792 / 1000 loss: 2.43016103562\n",
      "Epoch 793 / 1000 loss: 2.43010867853\n",
      "Epoch 794 / 1000 loss: 2.43005524203\n",
      "Epoch 795 / 1000 loss: 2.43000189681\n",
      "Epoch 796 / 1000 loss: 2.42994811013\n",
      "Epoch 797 / 1000 loss: 2.42989452835\n",
      "Epoch 798 / 1000 loss: 2.42984094098\n",
      "Epoch 799 / 1000 loss: 2.42978663463\n",
      "Epoch 800 / 1000 loss: 2.42973245773\n",
      "Epoch 801 / 1000 loss: 2.42967873253\n",
      "Epoch 802 / 1000 loss: 2.42962419521\n",
      "Epoch 803 / 1000 loss: 2.42957006767\n",
      "Epoch 804 / 1000 loss: 2.42951578181\n",
      "Epoch 805 / 1000 loss: 2.42946129199\n",
      "Epoch 806 / 1000 loss: 2.4294070974\n",
      "Epoch 807 / 1000 loss: 2.42935237102\n",
      "Epoch 808 / 1000 loss: 2.429296894\n",
      "Epoch 809 / 1000 loss: 2.4292416852\n",
      "Epoch 810 / 1000 loss: 2.4291860871\n",
      "Epoch 811 / 1000 loss: 2.4291310627\n",
      "Epoch 812 / 1000 loss: 2.42907518335\n",
      "Epoch 813 / 1000 loss: 2.42901979294\n",
      "Epoch 814 / 1000 loss: 2.42896343768\n",
      "Epoch 815 / 1000 loss: 2.42890730221\n",
      "Epoch 816 / 1000 loss: 2.42885092273\n",
      "Epoch 817 / 1000 loss: 2.42879490834\n",
      "Epoch 818 / 1000 loss: 2.42873817217\n",
      "Epoch 819 / 1000 loss: 2.42868195102\n",
      "Epoch 820 / 1000 loss: 2.42862527631\n",
      "Epoch 821 / 1000 loss: 2.42856858764\n",
      "Epoch 822 / 1000 loss: 2.42851207592\n",
      "Epoch 823 / 1000 loss: 2.42845513299\n",
      "Epoch 824 / 1000 loss: 2.4283976974\n",
      "Epoch 825 / 1000 loss: 2.42834052537\n",
      "Epoch 826 / 1000 loss: 2.42828301713\n",
      "Epoch 827 / 1000 loss: 2.42822615989\n",
      "Epoch 828 / 1000 loss: 2.42816883978\n",
      "Epoch 829 / 1000 loss: 2.42811128031\n",
      "Epoch 830 / 1000 loss: 2.42805343866\n",
      "Epoch 831 / 1000 loss: 2.42799567897\n",
      "Epoch 832 / 1000 loss: 2.42793752253\n",
      "Epoch 833 / 1000 loss: 2.42787919845\n",
      "Epoch 834 / 1000 loss: 2.42782099079\n",
      "Epoch 835 / 1000 loss: 2.42776263133\n",
      "Epoch 836 / 1000 loss: 2.42770421226\n",
      "Epoch 837 / 1000 loss: 2.42764597014\n",
      "Epoch 838 / 1000 loss: 2.42758704349\n",
      "Epoch 839 / 1000 loss: 2.42752879113\n",
      "Epoch 840 / 1000 loss: 2.42747019324\n",
      "Epoch 841 / 1000 loss: 2.42741099559\n",
      "Epoch 842 / 1000 loss: 2.42735143565\n",
      "Epoch 843 / 1000 loss: 2.42729258351\n",
      "Epoch 844 / 1000 loss: 2.4272322543\n",
      "Epoch 845 / 1000 loss: 2.42717318702\n",
      "Epoch 846 / 1000 loss: 2.42711239681\n",
      "Epoch 847 / 1000 loss: 2.42705122288\n",
      "Epoch 848 / 1000 loss: 2.42699012067\n",
      "Epoch 849 / 1000 loss: 2.42692851182\n",
      "Epoch 850 / 1000 loss: 2.42686676513\n",
      "Epoch 851 / 1000 loss: 2.42680480797\n",
      "Epoch 852 / 1000 loss: 2.42674210854\n",
      "Epoch 853 / 1000 loss: 2.42668033578\n",
      "Epoch 854 / 1000 loss: 2.42661780678\n",
      "Epoch 855 / 1000 loss: 2.42655575648\n",
      "Epoch 856 / 1000 loss: 2.42649351712\n",
      "Epoch 857 / 1000 loss: 2.4264306426\n",
      "Epoch 858 / 1000 loss: 2.426368827\n",
      "Epoch 859 / 1000 loss: 2.42630593386\n",
      "Epoch 860 / 1000 loss: 2.42624274362\n",
      "Epoch 861 / 1000 loss: 2.42617926933\n",
      "Epoch 862 / 1000 loss: 2.4261148097\n",
      "Epoch 863 / 1000 loss: 2.42605009675\n",
      "Epoch 864 / 1000 loss: 2.42598452605\n",
      "Epoch 865 / 1000 loss: 2.42591752019\n",
      "Epoch 866 / 1000 loss: 2.42584869266\n",
      "Epoch 867 / 1000 loss: 2.42577523924\n",
      "Epoch 868 / 1000 loss: 2.42569819\n",
      "Epoch 869 / 1000 loss: 2.42562236357\n",
      "Epoch 870 / 1000 loss: 2.42554792389\n",
      "Epoch 871 / 1000 loss: 2.4254643796\n",
      "Epoch 872 / 1000 loss: 2.42538652942\n",
      "Epoch 873 / 1000 loss: 2.42530793417\n",
      "Epoch 874 / 1000 loss: 2.42520271242\n",
      "Epoch 875 / 1000 loss: 2.4251046842\n",
      "Epoch 876 / 1000 loss: 2.42500406597\n",
      "Epoch 877 / 1000 loss: 2.42487928085\n",
      "Epoch 878 / 1000 loss: 2.42477396782\n",
      "Epoch 879 / 1000 loss: 2.42466853745\n",
      "Epoch 880 / 1000 loss: 2.42456876487\n",
      "Epoch 881 / 1000 loss: 2.42446704116\n",
      "Epoch 882 / 1000 loss: 2.42436011508\n",
      "Epoch 883 / 1000 loss: 2.42425818555\n",
      "Epoch 884 / 1000 loss: 2.42415801995\n",
      "Epoch 885 / 1000 loss: 2.42405999359\n",
      "Epoch 886 / 1000 loss: 2.42396612559\n",
      "Epoch 887 / 1000 loss: 2.42386649922\n",
      "Epoch 888 / 1000 loss: 2.42374598142\n",
      "Epoch 889 / 1000 loss: 2.42361935787\n",
      "Epoch 890 / 1000 loss: 2.42349070217\n",
      "Epoch 891 / 1000 loss: 2.42337115481\n",
      "Epoch 892 / 1000 loss: 2.42326091975\n",
      "Epoch 893 / 1000 loss: 2.42314563692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 894 / 1000 loss: 2.42303235922\n",
      "Epoch 895 / 1000 loss: 2.42292248085\n",
      "Epoch 896 / 1000 loss: 2.42281843629\n",
      "Epoch 897 / 1000 loss: 2.42271766067\n",
      "Epoch 898 / 1000 loss: 2.42262098659\n",
      "Epoch 899 / 1000 loss: 2.42252686899\n",
      "Epoch 900 / 1000 loss: 2.42243206222\n",
      "Epoch 901 / 1000 loss: 2.42234020494\n",
      "Epoch 902 / 1000 loss: 2.42224829923\n",
      "Epoch 903 / 1000 loss: 2.42216014583\n",
      "Epoch 904 / 1000 loss: 2.42206925433\n",
      "Epoch 905 / 1000 loss: 2.42198222782\n",
      "Epoch 906 / 1000 loss: 2.42189576477\n",
      "Epoch 907 / 1000 loss: 2.42180787493\n",
      "Epoch 908 / 1000 loss: 2.42172090337\n",
      "Epoch 909 / 1000 loss: 2.42163479608\n",
      "Epoch 910 / 1000 loss: 2.42154883221\n",
      "Epoch 911 / 1000 loss: 2.42146343458\n",
      "Epoch 912 / 1000 loss: 2.42137541529\n",
      "Epoch 913 / 1000 loss: 2.42128983792\n",
      "Epoch 914 / 1000 loss: 2.42120154761\n",
      "Epoch 915 / 1000 loss: 2.42111262586\n",
      "Epoch 916 / 1000 loss: 2.42102341633\n",
      "Epoch 917 / 1000 loss: 2.42093397398\n",
      "Epoch 918 / 1000 loss: 2.4208434606\n",
      "Epoch 919 / 1000 loss: 2.42075266782\n",
      "Epoch 920 / 1000 loss: 2.42066338658\n",
      "Epoch 921 / 1000 loss: 2.42057457939\n",
      "Epoch 922 / 1000 loss: 2.42049070075\n",
      "Epoch 923 / 1000 loss: 2.42040037923\n",
      "Epoch 924 / 1000 loss: 2.42030664627\n",
      "Epoch 925 / 1000 loss: 2.42021406535\n",
      "Epoch 926 / 1000 loss: 2.42011556495\n",
      "Epoch 927 / 1000 loss: 2.42002247367\n",
      "Epoch 928 / 1000 loss: 2.41993144248\n",
      "Epoch 929 / 1000 loss: 2.41984066553\n",
      "Epoch 930 / 1000 loss: 2.41974947229\n",
      "Epoch 931 / 1000 loss: 2.41965759639\n",
      "Epoch 932 / 1000 loss: 2.41956189275\n",
      "Epoch 933 / 1000 loss: 2.41946300026\n",
      "Epoch 934 / 1000 loss: 2.41936820094\n",
      "Epoch 935 / 1000 loss: 2.4192712456\n",
      "Epoch 936 / 1000 loss: 2.41917601787\n",
      "Epoch 937 / 1000 loss: 2.41908042226\n",
      "Epoch 938 / 1000 loss: 2.41898500081\n",
      "Epoch 939 / 1000 loss: 2.41888985783\n",
      "Epoch 940 / 1000 loss: 2.41879503336\n",
      "Epoch 941 / 1000 loss: 2.41869753972\n",
      "Epoch 942 / 1000 loss: 2.41859537922\n",
      "Epoch 943 / 1000 loss: 2.41849472374\n",
      "Epoch 944 / 1000 loss: 2.4183936147\n",
      "Epoch 945 / 1000 loss: 2.41829345375\n",
      "Epoch 946 / 1000 loss: 2.41819505487\n",
      "Epoch 947 / 1000 loss: 2.41809418052\n",
      "Epoch 948 / 1000 loss: 2.41799538024\n",
      "Epoch 949 / 1000 loss: 2.41789310798\n",
      "Epoch 950 / 1000 loss: 2.41778801288\n",
      "Epoch 951 / 1000 loss: 2.41768387519\n",
      "Epoch 952 / 1000 loss: 2.41757875215\n",
      "Epoch 953 / 1000 loss: 2.41747516114\n",
      "Epoch 954 / 1000 loss: 2.41736801062\n",
      "Epoch 955 / 1000 loss: 2.41726132762\n",
      "Epoch 956 / 1000 loss: 2.41715866886\n",
      "Epoch 957 / 1000 loss: 2.41705549601\n",
      "Epoch 958 / 1000 loss: 2.41695042606\n",
      "Epoch 959 / 1000 loss: 2.41684414912\n",
      "Epoch 960 / 1000 loss: 2.41674035508\n",
      "Epoch 961 / 1000 loss: 2.41663505509\n",
      "Epoch 962 / 1000 loss: 2.41653068829\n",
      "Epoch 963 / 1000 loss: 2.41642554291\n",
      "Epoch 964 / 1000 loss: 2.41632144246\n",
      "Epoch 965 / 1000 loss: 2.41621551476\n",
      "Epoch 966 / 1000 loss: 2.41611205507\n",
      "Epoch 967 / 1000 loss: 2.41600311175\n",
      "Epoch 968 / 1000 loss: 2.41589399893\n",
      "Epoch 969 / 1000 loss: 2.41578205861\n",
      "Epoch 970 / 1000 loss: 2.41566914134\n",
      "Epoch 971 / 1000 loss: 2.41555820312\n",
      "Epoch 972 / 1000 loss: 2.41544622183\n",
      "Epoch 973 / 1000 loss: 2.41533350665\n",
      "Epoch 974 / 1000 loss: 2.41522077378\n",
      "Epoch 975 / 1000 loss: 2.41510596685\n",
      "Epoch 976 / 1000 loss: 2.41499446519\n",
      "Epoch 977 / 1000 loss: 2.41487331036\n",
      "Epoch 978 / 1000 loss: 2.41475905664\n",
      "Epoch 979 / 1000 loss: 2.41464297567\n",
      "Epoch 980 / 1000 loss: 2.41452634521\n",
      "Epoch 981 / 1000 loss: 2.41441107262\n",
      "Epoch 982 / 1000 loss: 2.41429674719\n",
      "Epoch 983 / 1000 loss: 2.414177089\n",
      "Epoch 984 / 1000 loss: 2.41405860428\n",
      "Epoch 985 / 1000 loss: 2.41393523943\n",
      "Epoch 986 / 1000 loss: 2.41380917374\n",
      "Epoch 987 / 1000 loss: 2.41367979255\n",
      "Epoch 988 / 1000 loss: 2.41354944743\n",
      "Epoch 989 / 1000 loss: 2.41342017055\n",
      "Epoch 990 / 1000 loss: 2.41329169367\n",
      "Epoch 991 / 1000 loss: 2.41316062305\n",
      "Epoch 992 / 1000 loss: 2.41302271001\n",
      "Epoch 993 / 1000 loss: 2.41288569383\n",
      "Epoch 994 / 1000 loss: 2.41274838801\n",
      "Epoch 995 / 1000 loss: 2.41260730755\n",
      "Epoch 996 / 1000 loss: 2.41246596351\n",
      "Epoch 997 / 1000 loss: 2.4123208262\n",
      "Epoch 998 / 1000 loss: 2.41217003204\n",
      "Epoch 999 / 1000 loss: 2.41202306282\n"
     ]
    }
   ],
   "source": [
    "# initialising stuff and starting the session\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# defining batch size, number of epochs and learning rate\n",
    "\n",
    "batch_size = 500  # how many images to use together for training\n",
    "hm_epochs =1000    # how many times to go through the entire dataset\n",
    "tot_images =  len(tdata.transpose()[0])# total number of images\n",
    "\n",
    "# running the model for a 1000 epochs taking 100 images in batches\n",
    "# total improvement is printed out after each epoch\n",
    "\n",
    "for epoch in range(hm_epochs):\n",
    "\n",
    "    epoch_loss = 0    # initializing error as 0\n",
    "\n",
    "    for i in range(int(tot_images/batch_size)):\n",
    "\n",
    "        epoch_x = tdata[ i*batch_size : (i+1)*batch_size ]\n",
    "\n",
    "        _, c = sess.run([optimizer, meansq],\\\n",
    "               feed_dict={input_layer: epoch_x, \\\n",
    "               output_true: epoch_x})\n",
    "\n",
    "        epoch_loss += c\n",
    "\n",
    "    print('Epoch', epoch, '/', hm_epochs, 'loss:',epoch_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-31bd6aee193d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# pick any image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0many_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_images\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m999\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# run it though the autoencoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_images' is not defined"
     ]
    }
   ],
   "source": [
    "# pick any image\n",
    "\n",
    "any_image = all_images[999]\n",
    "\n",
    "# run it though the autoencoder\n",
    "\n",
    "output_any_image = sess.run(output_layer,\\\n",
    "                   feed_dict={input_layer:[any_image]})\n",
    "\n",
    "# run it though just the encoder\n",
    "\n",
    "encoded_any_image = sess.run(layer_1,\\\n",
    "                   feed_dict={input_layer:[any_image]})\n",
    "\n",
    "# print the original image\n",
    "\n",
    "plt.imshow(any_image(28,28),  cmap='Greys')\n",
    "plt.show()\n",
    "\n",
    "# print the encoding\n",
    "\n",
    "print(encoded_any_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
